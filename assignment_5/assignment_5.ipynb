{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fcc4f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "writer = SummaryWriter('runs/tweeteval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f973c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_to_ix = {}\n",
    "tag_to_ix = {0: \"anger\", 1: \"joy\",2: \"optimism\", 3: \"sadness\"} \n",
    "def process_tweets(data_dir, is_train=True, word_to_ix={}):\n",
    "    file_name = \"train_text.txt\" if is_train else \"test_text.txt\"\n",
    "    label_file_name = \"train_labels.txt\" if is_train else \"test_labels.txt\"\n",
    "    \n",
    "    with open(data_dir / file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = [line.strip() for line in f]\n",
    "    \n",
    "    with open(data_dir / label_file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        labels = [int(line.strip()) for line in f]\n",
    "    tweets = zip(texts, labels)\n",
    "\n",
    "    processed_tweets = []\n",
    "    \n",
    "    word_counter = Counter()\n",
    "    for tweet, label in tweets:\n",
    "        doc = nlp(tweet)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                token_text = token.text.lower()\n",
    "                tokens.append(token_text)\n",
    "                word_counter[token_text] += 1\n",
    "        processed_tweets.append((tokens, label))\n",
    "    \n",
    "    most_common_words = word_counter.most_common(3000)\n",
    "    for sent, _ in processed_tweets:\n",
    "        for word in sent:\n",
    "            if word not in most_common_words:\n",
    "                word_to_ix[word] = 3001\n",
    "            elif word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    return processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6adae4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = process_tweets(Path(\"tweeteval/datasets/emotion\"), is_train=True, word_to_ix=word_to_ix)\n",
    "processed_tweets_test = process_tweets(Path(\"tweeteval/datasets/emotion\"), is_train=False, word_to_ix=word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47822580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "738d4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        last_lstm_out = lstm_out[-1]  # Get the last output of the LSTM\n",
    "        tag_space = self.hidden2tag(last_lstm_out.view(1, -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "edb68345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(GRUTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The GRU takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        ast_gru_out = gru_out[-1]  # Get the last output of the LSTM\n",
    "        tag_space = self.hidden2tag(ast_gru_out.view(1, -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a332b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15707acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics_to_tensorboard(model_name, epoch, test_loss):\n",
    "    writer.add_scalar(f'{model_name}/test_loss', test_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee54568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, model_name: str, loss_function, optimizer, epochs: int):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for sentence, label in processed_tweets:\n",
    "            model.zero_grad()\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            target = torch.tensor([label], dtype=torch.long)\n",
    "\n",
    "            # forward pass\n",
    "            last_tag_score = model(sentence_in)\n",
    "\n",
    "            # Compute the loss, gradients, and update the parameters\n",
    "            loss = loss_function(last_tag_score, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        log_metrics_to_tensorboard(model_name, epoch, total_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "def test_model(model, processed_tweets_test):\n",
    "    correct = 0\n",
    "    total = len(processed_tweets_test)\n",
    "    with torch.no_grad():\n",
    "        for sentence, label in processed_tweets_test:\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            tag_scores = model(sentence_in)\n",
    "            predicted_tags = torch.argmax(tag_scores, dim=1)\n",
    "            if predicted_tags.item() == label:\n",
    "                correct += 1\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807e81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1113246.3883\n",
      "Epoch 2/100, Loss: 5790.8274\n",
      "Epoch 3/100, Loss: 5790.3456\n",
      "Epoch 4/100, Loss: 5789.8798\n",
      "Epoch 5/100, Loss: 5789.4182\n",
      "Epoch 6/100, Loss: 5788.9604\n",
      "Epoch 7/100, Loss: 5788.5037\n",
      "Epoch 8/100, Loss: 5788.0469\n",
      "Epoch 9/100, Loss: 5787.5909\n",
      "Epoch 10/100, Loss: 5787.1358\n",
      "Epoch 11/100, Loss: 5786.6805\n",
      "Epoch 12/100, Loss: 5786.2243\n",
      "Epoch 13/100, Loss: 5785.7697\n",
      "Epoch 14/100, Loss: 5785.3148\n",
      "Epoch 15/100, Loss: 5784.8594\n",
      "Epoch 16/100, Loss: 5784.4049\n",
      "Epoch 17/100, Loss: 5783.9500\n",
      "Epoch 18/100, Loss: 5783.4962\n",
      "Epoch 19/100, Loss: 5783.0411\n",
      "Epoch 20/100, Loss: 5782.5865\n",
      "Epoch 21/100, Loss: 5782.1308\n",
      "Epoch 22/100, Loss: 5781.6764\n",
      "Epoch 23/100, Loss: 5781.2218\n",
      "Epoch 24/100, Loss: 5780.7663\n",
      "Epoch 25/100, Loss: 5780.3111\n",
      "Epoch 26/100, Loss: 5779.8563\n",
      "Epoch 27/100, Loss: 5779.4013\n",
      "Epoch 28/100, Loss: 5778.9461\n",
      "Epoch 29/100, Loss: 5778.4915\n",
      "Epoch 30/100, Loss: 5778.0373\n",
      "Epoch 31/100, Loss: 5777.5830\n",
      "Epoch 32/100, Loss: 5777.1293\n",
      "Epoch 33/100, Loss: 5776.6739\n",
      "Epoch 34/100, Loss: 5776.2197\n",
      "Epoch 35/100, Loss: 5775.7649\n",
      "Epoch 36/100, Loss: 5775.3103\n",
      "Epoch 37/100, Loss: 5774.8546\n",
      "Epoch 38/100, Loss: 5774.4001\n",
      "Epoch 39/100, Loss: 5773.9451\n",
      "Epoch 40/100, Loss: 5773.4905\n",
      "Epoch 41/100, Loss: 5773.0364\n",
      "Epoch 42/100, Loss: 5772.5824\n",
      "Epoch 43/100, Loss: 5772.1281\n",
      "Epoch 44/100, Loss: 5771.6738\n",
      "Epoch 45/100, Loss: 5771.2202\n",
      "Epoch 46/100, Loss: 5770.7657\n",
      "Epoch 47/100, Loss: 5770.3101\n",
      "Epoch 48/100, Loss: 5769.8559\n",
      "Epoch 49/100, Loss: 5769.4019\n",
      "Epoch 50/100, Loss: 5768.9471\n",
      "Epoch 51/100, Loss: 5768.4937\n",
      "Epoch 52/100, Loss: 5768.0372\n",
      "Epoch 53/100, Loss: 5767.5811\n",
      "Epoch 54/100, Loss: 5767.1262\n",
      "Epoch 55/100, Loss: 5766.6704\n",
      "Epoch 56/100, Loss: 5766.2144\n",
      "Epoch 57/100, Loss: 5765.7596\n",
      "Epoch 58/100, Loss: 5765.3046\n",
      "Epoch 59/100, Loss: 5764.8498\n",
      "Epoch 60/100, Loss: 5764.3961\n",
      "Epoch 61/100, Loss: 5763.9415\n",
      "Epoch 62/100, Loss: 5763.4861\n",
      "Epoch 63/100, Loss: 5763.0321\n",
      "Epoch 64/100, Loss: 5762.5781\n",
      "Epoch 65/100, Loss: 5762.1235\n",
      "Epoch 66/100, Loss: 5761.6700\n",
      "Epoch 67/100, Loss: 5761.2164\n",
      "Epoch 68/100, Loss: 5760.7616\n",
      "Epoch 69/100, Loss: 5760.3071\n",
      "Epoch 70/100, Loss: 5759.8518\n",
      "Epoch 71/100, Loss: 5759.3957\n",
      "Epoch 72/100, Loss: 5758.9404\n",
      "Epoch 73/100, Loss: 5758.4857\n",
      "Epoch 74/100, Loss: 5758.0300\n",
      "Epoch 75/100, Loss: 5757.5754\n",
      "Epoch 76/100, Loss: 5757.1210\n",
      "Epoch 77/100, Loss: 5756.6673\n",
      "Epoch 78/100, Loss: 5756.2139\n",
      "Epoch 79/100, Loss: 5755.7593\n",
      "Epoch 80/100, Loss: 5755.3047\n",
      "Epoch 81/100, Loss: 5754.8510\n",
      "Epoch 82/100, Loss: 5754.3976\n",
      "Epoch 83/100, Loss: 5753.9446\n",
      "Epoch 84/100, Loss: 5753.4913\n",
      "Epoch 85/100, Loss: 5753.0393\n",
      "Epoch 86/100, Loss: 5752.5865\n",
      "Epoch 87/100, Loss: 5752.1346\n",
      "Epoch 88/100, Loss: 5751.6827\n",
      "Epoch 89/100, Loss: 5751.2299\n",
      "Epoch 90/100, Loss: 5750.7785\n",
      "Epoch 91/100, Loss: 5750.3256\n",
      "Epoch 92/100, Loss: 5749.8732\n",
      "Epoch 93/100, Loss: 5749.4208\n",
      "Epoch 94/100, Loss: 5748.9689\n",
      "Epoch 95/100, Loss: 5748.5175\n",
      "Epoch 96/100, Loss: 5748.0652\n",
      "Epoch 97/100, Loss: 5747.6137\n",
      "Epoch 98/100, Loss: 5747.1616\n",
      "Epoch 99/100, Loss: 5746.7105\n",
      "Epoch 100/100, Loss: 5746.2602\n",
      "Accuracy: 0.3913\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_model(model, \"lstm\", loss_function, optimizer, epochs=100)\n",
    "test_model(model, processed_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4fb72921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4442.3897\n",
      "Epoch 2/100, Loss: 4213.4029\n",
      "Epoch 3/100, Loss: 4194.1765\n",
      "Epoch 4/100, Loss: 4187.9751\n",
      "Epoch 5/100, Loss: 4181.4128\n",
      "Epoch 6/100, Loss: 4177.5328\n",
      "Epoch 7/100, Loss: 4174.9855\n",
      "Epoch 8/100, Loss: 4172.6767\n",
      "Epoch 9/100, Loss: 4170.2457\n",
      "Epoch 10/100, Loss: 4167.4800\n",
      "Epoch 11/100, Loss: 4165.7044\n",
      "Epoch 12/100, Loss: 4165.2013\n",
      "Epoch 13/100, Loss: 4164.9363\n",
      "Epoch 14/100, Loss: 4164.7352\n",
      "Epoch 15/100, Loss: 4164.5601\n",
      "Epoch 16/100, Loss: 4164.4009\n",
      "Epoch 17/100, Loss: 4164.2524\n",
      "Epoch 18/100, Loss: 4164.1114\n",
      "Epoch 19/100, Loss: 4163.9753\n",
      "Epoch 20/100, Loss: 4163.8426\n",
      "Epoch 21/100, Loss: 4163.7123\n",
      "Epoch 22/100, Loss: 4163.5832\n",
      "Epoch 23/100, Loss: 4163.4544\n",
      "Epoch 24/100, Loss: 4163.3248\n",
      "Epoch 25/100, Loss: 4163.1933\n",
      "Epoch 26/100, Loss: 4163.0584\n",
      "Epoch 27/100, Loss: 4162.9184\n",
      "Epoch 28/100, Loss: 4162.7714\n",
      "Epoch 29/100, Loss: 4162.6152\n",
      "Epoch 30/100, Loss: 4162.4468\n",
      "Epoch 31/100, Loss: 4162.2633\n",
      "Epoch 32/100, Loss: 4162.0609\n",
      "Epoch 33/100, Loss: 4161.8351\n",
      "Epoch 34/100, Loss: 4161.5815\n",
      "Epoch 35/100, Loss: 4161.2959\n",
      "Epoch 36/100, Loss: 4160.9760\n",
      "Epoch 37/100, Loss: 4160.6244\n",
      "Epoch 38/100, Loss: 4160.2503\n",
      "Epoch 39/100, Loss: 4159.8697\n",
      "Epoch 40/100, Loss: 4159.5000\n",
      "Epoch 41/100, Loss: 4159.1548\n",
      "Epoch 42/100, Loss: 4158.8413\n",
      "Epoch 43/100, Loss: 4158.5611\n",
      "Epoch 44/100, Loss: 4158.3124\n",
      "Epoch 45/100, Loss: 4158.0923\n",
      "Epoch 46/100, Loss: 4157.8972\n",
      "Epoch 47/100, Loss: 4157.7238\n",
      "Epoch 48/100, Loss: 4157.5690\n",
      "Epoch 49/100, Loss: 4157.4303\n",
      "Epoch 50/100, Loss: 4157.3054\n",
      "Epoch 51/100, Loss: 4157.1926\n",
      "Epoch 52/100, Loss: 4157.0901\n",
      "Epoch 53/100, Loss: 4156.9968\n",
      "Epoch 54/100, Loss: 4156.9115\n",
      "Epoch 55/100, Loss: 4156.8332\n",
      "Epoch 56/100, Loss: 4156.7610\n",
      "Epoch 57/100, Loss: 4156.6944\n",
      "Epoch 58/100, Loss: 4156.6328\n",
      "Epoch 59/100, Loss: 4156.5754\n",
      "Epoch 60/100, Loss: 4156.5221\n",
      "Epoch 61/100, Loss: 4156.4723\n",
      "Epoch 62/100, Loss: 4156.4256\n",
      "Epoch 63/100, Loss: 4156.3819\n",
      "Epoch 64/100, Loss: 4156.3407\n",
      "Epoch 65/100, Loss: 4156.3020\n",
      "Epoch 66/100, Loss: 4156.2655\n",
      "Epoch 67/100, Loss: 4156.2309\n",
      "Epoch 68/100, Loss: 4156.1982\n",
      "Epoch 69/100, Loss: 4156.1671\n",
      "Epoch 70/100, Loss: 4156.1376\n",
      "Epoch 71/100, Loss: 4156.1095\n",
      "Epoch 72/100, Loss: 4156.0827\n",
      "Epoch 73/100, Loss: 4156.0572\n",
      "Epoch 74/100, Loss: 4156.0328\n",
      "Epoch 75/100, Loss: 4156.0094\n",
      "Epoch 76/100, Loss: 4155.9870\n",
      "Epoch 77/100, Loss: 4155.9656\n",
      "Epoch 78/100, Loss: 4155.9449\n",
      "Epoch 79/100, Loss: 4155.9251\n",
      "Epoch 80/100, Loss: 4155.9060\n",
      "Epoch 81/100, Loss: 4155.8876\n",
      "Epoch 82/100, Loss: 4155.8699\n",
      "Epoch 83/100, Loss: 4155.8528\n",
      "Epoch 84/100, Loss: 4155.8363\n",
      "Epoch 85/100, Loss: 4155.8203\n",
      "Epoch 86/100, Loss: 4155.8048\n",
      "Epoch 87/100, Loss: 4155.7898\n",
      "Epoch 88/100, Loss: 4155.7753\n",
      "Epoch 89/100, Loss: 4155.7612\n",
      "Epoch 90/100, Loss: 4155.7476\n",
      "Epoch 91/100, Loss: 4155.7343\n",
      "Epoch 92/100, Loss: 4155.7214\n",
      "Epoch 93/100, Loss: 4155.7088\n",
      "Epoch 94/100, Loss: 4155.6967\n",
      "Epoch 95/100, Loss: 4155.6848\n",
      "Epoch 96/100, Loss: 4155.6732\n",
      "Epoch 97/100, Loss: 4155.6620\n",
      "Epoch 98/100, Loss: 4155.6510\n",
      "Epoch 99/100, Loss: 4155.6403\n",
      "Epoch 100/100, Loss: 4155.6298\n",
      "Accuracy: 0.3927\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRUTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(gru_model.parameters(), lr=0.1)\n",
    "train_model(gru_model, \"gru\", loss_function, optimizer, epochs=100)\n",
    "test_model(gru_model, processed_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d07e7afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15443), started 0:00:04 ago. (Use '!kill 15443' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ce525dbfdd30af51\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ce525dbfdd30af51\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

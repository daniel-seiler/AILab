{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fcc4f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "writer = SummaryWriter('runs/tweeteval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f973c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_to_ix = {}\n",
    "tag_to_ix = {0: \"anger\", 1: \"joy\",2: \"optimism\", 3: \"sadness\"} \n",
    "def process_tweets(data_dir, is_train=True, word_to_ix={}):\n",
    "    file_name = \"train_text.txt\" if is_train else \"test_text.txt\"\n",
    "    label_file_name = \"train_labels.txt\" if is_train else \"test_labels.txt\"\n",
    "    \n",
    "    with open(data_dir / file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = [line.strip() for line in f]\n",
    "    \n",
    "    with open(data_dir / label_file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        labels = [int(line.strip()) for line in f]\n",
    "    tweets = zip(texts, labels)\n",
    "\n",
    "    processed_tweets = []\n",
    "    \n",
    "    word_counter = Counter()\n",
    "    for tweet, label in tweets:\n",
    "        doc = nlp(tweet)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "                token_text = token.text.lower()\n",
    "                tokens.append(token_text)\n",
    "                word_counter[token_text] += 1\n",
    "        processed_tweets.append((tokens, label))\n",
    "    \n",
    "    most_common_words = word_counter.most_common(3000)\n",
    "    for sent, _ in processed_tweets:\n",
    "        for word in sent:\n",
    "            if word not in most_common_words:\n",
    "                word_to_ix[word] = 3001\n",
    "            elif word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    return processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6adae4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = process_tweets(Path(\"tweeteval/datasets/emotion\"), is_train=True, word_to_ix=word_to_ix)\n",
    "processed_tweets_test = process_tweets(Path(\"tweeteval/datasets/emotion\"), is_train=False, word_to_ix=word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47822580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "738d4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        last_lstm_out = lstm_out[-1]  # Get the last output of the LSTM\n",
    "        tag_space = self.hidden2tag(last_lstm_out.view(1, -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a332b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15707acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics_to_tensorboard(model_name, epoch, test_loss):\n",
    "    writer.add_scalar(f'{model_name}/test_loss', test_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ee54568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_model(model, loss_function, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for sentence, label in processed_tweets:\n",
    "            model.zero_grad()\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            target = torch.tensor([label], dtype=torch.long)\n",
    "\n",
    "            # forward pass\n",
    "            last_tag_score = model(sentence_in)\n",
    "\n",
    "            # Compute the loss, gradients, and update the parameters\n",
    "            loss = loss_function(last_tag_score, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        log_metrics_to_tensorboard(\"LSTMTagger\", epoch, total_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "def test_model(model, processed_tweets_test):\n",
    "    correct = 0\n",
    "    total = len(processed_tweets_test)\n",
    "    with torch.no_grad():\n",
    "        for sentence, label in processed_tweets_test:\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            tag_scores = model(sentence_in)\n",
    "            predicted_tags = torch.argmax(tag_scores, dim=1)\n",
    "            if predicted_tags.item() == label:\n",
    "                correct += 1\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3807e81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4118.7536\n",
      "Epoch 2/100, Loss: 4117.7466\n",
      "Epoch 3/100, Loss: 4116.9765\n",
      "Epoch 4/100, Loss: 4116.4035\n",
      "Epoch 5/100, Loss: 4115.9769\n",
      "Epoch 6/100, Loss: 4115.6591\n",
      "Epoch 7/100, Loss: 4115.4205\n",
      "Epoch 8/100, Loss: 4115.2379\n",
      "Epoch 9/100, Loss: 4115.0942\n",
      "Epoch 10/100, Loss: 4114.9775\n",
      "Epoch 11/100, Loss: 4114.8801\n",
      "Epoch 12/100, Loss: 4114.7968\n",
      "Epoch 13/100, Loss: 4114.7243\n",
      "Epoch 14/100, Loss: 4114.6606\n",
      "Epoch 15/100, Loss: 4114.6041\n",
      "Epoch 16/100, Loss: 4114.5539\n",
      "Epoch 17/100, Loss: 4114.5090\n",
      "Epoch 18/100, Loss: 4114.4687\n",
      "Epoch 19/100, Loss: 4114.4323\n",
      "Epoch 20/100, Loss: 4114.3995\n",
      "Epoch 21/100, Loss: 4114.3696\n",
      "Epoch 22/100, Loss: 4114.3422\n",
      "Epoch 23/100, Loss: 4114.3166\n",
      "Epoch 24/100, Loss: 4114.2925\n",
      "Epoch 25/100, Loss: 4114.2694\n",
      "Epoch 26/100, Loss: 4114.2470\n",
      "Epoch 27/100, Loss: 4114.2251\n",
      "Epoch 28/100, Loss: 4114.2034\n",
      "Epoch 29/100, Loss: 4114.1818\n",
      "Epoch 30/100, Loss: 4114.1601\n",
      "Epoch 31/100, Loss: 4114.1382\n",
      "Epoch 32/100, Loss: 4114.1161\n",
      "Epoch 33/100, Loss: 4114.0935\n",
      "Epoch 34/100, Loss: 4114.0705\n",
      "Epoch 35/100, Loss: 4114.0469\n",
      "Epoch 36/100, Loss: 4114.0226\n",
      "Epoch 37/100, Loss: 4113.9975\n",
      "Epoch 38/100, Loss: 4113.9715\n",
      "Epoch 39/100, Loss: 4113.9445\n",
      "Epoch 40/100, Loss: 4113.9163\n",
      "Epoch 41/100, Loss: 4113.8869\n",
      "Epoch 42/100, Loss: 4113.8560\n",
      "Epoch 43/100, Loss: 4113.8239\n",
      "Epoch 44/100, Loss: 4113.7903\n",
      "Epoch 45/100, Loss: 4113.7555\n",
      "Epoch 46/100, Loss: 4113.7199\n",
      "Epoch 47/100, Loss: 4113.6836\n",
      "Epoch 48/100, Loss: 4113.6473\n",
      "Epoch 49/100, Loss: 4113.6116\n",
      "Epoch 50/100, Loss: 4113.5768\n",
      "Epoch 51/100, Loss: 4113.5435\n",
      "Epoch 52/100, Loss: 4113.5120\n",
      "Epoch 53/100, Loss: 4113.4823\n",
      "Epoch 54/100, Loss: 4113.4545\n",
      "Epoch 55/100, Loss: 4113.4284\n",
      "Epoch 56/100, Loss: 4113.4038\n",
      "Epoch 57/100, Loss: 4113.3804\n",
      "Epoch 58/100, Loss: 4113.3579\n",
      "Epoch 59/100, Loss: 4113.3361\n",
      "Epoch 60/100, Loss: 4113.3145\n",
      "Epoch 61/100, Loss: 4113.2929\n",
      "Epoch 62/100, Loss: 4113.2710\n",
      "Epoch 63/100, Loss: 4113.2483\n",
      "Epoch 64/100, Loss: 4113.2246\n",
      "Epoch 65/100, Loss: 4113.1995\n",
      "Epoch 66/100, Loss: 4113.1727\n",
      "Epoch 67/100, Loss: 4113.1440\n",
      "Epoch 68/100, Loss: 4113.1132\n",
      "Epoch 69/100, Loss: 4113.0805\n",
      "Epoch 70/100, Loss: 4113.0461\n",
      "Epoch 71/100, Loss: 4113.0103\n",
      "Epoch 72/100, Loss: 4112.9738\n",
      "Epoch 73/100, Loss: 4112.9370\n",
      "Epoch 74/100, Loss: 4112.9004\n",
      "Epoch 75/100, Loss: 4112.8645\n",
      "Epoch 76/100, Loss: 4112.8296\n",
      "Epoch 77/100, Loss: 4112.7960\n",
      "Epoch 78/100, Loss: 4112.7639\n",
      "Epoch 79/100, Loss: 4112.7334\n",
      "Epoch 80/100, Loss: 4112.7047\n",
      "Epoch 81/100, Loss: 4112.6777\n",
      "Epoch 82/100, Loss: 4112.6525\n",
      "Epoch 83/100, Loss: 4112.6289\n",
      "Epoch 84/100, Loss: 4112.6068\n",
      "Epoch 85/100, Loss: 4112.5861\n",
      "Epoch 86/100, Loss: 4112.5667\n",
      "Epoch 87/100, Loss: 4112.5484\n",
      "Epoch 88/100, Loss: 4112.5310\n",
      "Epoch 89/100, Loss: 4112.5145\n",
      "Epoch 90/100, Loss: 4112.4987\n",
      "Epoch 91/100, Loss: 4112.4836\n",
      "Epoch 92/100, Loss: 4112.4690\n",
      "Epoch 93/100, Loss: 4112.4548\n",
      "Epoch 94/100, Loss: 4112.4410\n",
      "Epoch 95/100, Loss: 4112.4276\n",
      "Epoch 96/100, Loss: 4112.4144\n",
      "Epoch 97/100, Loss: 4112.4015\n",
      "Epoch 98/100, Loss: 4112.3887\n",
      "Epoch 99/100, Loss: 4112.3762\n",
      "Epoch 100/100, Loss: 4112.3637\n",
      "Accuracy: 0.3927\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_function, optimizer, epochs=100)\n",
    "test_model(model, processed_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d07e7afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7104), started 0:12:36 ago. (Use '!kill 7104' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-92cbfe7cbfc0a54a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-92cbfe7cbfc0a54a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

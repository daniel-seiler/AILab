{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1663cf1",
   "metadata": {},
   "source": [
    "## Assignment 1 - Mnist with a simple (one hidden layer) Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "04d388c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cpu\") #Use CPU\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #If nvidea GPU is available use GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de9d27",
   "metadata": {},
   "source": [
    "Creating the nn.module for a neural network with two layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d56704ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "    \"\"\"\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    self.D_out = D_out\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    In the forward function we accept a Tensor of input data and we must return\n",
    "    a Tensor of output data. We can use Modules defined in the constructor as\n",
    "    well as arbitrary (differentiable) operations on Tensors.\n",
    "    Uses log_softmax\n",
    "    \"\"\"\n",
    "    # flatten the tensor\n",
    "    x = x.view(x.size(0), -1)\n",
    "    h_relu = self.linear1(x).clamp(min=0)\n",
    "    y_pred = self.linear2(h_relu)\n",
    "    return torch.nn.functional.log_softmax(y_pred, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323d337",
   "metadata": {},
   "source": [
    "Setup vars that impact the time and effectivness of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7aed69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64*10\n",
    "batch_size_test = 1000\n",
    "learning_rate = 1e-3\n",
    "n_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d0f37",
   "metadata": {},
   "source": [
    "Loading the mnist-dataset, pictures of handwritten numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8aefdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "  transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "      (0.1307,), (0.3081,)),\n",
    "  ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e09b7c",
   "metadata": {},
   "source": [
    "Precomputing the *torchvision.transforms* and loading the **complete** dataset on the GPU. This is not nessesary and in some cases a stupid idea, but in this case it causes a huge speedup. This is why we decided to keep it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "02867b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "for img, label in train_mnist:\n",
    "    train_data.append(img)\n",
    "    train_labels.append(label)\n",
    "\n",
    "train_data = torch.stack(train_data)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torch.utils.data.TensorDataset(train_data, train_labels),\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73a6ed",
   "metadata": {},
   "source": [
    "Loading the test-dataset. Here we don't precompute and don't transfer the data to the GPU because we will only use this data a few times. The train data will be used in multiple epochs so we get much more use out of the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "289afc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "      torchvision.transforms.ToTensor(),\n",
    "      torchvision.transforms.Normalize(\n",
    "        (0.1307,), (0.3081,))\n",
    "    ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eaf22",
   "metadata": {},
   "source": [
    "Setting up the vars for the network. \n",
    "\n",
    "The hidden layer is 100 per assignment. \n",
    "\n",
    "The  output needs to be 10 for numbers 0-9.\n",
    "\n",
    "And finally the input is 784=28*28 for the gray-value each pixel of a 28x28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "536661d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, H, D_out = 784, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0d2f8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model = model.to(device)\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82286e",
   "metadata": {},
   "source": [
    "Training the model using nn.functional.nll_loss as loss and optim.SGD as optimizer \n",
    "\n",
    "Time \n",
    "cpu(i5-9400F): 33.8s\n",
    "\n",
    "gpu(4060 Ti): 35.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "460a996f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 2.3232221603393555\n",
      "Epoch 0, Batch 1, Loss: 2.332162380218506\n",
      "Epoch 0, Batch 2, Loss: 2.3245551586151123\n",
      "Epoch 0, Batch 3, Loss: 2.3153510093688965\n",
      "Epoch 0, Batch 4, Loss: 2.3192992210388184\n",
      "Epoch 0, Batch 5, Loss: 2.3153114318847656\n",
      "Epoch 0, Batch 6, Loss: 2.3272793292999268\n",
      "Epoch 0, Batch 7, Loss: 2.2868707180023193\n",
      "Epoch 0, Batch 8, Loss: 2.309028387069702\n",
      "Epoch 0, Batch 9, Loss: 2.314734935760498\n",
      "Epoch 0, Batch 10, Loss: 2.30753231048584\n",
      "Epoch 0, Batch 11, Loss: 2.302541971206665\n",
      "Epoch 0, Batch 12, Loss: 2.310297727584839\n",
      "Epoch 0, Batch 13, Loss: 2.3037075996398926\n",
      "Epoch 0, Batch 14, Loss: 2.2972826957702637\n",
      "Epoch 0, Batch 15, Loss: 2.2993621826171875\n",
      "Epoch 0, Batch 16, Loss: 2.296792507171631\n",
      "Epoch 0, Batch 17, Loss: 2.292419672012329\n",
      "Epoch 0, Batch 18, Loss: 2.3023266792297363\n",
      "Epoch 0, Batch 19, Loss: 2.298276662826538\n",
      "Epoch 0, Batch 20, Loss: 2.289170742034912\n",
      "Epoch 0, Batch 21, Loss: 2.2809579372406006\n",
      "Epoch 0, Batch 22, Loss: 2.2904374599456787\n",
      "Epoch 0, Batch 23, Loss: 2.291614055633545\n",
      "Epoch 0, Batch 24, Loss: 2.2940289974212646\n",
      "Epoch 0, Batch 25, Loss: 2.270871639251709\n",
      "Epoch 0, Batch 26, Loss: 2.299074649810791\n",
      "Epoch 0, Batch 27, Loss: 2.266112804412842\n",
      "Epoch 0, Batch 28, Loss: 2.2691586017608643\n",
      "Epoch 0, Batch 29, Loss: 2.2612264156341553\n",
      "Epoch 0, Batch 30, Loss: 2.2680163383483887\n",
      "Epoch 0, Batch 31, Loss: 2.2767586708068848\n",
      "Epoch 0, Batch 32, Loss: 2.273365020751953\n",
      "Epoch 0, Batch 33, Loss: 2.2527565956115723\n",
      "Epoch 0, Batch 34, Loss: 2.270669460296631\n",
      "Epoch 0, Batch 35, Loss: 2.262861728668213\n",
      "Epoch 0, Batch 36, Loss: 2.256918430328369\n",
      "Epoch 0, Batch 37, Loss: 2.2570042610168457\n",
      "Epoch 0, Batch 38, Loss: 2.2659237384796143\n",
      "Epoch 0, Batch 39, Loss: 2.2643351554870605\n",
      "Epoch 0, Batch 40, Loss: 2.249107837677002\n",
      "Epoch 0, Batch 41, Loss: 2.2529406547546387\n",
      "Epoch 0, Batch 42, Loss: 2.2446298599243164\n",
      "Epoch 0, Batch 43, Loss: 2.2570884227752686\n",
      "Epoch 0, Batch 44, Loss: 2.2462475299835205\n",
      "Epoch 0, Batch 45, Loss: 2.2380316257476807\n",
      "Epoch 0, Batch 46, Loss: 2.2540459632873535\n",
      "Epoch 0, Batch 47, Loss: 2.2524120807647705\n",
      "Epoch 0, Batch 48, Loss: 2.2373149394989014\n",
      "Epoch 0, Batch 49, Loss: 2.2477450370788574\n",
      "Epoch 0, Batch 50, Loss: 2.2350926399230957\n",
      "Epoch 0, Batch 51, Loss: 2.232755422592163\n",
      "Epoch 0, Batch 52, Loss: 2.2302982807159424\n",
      "Epoch 0, Batch 53, Loss: 2.2434346675872803\n",
      "Epoch 0, Batch 54, Loss: 2.2312073707580566\n",
      "Epoch 0, Batch 55, Loss: 2.237462043762207\n",
      "Epoch 0, Batch 56, Loss: 2.2358016967773438\n",
      "Epoch 0, Batch 57, Loss: 2.2260282039642334\n",
      "Epoch 0, Batch 58, Loss: 2.2109508514404297\n",
      "Epoch 0, Batch 59, Loss: 2.222625732421875\n",
      "Epoch 0, Batch 60, Loss: 2.2219040393829346\n",
      "Epoch 0, Batch 61, Loss: 2.2103638648986816\n",
      "Epoch 0, Batch 62, Loss: 2.2149109840393066\n",
      "Epoch 0, Batch 63, Loss: 2.2164087295532227\n",
      "Epoch 0, Batch 64, Loss: 2.21079683303833\n",
      "Epoch 0, Batch 65, Loss: 2.225248336791992\n",
      "Epoch 0, Batch 66, Loss: 2.2113609313964844\n",
      "Epoch 0, Batch 67, Loss: 2.205859422683716\n",
      "Epoch 0, Batch 68, Loss: 2.2033982276916504\n",
      "Epoch 0, Batch 69, Loss: 2.1945993900299072\n",
      "Epoch 0, Batch 70, Loss: 2.2097039222717285\n",
      "Epoch 0, Batch 71, Loss: 2.1981310844421387\n",
      "Epoch 0, Batch 72, Loss: 2.1927640438079834\n",
      "Epoch 0, Batch 73, Loss: 2.1918375492095947\n",
      "Epoch 0, Batch 74, Loss: 2.206439733505249\n",
      "Epoch 0, Batch 75, Loss: 2.190884828567505\n",
      "Epoch 0, Batch 76, Loss: 2.1914706230163574\n",
      "Epoch 0, Batch 77, Loss: 2.1950583457946777\n",
      "Epoch 0, Batch 78, Loss: 2.1823058128356934\n",
      "Epoch 0, Batch 79, Loss: 2.1882176399230957\n",
      "Epoch 0, Batch 80, Loss: 2.1844613552093506\n",
      "Epoch 0, Batch 81, Loss: 2.1804614067077637\n",
      "Epoch 0, Batch 82, Loss: 2.179657459259033\n",
      "Epoch 0, Batch 83, Loss: 2.191504955291748\n",
      "Epoch 0, Batch 84, Loss: 2.186353921890259\n",
      "Epoch 0, Batch 85, Loss: 2.185250997543335\n",
      "Epoch 0, Batch 86, Loss: 2.186095714569092\n",
      "Epoch 0, Batch 87, Loss: 2.1762051582336426\n",
      "Epoch 0, Batch 88, Loss: 2.1677253246307373\n",
      "Epoch 0, Batch 89, Loss: 2.1780810356140137\n",
      "Epoch 0, Batch 90, Loss: 2.1740212440490723\n",
      "Epoch 0, Batch 91, Loss: 2.1826558113098145\n",
      "Epoch 0, Batch 92, Loss: 2.171809196472168\n",
      "Epoch 0, Batch 93, Loss: 2.1661412715911865\n",
      "Epoch 1, Batch 0, Loss: 2.1716506481170654\n",
      "Epoch 1, Batch 1, Loss: 2.1611013412475586\n",
      "Epoch 1, Batch 2, Loss: 2.147383213043213\n",
      "Epoch 1, Batch 3, Loss: 2.1777071952819824\n",
      "Epoch 1, Batch 4, Loss: 2.1515796184539795\n",
      "Epoch 1, Batch 5, Loss: 2.1520867347717285\n",
      "Epoch 1, Batch 6, Loss: 2.1570582389831543\n",
      "Epoch 1, Batch 7, Loss: 2.1563823223114014\n",
      "Epoch 1, Batch 8, Loss: 2.17055082321167\n",
      "Epoch 1, Batch 9, Loss: 2.154334306716919\n",
      "Epoch 1, Batch 10, Loss: 2.146263599395752\n",
      "Epoch 1, Batch 11, Loss: 2.143071174621582\n",
      "Epoch 1, Batch 12, Loss: 2.14741849899292\n",
      "Epoch 1, Batch 13, Loss: 2.1527421474456787\n",
      "Epoch 1, Batch 14, Loss: 2.1406006813049316\n",
      "Epoch 1, Batch 15, Loss: 2.1359081268310547\n",
      "Epoch 1, Batch 16, Loss: 2.1332690715789795\n",
      "Epoch 1, Batch 17, Loss: 2.144035816192627\n",
      "Epoch 1, Batch 18, Loss: 2.1427206993103027\n",
      "Epoch 1, Batch 19, Loss: 2.145998239517212\n",
      "Epoch 1, Batch 20, Loss: 2.128037929534912\n",
      "Epoch 1, Batch 21, Loss: 2.1462504863739014\n",
      "Epoch 1, Batch 22, Loss: 2.124607563018799\n",
      "Epoch 1, Batch 23, Loss: 2.1304802894592285\n",
      "Epoch 1, Batch 24, Loss: 2.1159274578094482\n",
      "Epoch 1, Batch 25, Loss: 2.1265666484832764\n",
      "Epoch 1, Batch 26, Loss: 2.119558811187744\n",
      "Epoch 1, Batch 27, Loss: 2.1214258670806885\n",
      "Epoch 1, Batch 28, Loss: 2.1231024265289307\n",
      "Epoch 1, Batch 29, Loss: 2.1158697605133057\n",
      "Epoch 1, Batch 30, Loss: 2.115602731704712\n",
      "Epoch 1, Batch 31, Loss: 2.124307870864868\n",
      "Epoch 1, Batch 32, Loss: 2.105475664138794\n",
      "Epoch 1, Batch 33, Loss: 2.106410264968872\n",
      "Epoch 1, Batch 34, Loss: 2.109118938446045\n",
      "Epoch 1, Batch 35, Loss: 2.1062138080596924\n",
      "Epoch 1, Batch 36, Loss: 2.0971851348876953\n",
      "Epoch 1, Batch 37, Loss: 2.1197314262390137\n",
      "Epoch 1, Batch 38, Loss: 2.1037774085998535\n",
      "Epoch 1, Batch 39, Loss: 2.1036386489868164\n",
      "Epoch 1, Batch 40, Loss: 2.0795629024505615\n",
      "Epoch 1, Batch 41, Loss: 2.1057021617889404\n",
      "Epoch 1, Batch 42, Loss: 2.095825672149658\n",
      "Epoch 1, Batch 43, Loss: 2.092905044555664\n",
      "Epoch 1, Batch 44, Loss: 2.114114284515381\n",
      "Epoch 1, Batch 45, Loss: 2.103942632675171\n",
      "Epoch 1, Batch 46, Loss: 2.094770908355713\n",
      "Epoch 1, Batch 47, Loss: 2.089214563369751\n",
      "Epoch 1, Batch 48, Loss: 2.086019992828369\n",
      "Epoch 1, Batch 49, Loss: 2.0990192890167236\n",
      "Epoch 1, Batch 50, Loss: 2.0907115936279297\n",
      "Epoch 1, Batch 51, Loss: 2.080106258392334\n",
      "Epoch 1, Batch 52, Loss: 2.0765655040740967\n",
      "Epoch 1, Batch 53, Loss: 2.084010124206543\n",
      "Epoch 1, Batch 54, Loss: 2.0741026401519775\n",
      "Epoch 1, Batch 55, Loss: 2.0850400924682617\n",
      "Epoch 1, Batch 56, Loss: 2.072596549987793\n",
      "Epoch 1, Batch 57, Loss: 2.0835914611816406\n",
      "Epoch 1, Batch 58, Loss: 2.090088367462158\n",
      "Epoch 1, Batch 59, Loss: 2.0729150772094727\n",
      "Epoch 1, Batch 60, Loss: 2.0615453720092773\n",
      "Epoch 1, Batch 61, Loss: 2.06140398979187\n",
      "Epoch 1, Batch 62, Loss: 2.0709750652313232\n",
      "Epoch 1, Batch 63, Loss: 2.0682640075683594\n",
      "Epoch 1, Batch 64, Loss: 2.064546823501587\n",
      "Epoch 1, Batch 65, Loss: 2.067351818084717\n",
      "Epoch 1, Batch 66, Loss: 2.0658304691314697\n",
      "Epoch 1, Batch 67, Loss: 2.0506160259246826\n",
      "Epoch 1, Batch 68, Loss: 2.0663890838623047\n",
      "Epoch 1, Batch 69, Loss: 2.0424749851226807\n",
      "Epoch 1, Batch 70, Loss: 2.0500235557556152\n",
      "Epoch 1, Batch 71, Loss: 2.046623468399048\n",
      "Epoch 1, Batch 72, Loss: 2.052318572998047\n",
      "Epoch 1, Batch 73, Loss: 2.0414624214172363\n",
      "Epoch 1, Batch 74, Loss: 2.0554795265197754\n",
      "Epoch 1, Batch 75, Loss: 2.03855562210083\n",
      "Epoch 1, Batch 76, Loss: 2.0443520545959473\n",
      "Epoch 1, Batch 77, Loss: 2.0417988300323486\n",
      "Epoch 1, Batch 78, Loss: 2.0448946952819824\n",
      "Epoch 1, Batch 79, Loss: 2.043229341506958\n",
      "Epoch 1, Batch 80, Loss: 2.039947509765625\n",
      "Epoch 1, Batch 81, Loss: 2.0419058799743652\n",
      "Epoch 1, Batch 82, Loss: 2.036128520965576\n",
      "Epoch 1, Batch 83, Loss: 2.0504322052001953\n",
      "Epoch 1, Batch 84, Loss: 2.0317721366882324\n",
      "Epoch 1, Batch 85, Loss: 2.020325183868408\n",
      "Epoch 1, Batch 86, Loss: 2.029306173324585\n",
      "Epoch 1, Batch 87, Loss: 2.0116727352142334\n",
      "Epoch 1, Batch 88, Loss: 2.015331983566284\n",
      "Epoch 1, Batch 89, Loss: 2.0114705562591553\n",
      "Epoch 1, Batch 90, Loss: 2.042952537536621\n",
      "Epoch 1, Batch 91, Loss: 2.0041446685791016\n",
      "Epoch 1, Batch 92, Loss: 2.0149741172790527\n",
      "Epoch 1, Batch 93, Loss: 1.998423457145691\n",
      "Epoch 2, Batch 0, Loss: 2.009276866912842\n",
      "Epoch 2, Batch 1, Loss: 2.0151455402374268\n",
      "Epoch 2, Batch 2, Loss: 2.0163166522979736\n",
      "Epoch 2, Batch 3, Loss: 2.0166828632354736\n",
      "Epoch 2, Batch 4, Loss: 1.9975793361663818\n",
      "Epoch 2, Batch 5, Loss: 2.0184121131896973\n",
      "Epoch 2, Batch 6, Loss: 2.003297805786133\n",
      "Epoch 2, Batch 7, Loss: 1.9944015741348267\n",
      "Epoch 2, Batch 8, Loss: 1.986633062362671\n",
      "Epoch 2, Batch 9, Loss: 2.0119500160217285\n",
      "Epoch 2, Batch 10, Loss: 1.9954421520233154\n",
      "Epoch 2, Batch 11, Loss: 2.001955509185791\n",
      "Epoch 2, Batch 12, Loss: 1.9886503219604492\n",
      "Epoch 2, Batch 13, Loss: 1.9825716018676758\n",
      "Epoch 2, Batch 14, Loss: 2.003070116043091\n",
      "Epoch 2, Batch 15, Loss: 1.9848291873931885\n",
      "Epoch 2, Batch 16, Loss: 1.9660499095916748\n",
      "Epoch 2, Batch 17, Loss: 2.0075581073760986\n",
      "Epoch 2, Batch 18, Loss: 2.001270294189453\n",
      "Epoch 2, Batch 19, Loss: 1.9821968078613281\n",
      "Epoch 2, Batch 20, Loss: 1.9822098016738892\n",
      "Epoch 2, Batch 21, Loss: 1.977526068687439\n",
      "Epoch 2, Batch 22, Loss: 1.9838154315948486\n",
      "Epoch 2, Batch 23, Loss: 1.9662145376205444\n",
      "Epoch 2, Batch 24, Loss: 1.9633487462997437\n",
      "Epoch 2, Batch 25, Loss: 1.980735182762146\n",
      "Epoch 2, Batch 26, Loss: 1.9760631322860718\n",
      "Epoch 2, Batch 27, Loss: 1.9751144647598267\n",
      "Epoch 2, Batch 28, Loss: 1.9647270441055298\n",
      "Epoch 2, Batch 29, Loss: 1.9716078042984009\n",
      "Epoch 2, Batch 30, Loss: 1.9783217906951904\n",
      "Epoch 2, Batch 31, Loss: 1.9578298330307007\n",
      "Epoch 2, Batch 32, Loss: 1.9754602909088135\n",
      "Epoch 2, Batch 33, Loss: 1.97907292842865\n",
      "Epoch 2, Batch 34, Loss: 1.9574638605117798\n",
      "Epoch 2, Batch 35, Loss: 1.958569884300232\n",
      "Epoch 2, Batch 36, Loss: 1.9775301218032837\n",
      "Epoch 2, Batch 37, Loss: 1.9675137996673584\n",
      "Epoch 2, Batch 38, Loss: 1.9503597021102905\n",
      "Epoch 2, Batch 39, Loss: 1.9603025913238525\n",
      "Epoch 2, Batch 40, Loss: 1.9406360387802124\n",
      "Epoch 2, Batch 41, Loss: 1.9331632852554321\n",
      "Epoch 2, Batch 42, Loss: 1.9621896743774414\n",
      "Epoch 2, Batch 43, Loss: 1.9481309652328491\n",
      "Epoch 2, Batch 44, Loss: 1.941875696182251\n",
      "Epoch 2, Batch 45, Loss: 1.9335905313491821\n",
      "Epoch 2, Batch 46, Loss: 1.9327253103256226\n",
      "Epoch 2, Batch 47, Loss: 1.93357253074646\n",
      "Epoch 2, Batch 48, Loss: 1.9458868503570557\n",
      "Epoch 2, Batch 49, Loss: 1.9431867599487305\n",
      "Epoch 2, Batch 50, Loss: 1.9334245920181274\n",
      "Epoch 2, Batch 51, Loss: 1.93026602268219\n",
      "Epoch 2, Batch 52, Loss: 1.9132505655288696\n",
      "Epoch 2, Batch 53, Loss: 1.9316561222076416\n",
      "Epoch 2, Batch 54, Loss: 1.9153735637664795\n",
      "Epoch 2, Batch 55, Loss: 1.9189542531967163\n",
      "Epoch 2, Batch 56, Loss: 1.9225318431854248\n",
      "Epoch 2, Batch 57, Loss: 1.917569875717163\n",
      "Epoch 2, Batch 58, Loss: 1.9164708852767944\n",
      "Epoch 2, Batch 59, Loss: 1.9080400466918945\n",
      "Epoch 2, Batch 60, Loss: 1.9134008884429932\n",
      "Epoch 2, Batch 61, Loss: 1.903494119644165\n",
      "Epoch 2, Batch 62, Loss: 1.9063562154769897\n",
      "Epoch 2, Batch 63, Loss: 1.9373782873153687\n",
      "Epoch 2, Batch 64, Loss: 1.9018051624298096\n",
      "Epoch 2, Batch 65, Loss: 1.896737813949585\n",
      "Epoch 2, Batch 66, Loss: 1.892327070236206\n",
      "Epoch 2, Batch 67, Loss: 1.9028701782226562\n",
      "Epoch 2, Batch 68, Loss: 1.8977031707763672\n",
      "Epoch 2, Batch 69, Loss: 1.9239997863769531\n",
      "Epoch 2, Batch 70, Loss: 1.8911364078521729\n",
      "Epoch 2, Batch 71, Loss: 1.9134031534194946\n",
      "Epoch 2, Batch 72, Loss: 1.8983615636825562\n",
      "Epoch 2, Batch 73, Loss: 1.9058994054794312\n",
      "Epoch 2, Batch 74, Loss: 1.887139081954956\n",
      "Epoch 2, Batch 75, Loss: 1.881643533706665\n",
      "Epoch 2, Batch 76, Loss: 1.8985401391983032\n",
      "Epoch 2, Batch 77, Loss: 1.8927104473114014\n",
      "Epoch 2, Batch 78, Loss: 1.8882026672363281\n",
      "Epoch 2, Batch 79, Loss: 1.8864717483520508\n",
      "Epoch 2, Batch 80, Loss: 1.896864652633667\n",
      "Epoch 2, Batch 81, Loss: 1.8954017162322998\n",
      "Epoch 2, Batch 82, Loss: 1.8967632055282593\n",
      "Epoch 2, Batch 83, Loss: 1.864990234375\n",
      "Epoch 2, Batch 84, Loss: 1.8823041915893555\n",
      "Epoch 2, Batch 85, Loss: 1.8823902606964111\n",
      "Epoch 2, Batch 86, Loss: 1.8650230169296265\n",
      "Epoch 2, Batch 87, Loss: 1.8765935897827148\n",
      "Epoch 2, Batch 88, Loss: 1.8713665008544922\n",
      "Epoch 2, Batch 89, Loss: 1.873374581336975\n",
      "Epoch 2, Batch 90, Loss: 1.8732163906097412\n",
      "Epoch 2, Batch 91, Loss: 1.8574243783950806\n",
      "Epoch 2, Batch 92, Loss: 1.8735679388046265\n",
      "Epoch 2, Batch 93, Loss: 1.880733847618103\n",
      "Epoch 3, Batch 0, Loss: 1.8621704578399658\n",
      "Epoch 3, Batch 1, Loss: 1.8606510162353516\n",
      "Epoch 3, Batch 2, Loss: 1.8621594905853271\n",
      "Epoch 3, Batch 3, Loss: 1.8599554300308228\n",
      "Epoch 3, Batch 4, Loss: 1.848586082458496\n",
      "Epoch 3, Batch 5, Loss: 1.83878493309021\n",
      "Epoch 3, Batch 6, Loss: 1.8418052196502686\n",
      "Epoch 3, Batch 7, Loss: 1.8508508205413818\n",
      "Epoch 3, Batch 8, Loss: 1.8450692892074585\n",
      "Epoch 3, Batch 9, Loss: 1.8523142337799072\n",
      "Epoch 3, Batch 10, Loss: 1.8462251424789429\n",
      "Epoch 3, Batch 11, Loss: 1.8458549976348877\n",
      "Epoch 3, Batch 12, Loss: 1.8370298147201538\n",
      "Epoch 3, Batch 13, Loss: 1.8349672555923462\n",
      "Epoch 3, Batch 14, Loss: 1.8247168064117432\n",
      "Epoch 3, Batch 15, Loss: 1.8599952459335327\n",
      "Epoch 3, Batch 16, Loss: 1.8497610092163086\n",
      "Epoch 3, Batch 17, Loss: 1.8386625051498413\n",
      "Epoch 3, Batch 18, Loss: 1.8297580480575562\n",
      "Epoch 3, Batch 19, Loss: 1.841660499572754\n",
      "Epoch 3, Batch 20, Loss: 1.8367846012115479\n",
      "Epoch 3, Batch 21, Loss: 1.8253501653671265\n",
      "Epoch 3, Batch 22, Loss: 1.8343899250030518\n",
      "Epoch 3, Batch 23, Loss: 1.812517762184143\n",
      "Epoch 3, Batch 24, Loss: 1.825958013534546\n",
      "Epoch 3, Batch 25, Loss: 1.8206913471221924\n",
      "Epoch 3, Batch 26, Loss: 1.8150670528411865\n",
      "Epoch 3, Batch 27, Loss: 1.8219493627548218\n",
      "Epoch 3, Batch 28, Loss: 1.805489182472229\n",
      "Epoch 3, Batch 29, Loss: 1.8127788305282593\n",
      "Epoch 3, Batch 30, Loss: 1.8215138912200928\n",
      "Epoch 3, Batch 31, Loss: 1.8078511953353882\n",
      "Epoch 3, Batch 32, Loss: 1.794559121131897\n",
      "Epoch 3, Batch 33, Loss: 1.814387321472168\n",
      "Epoch 3, Batch 34, Loss: 1.8067067861557007\n",
      "Epoch 3, Batch 35, Loss: 1.7995296716690063\n",
      "Epoch 3, Batch 36, Loss: 1.778236985206604\n",
      "Epoch 3, Batch 37, Loss: 1.8014070987701416\n",
      "Epoch 3, Batch 38, Loss: 1.7982546091079712\n",
      "Epoch 3, Batch 39, Loss: 1.8149207830429077\n",
      "Epoch 3, Batch 40, Loss: 1.8295392990112305\n",
      "Epoch 3, Batch 41, Loss: 1.7973453998565674\n",
      "Epoch 3, Batch 42, Loss: 1.7884200811386108\n",
      "Epoch 3, Batch 43, Loss: 1.7905172109603882\n",
      "Epoch 3, Batch 44, Loss: 1.8008873462677002\n",
      "Epoch 3, Batch 45, Loss: 1.7694246768951416\n",
      "Epoch 3, Batch 46, Loss: 1.805198073387146\n",
      "Epoch 3, Batch 47, Loss: 1.7996044158935547\n",
      "Epoch 3, Batch 48, Loss: 1.7910916805267334\n",
      "Epoch 3, Batch 49, Loss: 1.7786808013916016\n",
      "Epoch 3, Batch 50, Loss: 1.7801326513290405\n",
      "Epoch 3, Batch 51, Loss: 1.7881135940551758\n",
      "Epoch 3, Batch 52, Loss: 1.780289649963379\n",
      "Epoch 3, Batch 53, Loss: 1.7708017826080322\n",
      "Epoch 3, Batch 54, Loss: 1.7852081060409546\n",
      "Epoch 3, Batch 55, Loss: 1.7662689685821533\n",
      "Epoch 3, Batch 56, Loss: 1.7729142904281616\n",
      "Epoch 3, Batch 57, Loss: 1.778700828552246\n",
      "Epoch 3, Batch 58, Loss: 1.7661969661712646\n",
      "Epoch 3, Batch 59, Loss: 1.7856807708740234\n",
      "Epoch 3, Batch 60, Loss: 1.7568715810775757\n",
      "Epoch 3, Batch 61, Loss: 1.782649040222168\n",
      "Epoch 3, Batch 62, Loss: 1.7744529247283936\n",
      "Epoch 3, Batch 63, Loss: 1.7654311656951904\n",
      "Epoch 3, Batch 64, Loss: 1.7840344905853271\n",
      "Epoch 3, Batch 65, Loss: 1.766033411026001\n",
      "Epoch 3, Batch 66, Loss: 1.7436720132827759\n",
      "Epoch 3, Batch 67, Loss: 1.760972023010254\n",
      "Epoch 3, Batch 68, Loss: 1.7311944961547852\n",
      "Epoch 3, Batch 69, Loss: 1.7437292337417603\n",
      "Epoch 3, Batch 70, Loss: 1.7457996606826782\n",
      "Epoch 3, Batch 71, Loss: 1.742954969406128\n",
      "Epoch 3, Batch 72, Loss: 1.743453025817871\n",
      "Epoch 3, Batch 73, Loss: 1.746948480606079\n",
      "Epoch 3, Batch 74, Loss: 1.726424217224121\n",
      "Epoch 3, Batch 75, Loss: 1.7293723821640015\n",
      "Epoch 3, Batch 76, Loss: 1.7460206747055054\n",
      "Epoch 3, Batch 77, Loss: 1.7288154363632202\n",
      "Epoch 3, Batch 78, Loss: 1.717781662940979\n",
      "Epoch 3, Batch 79, Loss: 1.7366390228271484\n",
      "Epoch 3, Batch 80, Loss: 1.7283527851104736\n",
      "Epoch 3, Batch 81, Loss: 1.7436068058013916\n",
      "Epoch 3, Batch 82, Loss: 1.7221612930297852\n",
      "Epoch 3, Batch 83, Loss: 1.7434742450714111\n",
      "Epoch 3, Batch 84, Loss: 1.7491023540496826\n",
      "Epoch 3, Batch 85, Loss: 1.715417504310608\n",
      "Epoch 3, Batch 86, Loss: 1.720807671546936\n",
      "Epoch 3, Batch 87, Loss: 1.7008039951324463\n",
      "Epoch 3, Batch 88, Loss: 1.7171474695205688\n",
      "Epoch 3, Batch 89, Loss: 1.7213621139526367\n",
      "Epoch 3, Batch 90, Loss: 1.7123864889144897\n",
      "Epoch 3, Batch 91, Loss: 1.6887201070785522\n",
      "Epoch 3, Batch 92, Loss: 1.7330844402313232\n",
      "Epoch 3, Batch 93, Loss: 1.712088942527771\n",
      "Epoch 4, Batch 0, Loss: 1.7040116786956787\n",
      "Epoch 4, Batch 1, Loss: 1.750734567642212\n",
      "Epoch 4, Batch 2, Loss: 1.6762555837631226\n",
      "Epoch 4, Batch 3, Loss: 1.717124581336975\n",
      "Epoch 4, Batch 4, Loss: 1.70663583278656\n",
      "Epoch 4, Batch 5, Loss: 1.7043664455413818\n",
      "Epoch 4, Batch 6, Loss: 1.7074905633926392\n",
      "Epoch 4, Batch 7, Loss: 1.6759073734283447\n",
      "Epoch 4, Batch 8, Loss: 1.715376615524292\n",
      "Epoch 4, Batch 9, Loss: 1.7077445983886719\n",
      "Epoch 4, Batch 10, Loss: 1.6784563064575195\n",
      "Epoch 4, Batch 11, Loss: 1.6908605098724365\n",
      "Epoch 4, Batch 12, Loss: 1.6574862003326416\n",
      "Epoch 4, Batch 13, Loss: 1.6776825189590454\n",
      "Epoch 4, Batch 14, Loss: 1.6889721155166626\n",
      "Epoch 4, Batch 15, Loss: 1.7025362253189087\n",
      "Epoch 4, Batch 16, Loss: 1.706167459487915\n",
      "Epoch 4, Batch 17, Loss: 1.696045160293579\n",
      "Epoch 4, Batch 18, Loss: 1.6996742486953735\n",
      "Epoch 4, Batch 19, Loss: 1.689732551574707\n",
      "Epoch 4, Batch 20, Loss: 1.6621137857437134\n",
      "Epoch 4, Batch 21, Loss: 1.6905899047851562\n",
      "Epoch 4, Batch 22, Loss: 1.6709859371185303\n",
      "Epoch 4, Batch 23, Loss: 1.6667678356170654\n",
      "Epoch 4, Batch 24, Loss: 1.672355055809021\n",
      "Epoch 4, Batch 25, Loss: 1.6721611022949219\n",
      "Epoch 4, Batch 26, Loss: 1.6636903285980225\n",
      "Epoch 4, Batch 27, Loss: 1.6669174432754517\n",
      "Epoch 4, Batch 28, Loss: 1.650668740272522\n",
      "Epoch 4, Batch 29, Loss: 1.6822763681411743\n",
      "Epoch 4, Batch 30, Loss: 1.640181303024292\n",
      "Epoch 4, Batch 31, Loss: 1.642465353012085\n",
      "Epoch 4, Batch 32, Loss: 1.6560875177383423\n",
      "Epoch 4, Batch 33, Loss: 1.6541436910629272\n",
      "Epoch 4, Batch 34, Loss: 1.6392488479614258\n",
      "Epoch 4, Batch 35, Loss: 1.650586485862732\n",
      "Epoch 4, Batch 36, Loss: 1.6495411396026611\n",
      "Epoch 4, Batch 37, Loss: 1.654963731765747\n",
      "Epoch 4, Batch 38, Loss: 1.6402546167373657\n",
      "Epoch 4, Batch 39, Loss: 1.6612848043441772\n",
      "Epoch 4, Batch 40, Loss: 1.6417315006256104\n",
      "Epoch 4, Batch 41, Loss: 1.6292682886123657\n",
      "Epoch 4, Batch 42, Loss: 1.6303688287734985\n",
      "Epoch 4, Batch 43, Loss: 1.6549766063690186\n",
      "Epoch 4, Batch 44, Loss: 1.6483490467071533\n",
      "Epoch 4, Batch 45, Loss: 1.6296510696411133\n",
      "Epoch 4, Batch 46, Loss: 1.6142816543579102\n",
      "Epoch 4, Batch 47, Loss: 1.6222032308578491\n",
      "Epoch 4, Batch 48, Loss: 1.63511061668396\n",
      "Epoch 4, Batch 49, Loss: 1.6535907983779907\n",
      "Epoch 4, Batch 50, Loss: 1.63572096824646\n",
      "Epoch 4, Batch 51, Loss: 1.6474955081939697\n",
      "Epoch 4, Batch 52, Loss: 1.6370112895965576\n",
      "Epoch 4, Batch 53, Loss: 1.6243864297866821\n",
      "Epoch 4, Batch 54, Loss: 1.6329600811004639\n",
      "Epoch 4, Batch 55, Loss: 1.6195281744003296\n",
      "Epoch 4, Batch 56, Loss: 1.6333484649658203\n",
      "Epoch 4, Batch 57, Loss: 1.6195122003555298\n",
      "Epoch 4, Batch 58, Loss: 1.5967261791229248\n",
      "Epoch 4, Batch 59, Loss: 1.6420156955718994\n",
      "Epoch 4, Batch 60, Loss: 1.6094707250595093\n",
      "Epoch 4, Batch 61, Loss: 1.6160532236099243\n",
      "Epoch 4, Batch 62, Loss: 1.6327478885650635\n",
      "Epoch 4, Batch 63, Loss: 1.611331582069397\n",
      "Epoch 4, Batch 64, Loss: 1.616660475730896\n",
      "Epoch 4, Batch 65, Loss: 1.5916893482208252\n",
      "Epoch 4, Batch 66, Loss: 1.6247577667236328\n",
      "Epoch 4, Batch 67, Loss: 1.5692540407180786\n",
      "Epoch 4, Batch 68, Loss: 1.610687255859375\n",
      "Epoch 4, Batch 69, Loss: 1.6129693984985352\n",
      "Epoch 4, Batch 70, Loss: 1.5750410556793213\n",
      "Epoch 4, Batch 71, Loss: 1.607511281967163\n",
      "Epoch 4, Batch 72, Loss: 1.6000839471817017\n",
      "Epoch 4, Batch 73, Loss: 1.6012322902679443\n",
      "Epoch 4, Batch 74, Loss: 1.6007620096206665\n",
      "Epoch 4, Batch 75, Loss: 1.600701093673706\n",
      "Epoch 4, Batch 76, Loss: 1.6030880212783813\n",
      "Epoch 4, Batch 77, Loss: 1.5873204469680786\n",
      "Epoch 4, Batch 78, Loss: 1.580519437789917\n",
      "Epoch 4, Batch 79, Loss: 1.5823770761489868\n",
      "Epoch 4, Batch 80, Loss: 1.6148889064788818\n",
      "Epoch 4, Batch 81, Loss: 1.5886332988739014\n",
      "Epoch 4, Batch 82, Loss: 1.56879460811615\n",
      "Epoch 4, Batch 83, Loss: 1.5740784406661987\n",
      "Epoch 4, Batch 84, Loss: 1.589689016342163\n",
      "Epoch 4, Batch 85, Loss: 1.5598018169403076\n",
      "Epoch 4, Batch 86, Loss: 1.5675225257873535\n",
      "Epoch 4, Batch 87, Loss: 1.559899091720581\n",
      "Epoch 4, Batch 88, Loss: 1.588278889656067\n",
      "Epoch 4, Batch 89, Loss: 1.603980302810669\n",
      "Epoch 4, Batch 90, Loss: 1.54802668094635\n",
      "Epoch 4, Batch 91, Loss: 1.5287201404571533\n",
      "Epoch 4, Batch 92, Loss: 1.5943222045898438\n",
      "Epoch 4, Batch 93, Loss: 1.5492689609527588\n",
      "Epoch 5, Batch 0, Loss: 1.5843055248260498\n",
      "Epoch 5, Batch 1, Loss: 1.5556048154830933\n",
      "Epoch 5, Batch 2, Loss: 1.5470607280731201\n",
      "Epoch 5, Batch 3, Loss: 1.5683369636535645\n",
      "Epoch 5, Batch 4, Loss: 1.554688811302185\n",
      "Epoch 5, Batch 5, Loss: 1.5390701293945312\n",
      "Epoch 5, Batch 6, Loss: 1.5707091093063354\n",
      "Epoch 5, Batch 7, Loss: 1.5525081157684326\n",
      "Epoch 5, Batch 8, Loss: 1.5598689317703247\n",
      "Epoch 5, Batch 9, Loss: 1.5657050609588623\n",
      "Epoch 5, Batch 10, Loss: 1.5711660385131836\n",
      "Epoch 5, Batch 11, Loss: 1.5561072826385498\n",
      "Epoch 5, Batch 12, Loss: 1.5644807815551758\n",
      "Epoch 5, Batch 13, Loss: 1.547558069229126\n",
      "Epoch 5, Batch 14, Loss: 1.5234127044677734\n",
      "Epoch 5, Batch 15, Loss: 1.5352766513824463\n",
      "Epoch 5, Batch 16, Loss: 1.5643123388290405\n",
      "Epoch 5, Batch 17, Loss: 1.5298950672149658\n",
      "Epoch 5, Batch 18, Loss: 1.5515830516815186\n",
      "Epoch 5, Batch 19, Loss: 1.5162465572357178\n",
      "Epoch 5, Batch 20, Loss: 1.523457407951355\n",
      "Epoch 5, Batch 21, Loss: 1.5325727462768555\n",
      "Epoch 5, Batch 22, Loss: 1.506603479385376\n",
      "Epoch 5, Batch 23, Loss: 1.5483607053756714\n",
      "Epoch 5, Batch 24, Loss: 1.5216314792633057\n",
      "Epoch 5, Batch 25, Loss: 1.546023964881897\n",
      "Epoch 5, Batch 26, Loss: 1.540210485458374\n",
      "Epoch 5, Batch 27, Loss: 1.4925289154052734\n",
      "Epoch 5, Batch 28, Loss: 1.5189740657806396\n",
      "Epoch 5, Batch 29, Loss: 1.5385996103286743\n",
      "Epoch 5, Batch 30, Loss: 1.5211224555969238\n",
      "Epoch 5, Batch 31, Loss: 1.5391510725021362\n",
      "Epoch 5, Batch 32, Loss: 1.540071725845337\n",
      "Epoch 5, Batch 33, Loss: 1.5065959692001343\n",
      "Epoch 5, Batch 34, Loss: 1.509556531906128\n",
      "Epoch 5, Batch 35, Loss: 1.4872453212738037\n",
      "Epoch 5, Batch 36, Loss: 1.5217857360839844\n",
      "Epoch 5, Batch 37, Loss: 1.478927731513977\n",
      "Epoch 5, Batch 38, Loss: 1.5034277439117432\n",
      "Epoch 5, Batch 39, Loss: 1.5231692790985107\n",
      "Epoch 5, Batch 40, Loss: 1.516828179359436\n",
      "Epoch 5, Batch 41, Loss: 1.4715927839279175\n",
      "Epoch 5, Batch 42, Loss: 1.5086731910705566\n",
      "Epoch 5, Batch 43, Loss: 1.5310572385787964\n",
      "Epoch 5, Batch 44, Loss: 1.5149569511413574\n",
      "Epoch 5, Batch 45, Loss: 1.4737958908081055\n",
      "Epoch 5, Batch 46, Loss: 1.456700086593628\n",
      "Epoch 5, Batch 47, Loss: 1.4651654958724976\n",
      "Epoch 5, Batch 48, Loss: 1.4794838428497314\n",
      "Epoch 5, Batch 49, Loss: 1.4651449918746948\n",
      "Epoch 5, Batch 50, Loss: 1.4607808589935303\n",
      "Epoch 5, Batch 51, Loss: 1.4772511720657349\n",
      "Epoch 5, Batch 52, Loss: 1.523158073425293\n",
      "Epoch 5, Batch 53, Loss: 1.4430125951766968\n",
      "Epoch 5, Batch 54, Loss: 1.496626615524292\n",
      "Epoch 5, Batch 55, Loss: 1.5003414154052734\n",
      "Epoch 5, Batch 56, Loss: 1.484725832939148\n",
      "Epoch 5, Batch 57, Loss: 1.4674451351165771\n",
      "Epoch 5, Batch 58, Loss: 1.488175630569458\n",
      "Epoch 5, Batch 59, Loss: 1.489989995956421\n",
      "Epoch 5, Batch 60, Loss: 1.4639931917190552\n",
      "Epoch 5, Batch 61, Loss: 1.4432176351547241\n",
      "Epoch 5, Batch 62, Loss: 1.4480807781219482\n",
      "Epoch 5, Batch 63, Loss: 1.4663485288619995\n",
      "Epoch 5, Batch 64, Loss: 1.4782624244689941\n",
      "Epoch 5, Batch 65, Loss: 1.5051155090332031\n",
      "Epoch 5, Batch 66, Loss: 1.4766430854797363\n",
      "Epoch 5, Batch 67, Loss: 1.4533977508544922\n",
      "Epoch 5, Batch 68, Loss: 1.4669822454452515\n",
      "Epoch 5, Batch 69, Loss: 1.4702755212783813\n",
      "Epoch 5, Batch 70, Loss: 1.4616185426712036\n",
      "Epoch 5, Batch 71, Loss: 1.4561669826507568\n",
      "Epoch 5, Batch 72, Loss: 1.4517790079116821\n",
      "Epoch 5, Batch 73, Loss: 1.4485150575637817\n",
      "Epoch 5, Batch 74, Loss: 1.4524370431900024\n",
      "Epoch 5, Batch 75, Loss: 1.4312838315963745\n",
      "Epoch 5, Batch 76, Loss: 1.4589203596115112\n",
      "Epoch 5, Batch 77, Loss: 1.4657800197601318\n",
      "Epoch 5, Batch 78, Loss: 1.4487944841384888\n",
      "Epoch 5, Batch 79, Loss: 1.4318501949310303\n",
      "Epoch 5, Batch 80, Loss: 1.4667056798934937\n",
      "Epoch 5, Batch 81, Loss: 1.4689185619354248\n",
      "Epoch 5, Batch 82, Loss: 1.397482991218567\n",
      "Epoch 5, Batch 83, Loss: 1.4358079433441162\n",
      "Epoch 5, Batch 84, Loss: 1.421455979347229\n",
      "Epoch 5, Batch 85, Loss: 1.4344456195831299\n",
      "Epoch 5, Batch 86, Loss: 1.429376482963562\n",
      "Epoch 5, Batch 87, Loss: 1.4486035108566284\n",
      "Epoch 5, Batch 88, Loss: 1.4186145067214966\n",
      "Epoch 5, Batch 89, Loss: 1.4579790830612183\n",
      "Epoch 5, Batch 90, Loss: 1.4370043277740479\n",
      "Epoch 5, Batch 91, Loss: 1.426741361618042\n",
      "Epoch 5, Batch 92, Loss: 1.4242291450500488\n",
      "Epoch 5, Batch 93, Loss: 1.4527933597564697\n",
      "Epoch 6, Batch 0, Loss: 1.4191499948501587\n",
      "Epoch 6, Batch 1, Loss: 1.4448978900909424\n",
      "Epoch 6, Batch 2, Loss: 1.4062317609786987\n",
      "Epoch 6, Batch 3, Loss: 1.422027587890625\n",
      "Epoch 6, Batch 4, Loss: 1.4473639726638794\n",
      "Epoch 6, Batch 5, Loss: 1.4299638271331787\n",
      "Epoch 6, Batch 6, Loss: 1.4472955465316772\n",
      "Epoch 6, Batch 7, Loss: 1.418541669845581\n",
      "Epoch 6, Batch 8, Loss: 1.38835871219635\n",
      "Epoch 6, Batch 9, Loss: 1.403793215751648\n",
      "Epoch 6, Batch 10, Loss: 1.4439274072647095\n",
      "Epoch 6, Batch 11, Loss: 1.4060018062591553\n",
      "Epoch 6, Batch 12, Loss: 1.3922321796417236\n",
      "Epoch 6, Batch 13, Loss: 1.4028937816619873\n",
      "Epoch 6, Batch 14, Loss: 1.420215368270874\n",
      "Epoch 6, Batch 15, Loss: 1.391270637512207\n",
      "Epoch 6, Batch 16, Loss: 1.412200689315796\n",
      "Epoch 6, Batch 17, Loss: 1.4015295505523682\n",
      "Epoch 6, Batch 18, Loss: 1.4082776308059692\n",
      "Epoch 6, Batch 19, Loss: 1.3677915334701538\n",
      "Epoch 6, Batch 20, Loss: 1.3829991817474365\n",
      "Epoch 6, Batch 21, Loss: 1.3853647708892822\n",
      "Epoch 6, Batch 22, Loss: 1.3824164867401123\n",
      "Epoch 6, Batch 23, Loss: 1.4153556823730469\n",
      "Epoch 6, Batch 24, Loss: 1.4147871732711792\n",
      "Epoch 6, Batch 25, Loss: 1.3836244344711304\n",
      "Epoch 6, Batch 26, Loss: 1.390297293663025\n",
      "Epoch 6, Batch 27, Loss: 1.4222835302352905\n",
      "Epoch 6, Batch 28, Loss: 1.3987202644348145\n",
      "Epoch 6, Batch 29, Loss: 1.4147255420684814\n",
      "Epoch 6, Batch 30, Loss: 1.3694288730621338\n",
      "Epoch 6, Batch 31, Loss: 1.3920236825942993\n",
      "Epoch 6, Batch 32, Loss: 1.353649616241455\n",
      "Epoch 6, Batch 33, Loss: 1.3813318014144897\n",
      "Epoch 6, Batch 34, Loss: 1.3714666366577148\n",
      "Epoch 6, Batch 35, Loss: 1.3767110109329224\n",
      "Epoch 6, Batch 36, Loss: 1.4146366119384766\n",
      "Epoch 6, Batch 37, Loss: 1.3703378438949585\n",
      "Epoch 6, Batch 38, Loss: 1.3939493894577026\n",
      "Epoch 6, Batch 39, Loss: 1.4003998041152954\n",
      "Epoch 6, Batch 40, Loss: 1.3620822429656982\n",
      "Epoch 6, Batch 41, Loss: 1.3374518156051636\n",
      "Epoch 6, Batch 42, Loss: 1.3737757205963135\n",
      "Epoch 6, Batch 43, Loss: 1.3561185598373413\n",
      "Epoch 6, Batch 44, Loss: 1.3666114807128906\n",
      "Epoch 6, Batch 45, Loss: 1.3482943773269653\n",
      "Epoch 6, Batch 46, Loss: 1.3454946279525757\n",
      "Epoch 6, Batch 47, Loss: 1.355043888092041\n",
      "Epoch 6, Batch 48, Loss: 1.3768916130065918\n",
      "Epoch 6, Batch 49, Loss: 1.3558021783828735\n",
      "Epoch 6, Batch 50, Loss: 1.3377959728240967\n",
      "Epoch 6, Batch 51, Loss: 1.3869445323944092\n",
      "Epoch 6, Batch 52, Loss: 1.3623912334442139\n",
      "Epoch 6, Batch 53, Loss: 1.366018295288086\n",
      "Epoch 6, Batch 54, Loss: 1.366349458694458\n",
      "Epoch 6, Batch 55, Loss: 1.382276177406311\n",
      "Epoch 6, Batch 56, Loss: 1.343212366104126\n",
      "Epoch 6, Batch 57, Loss: 1.342954397201538\n",
      "Epoch 6, Batch 58, Loss: 1.3468481302261353\n",
      "Epoch 6, Batch 59, Loss: 1.3311935663223267\n",
      "Epoch 6, Batch 60, Loss: 1.3522740602493286\n",
      "Epoch 6, Batch 61, Loss: 1.3601791858673096\n",
      "Epoch 6, Batch 62, Loss: 1.3457236289978027\n",
      "Epoch 6, Batch 63, Loss: 1.3502041101455688\n",
      "Epoch 6, Batch 64, Loss: 1.357229232788086\n",
      "Epoch 6, Batch 65, Loss: 1.3406240940093994\n",
      "Epoch 6, Batch 66, Loss: 1.3596150875091553\n",
      "Epoch 6, Batch 67, Loss: 1.3331546783447266\n",
      "Epoch 6, Batch 68, Loss: 1.336747646331787\n",
      "Epoch 6, Batch 69, Loss: 1.3849637508392334\n",
      "Epoch 6, Batch 70, Loss: 1.3270115852355957\n",
      "Epoch 6, Batch 71, Loss: 1.306177020072937\n",
      "Epoch 6, Batch 72, Loss: 1.308239221572876\n",
      "Epoch 6, Batch 73, Loss: 1.3439300060272217\n",
      "Epoch 6, Batch 74, Loss: 1.3079521656036377\n",
      "Epoch 6, Batch 75, Loss: 1.3337794542312622\n",
      "Epoch 6, Batch 76, Loss: 1.32550847530365\n",
      "Epoch 6, Batch 77, Loss: 1.3097156286239624\n",
      "Epoch 6, Batch 78, Loss: 1.298675298690796\n",
      "Epoch 6, Batch 79, Loss: 1.335444450378418\n",
      "Epoch 6, Batch 80, Loss: 1.3211324214935303\n",
      "Epoch 6, Batch 81, Loss: 1.3425393104553223\n",
      "Epoch 6, Batch 82, Loss: 1.312255859375\n",
      "Epoch 6, Batch 83, Loss: 1.3122740983963013\n",
      "Epoch 6, Batch 84, Loss: 1.2727042436599731\n",
      "Epoch 6, Batch 85, Loss: 1.314240574836731\n",
      "Epoch 6, Batch 86, Loss: 1.3261604309082031\n",
      "Epoch 6, Batch 87, Loss: 1.3112518787384033\n",
      "Epoch 6, Batch 88, Loss: 1.279586911201477\n",
      "Epoch 6, Batch 89, Loss: 1.326465368270874\n",
      "Epoch 6, Batch 90, Loss: 1.2961080074310303\n",
      "Epoch 6, Batch 91, Loss: 1.3234444856643677\n",
      "Epoch 6, Batch 92, Loss: 1.2989983558654785\n",
      "Epoch 6, Batch 93, Loss: 1.2711098194122314\n",
      "Epoch 7, Batch 0, Loss: 1.3204978704452515\n",
      "Epoch 7, Batch 1, Loss: 1.3271517753601074\n",
      "Epoch 7, Batch 2, Loss: 1.3318846225738525\n",
      "Epoch 7, Batch 3, Loss: 1.3392603397369385\n",
      "Epoch 7, Batch 4, Loss: 1.2659140825271606\n",
      "Epoch 7, Batch 5, Loss: 1.3009765148162842\n",
      "Epoch 7, Batch 6, Loss: 1.3118202686309814\n",
      "Epoch 7, Batch 7, Loss: 1.3083505630493164\n",
      "Epoch 7, Batch 8, Loss: 1.284249186515808\n",
      "Epoch 7, Batch 9, Loss: 1.295926809310913\n",
      "Epoch 7, Batch 10, Loss: 1.2913216352462769\n",
      "Epoch 7, Batch 11, Loss: 1.327646255493164\n",
      "Epoch 7, Batch 12, Loss: 1.2978991270065308\n",
      "Epoch 7, Batch 13, Loss: 1.2835004329681396\n",
      "Epoch 7, Batch 14, Loss: 1.2785825729370117\n",
      "Epoch 7, Batch 15, Loss: 1.2809865474700928\n",
      "Epoch 7, Batch 16, Loss: 1.285571575164795\n",
      "Epoch 7, Batch 17, Loss: 1.2843502759933472\n",
      "Epoch 7, Batch 18, Loss: 1.2903378009796143\n",
      "Epoch 7, Batch 19, Loss: 1.2878246307373047\n",
      "Epoch 7, Batch 20, Loss: 1.2727198600769043\n",
      "Epoch 7, Batch 21, Loss: 1.2793335914611816\n",
      "Epoch 7, Batch 22, Loss: 1.297755241394043\n",
      "Epoch 7, Batch 23, Loss: 1.267383337020874\n",
      "Epoch 7, Batch 24, Loss: 1.2954351902008057\n",
      "Epoch 7, Batch 25, Loss: 1.254551649093628\n",
      "Epoch 7, Batch 26, Loss: 1.295194149017334\n",
      "Epoch 7, Batch 27, Loss: 1.2814873456954956\n",
      "Epoch 7, Batch 28, Loss: 1.2617003917694092\n",
      "Epoch 7, Batch 29, Loss: 1.2542202472686768\n",
      "Epoch 7, Batch 30, Loss: 1.2630915641784668\n",
      "Epoch 7, Batch 31, Loss: 1.2830580472946167\n",
      "Epoch 7, Batch 32, Loss: 1.2904317378997803\n",
      "Epoch 7, Batch 33, Loss: 1.267438292503357\n",
      "Epoch 7, Batch 34, Loss: 1.2732704877853394\n",
      "Epoch 7, Batch 35, Loss: 1.2486915588378906\n",
      "Epoch 7, Batch 36, Loss: 1.2728993892669678\n",
      "Epoch 7, Batch 37, Loss: 1.2933132648468018\n",
      "Epoch 7, Batch 38, Loss: 1.2911813259124756\n",
      "Epoch 7, Batch 39, Loss: 1.2436628341674805\n",
      "Epoch 7, Batch 40, Loss: 1.2013273239135742\n",
      "Epoch 7, Batch 41, Loss: 1.2482833862304688\n",
      "Epoch 7, Batch 42, Loss: 1.2538564205169678\n",
      "Epoch 7, Batch 43, Loss: 1.2570081949234009\n",
      "Epoch 7, Batch 44, Loss: 1.2875792980194092\n",
      "Epoch 7, Batch 45, Loss: 1.222346305847168\n",
      "Epoch 7, Batch 46, Loss: 1.240876317024231\n",
      "Epoch 7, Batch 47, Loss: 1.2390358448028564\n",
      "Epoch 7, Batch 48, Loss: 1.2684953212738037\n",
      "Epoch 7, Batch 49, Loss: 1.2253860235214233\n",
      "Epoch 7, Batch 50, Loss: 1.2813379764556885\n",
      "Epoch 7, Batch 51, Loss: 1.2849739789962769\n",
      "Epoch 7, Batch 52, Loss: 1.225952386856079\n",
      "Epoch 7, Batch 53, Loss: 1.2395374774932861\n",
      "Epoch 7, Batch 54, Loss: 1.2474547624588013\n",
      "Epoch 7, Batch 55, Loss: 1.2310514450073242\n",
      "Epoch 7, Batch 56, Loss: 1.237451195716858\n",
      "Epoch 7, Batch 57, Loss: 1.2193559408187866\n",
      "Epoch 7, Batch 58, Loss: 1.2154861688613892\n",
      "Epoch 7, Batch 59, Loss: 1.2049603462219238\n",
      "Epoch 7, Batch 60, Loss: 1.236314058303833\n",
      "Epoch 7, Batch 61, Loss: 1.2366193532943726\n",
      "Epoch 7, Batch 62, Loss: 1.2194640636444092\n",
      "Epoch 7, Batch 63, Loss: 1.2408190965652466\n",
      "Epoch 7, Batch 64, Loss: 1.198023796081543\n",
      "Epoch 7, Batch 65, Loss: 1.2306278944015503\n",
      "Epoch 7, Batch 66, Loss: 1.235771894454956\n",
      "Epoch 7, Batch 67, Loss: 1.2441327571868896\n",
      "Epoch 7, Batch 68, Loss: 1.2534608840942383\n",
      "Epoch 7, Batch 69, Loss: 1.2248518466949463\n",
      "Epoch 7, Batch 70, Loss: 1.224496841430664\n",
      "Epoch 7, Batch 71, Loss: 1.2132351398468018\n",
      "Epoch 7, Batch 72, Loss: 1.1797988414764404\n",
      "Epoch 7, Batch 73, Loss: 1.2009549140930176\n",
      "Epoch 7, Batch 74, Loss: 1.223673939704895\n",
      "Epoch 7, Batch 75, Loss: 1.2043845653533936\n",
      "Epoch 7, Batch 76, Loss: 1.1910161972045898\n",
      "Epoch 7, Batch 77, Loss: 1.2136967182159424\n",
      "Epoch 7, Batch 78, Loss: 1.20992112159729\n",
      "Epoch 7, Batch 79, Loss: 1.222144365310669\n",
      "Epoch 7, Batch 80, Loss: 1.1798129081726074\n",
      "Epoch 7, Batch 81, Loss: 1.192695140838623\n",
      "Epoch 7, Batch 82, Loss: 1.193240761756897\n",
      "Epoch 7, Batch 83, Loss: 1.237365961074829\n",
      "Epoch 7, Batch 84, Loss: 1.202344298362732\n",
      "Epoch 7, Batch 85, Loss: 1.1983850002288818\n",
      "Epoch 7, Batch 86, Loss: 1.2117881774902344\n",
      "Epoch 7, Batch 87, Loss: 1.230769395828247\n",
      "Epoch 7, Batch 88, Loss: 1.143314242362976\n",
      "Epoch 7, Batch 89, Loss: 1.1625361442565918\n",
      "Epoch 7, Batch 90, Loss: 1.2131750583648682\n",
      "Epoch 7, Batch 91, Loss: 1.2348445653915405\n",
      "Epoch 7, Batch 92, Loss: 1.1781140565872192\n",
      "Epoch 7, Batch 93, Loss: 1.1976009607315063\n",
      "Epoch 8, Batch 0, Loss: 1.2090691328048706\n",
      "Epoch 8, Batch 1, Loss: 1.1819692850112915\n",
      "Epoch 8, Batch 2, Loss: 1.1625416278839111\n",
      "Epoch 8, Batch 3, Loss: 1.2112252712249756\n",
      "Epoch 8, Batch 4, Loss: 1.1685017347335815\n",
      "Epoch 8, Batch 5, Loss: 1.1938985586166382\n",
      "Epoch 8, Batch 6, Loss: 1.1780235767364502\n",
      "Epoch 8, Batch 7, Loss: 1.2043732404708862\n",
      "Epoch 8, Batch 8, Loss: 1.2135246992111206\n",
      "Epoch 8, Batch 9, Loss: 1.198967695236206\n",
      "Epoch 8, Batch 10, Loss: 1.1795378923416138\n",
      "Epoch 8, Batch 11, Loss: 1.1668709516525269\n",
      "Epoch 8, Batch 12, Loss: 1.1696341037750244\n",
      "Epoch 8, Batch 13, Loss: 1.157566785812378\n",
      "Epoch 8, Batch 14, Loss: 1.1665136814117432\n",
      "Epoch 8, Batch 15, Loss: 1.1848442554473877\n",
      "Epoch 8, Batch 16, Loss: 1.1780694723129272\n",
      "Epoch 8, Batch 17, Loss: 1.1775646209716797\n",
      "Epoch 8, Batch 18, Loss: 1.1765929460525513\n",
      "Epoch 8, Batch 19, Loss: 1.1749311685562134\n",
      "Epoch 8, Batch 20, Loss: 1.1727626323699951\n",
      "Epoch 8, Batch 21, Loss: 1.1632955074310303\n",
      "Epoch 8, Batch 22, Loss: 1.1686367988586426\n",
      "Epoch 8, Batch 23, Loss: 1.1561623811721802\n",
      "Epoch 8, Batch 24, Loss: 1.1888535022735596\n",
      "Epoch 8, Batch 25, Loss: 1.2154037952423096\n",
      "Epoch 8, Batch 26, Loss: 1.1774816513061523\n",
      "Epoch 8, Batch 27, Loss: 1.1807693243026733\n",
      "Epoch 8, Batch 28, Loss: 1.1914747953414917\n",
      "Epoch 8, Batch 29, Loss: 1.1882808208465576\n",
      "Epoch 8, Batch 30, Loss: 1.1905310153961182\n",
      "Epoch 8, Batch 31, Loss: 1.1487500667572021\n",
      "Epoch 8, Batch 32, Loss: 1.1676523685455322\n",
      "Epoch 8, Batch 33, Loss: 1.1399320363998413\n",
      "Epoch 8, Batch 34, Loss: 1.157193660736084\n",
      "Epoch 8, Batch 35, Loss: 1.1584219932556152\n",
      "Epoch 8, Batch 36, Loss: 1.1674487590789795\n",
      "Epoch 8, Batch 37, Loss: 1.1806045770645142\n",
      "Epoch 8, Batch 38, Loss: 1.150400161743164\n",
      "Epoch 8, Batch 39, Loss: 1.1229718923568726\n",
      "Epoch 8, Batch 40, Loss: 1.146850824356079\n",
      "Epoch 8, Batch 41, Loss: 1.188072919845581\n",
      "Epoch 8, Batch 42, Loss: 1.1723514795303345\n",
      "Epoch 8, Batch 43, Loss: 1.1170427799224854\n",
      "Epoch 8, Batch 44, Loss: 1.133355736732483\n",
      "Epoch 8, Batch 45, Loss: 1.1471749544143677\n",
      "Epoch 8, Batch 46, Loss: 1.1389849185943604\n",
      "Epoch 8, Batch 47, Loss: 1.1792340278625488\n",
      "Epoch 8, Batch 48, Loss: 1.1493510007858276\n",
      "Epoch 8, Batch 49, Loss: 1.1590404510498047\n",
      "Epoch 8, Batch 50, Loss: 1.1062257289886475\n",
      "Epoch 8, Batch 51, Loss: 1.1732823848724365\n",
      "Epoch 8, Batch 52, Loss: 1.1576225757598877\n",
      "Epoch 8, Batch 53, Loss: 1.0925204753875732\n",
      "Epoch 8, Batch 54, Loss: 1.1491925716400146\n",
      "Epoch 8, Batch 55, Loss: 1.119476556777954\n",
      "Epoch 8, Batch 56, Loss: 1.1656848192214966\n",
      "Epoch 8, Batch 57, Loss: 1.1117756366729736\n",
      "Epoch 8, Batch 58, Loss: 1.115405559539795\n",
      "Epoch 8, Batch 59, Loss: 1.1289594173431396\n",
      "Epoch 8, Batch 60, Loss: 1.163602590560913\n",
      "Epoch 8, Batch 61, Loss: 1.1393178701400757\n",
      "Epoch 8, Batch 62, Loss: 1.1267516613006592\n",
      "Epoch 8, Batch 63, Loss: 1.1542413234710693\n",
      "Epoch 8, Batch 64, Loss: 1.1285892724990845\n",
      "Epoch 8, Batch 65, Loss: 1.120311975479126\n",
      "Epoch 8, Batch 66, Loss: 1.1324799060821533\n",
      "Epoch 8, Batch 67, Loss: 1.1100690364837646\n",
      "Epoch 8, Batch 68, Loss: 1.1307703256607056\n",
      "Epoch 8, Batch 69, Loss: 1.1080577373504639\n",
      "Epoch 8, Batch 70, Loss: 1.1349414587020874\n",
      "Epoch 8, Batch 71, Loss: 1.1420390605926514\n",
      "Epoch 8, Batch 72, Loss: 1.1223595142364502\n",
      "Epoch 8, Batch 73, Loss: 1.1479381322860718\n",
      "Epoch 8, Batch 74, Loss: 1.1363369226455688\n",
      "Epoch 8, Batch 75, Loss: 1.1469159126281738\n",
      "Epoch 8, Batch 76, Loss: 1.1213200092315674\n",
      "Epoch 8, Batch 77, Loss: 1.1416614055633545\n",
      "Epoch 8, Batch 78, Loss: 1.1080453395843506\n",
      "Epoch 8, Batch 79, Loss: 1.1524550914764404\n",
      "Epoch 8, Batch 80, Loss: 1.1354351043701172\n",
      "Epoch 8, Batch 81, Loss: 1.1061911582946777\n",
      "Epoch 8, Batch 82, Loss: 1.1164764165878296\n",
      "Epoch 8, Batch 83, Loss: 1.0973479747772217\n",
      "Epoch 8, Batch 84, Loss: 1.1412718296051025\n",
      "Epoch 8, Batch 85, Loss: 1.1238620281219482\n",
      "Epoch 8, Batch 86, Loss: 1.1285346746444702\n",
      "Epoch 8, Batch 87, Loss: 1.1138083934783936\n",
      "Epoch 8, Batch 88, Loss: 1.1206597089767456\n",
      "Epoch 8, Batch 89, Loss: 1.1148828268051147\n",
      "Epoch 8, Batch 90, Loss: 1.0849714279174805\n",
      "Epoch 8, Batch 91, Loss: 1.1090009212493896\n",
      "Epoch 8, Batch 92, Loss: 1.1263494491577148\n",
      "Epoch 8, Batch 93, Loss: 1.0785890817642212\n",
      "Epoch 9, Batch 0, Loss: 1.1209909915924072\n",
      "Epoch 9, Batch 1, Loss: 1.099137306213379\n",
      "Epoch 9, Batch 2, Loss: 1.096233606338501\n",
      "Epoch 9, Batch 3, Loss: 1.1086589097976685\n",
      "Epoch 9, Batch 4, Loss: 1.1040886640548706\n",
      "Epoch 9, Batch 5, Loss: 1.1024200916290283\n",
      "Epoch 9, Batch 6, Loss: 1.109815001487732\n",
      "Epoch 9, Batch 7, Loss: 1.1036957502365112\n",
      "Epoch 9, Batch 8, Loss: 1.14052414894104\n",
      "Epoch 9, Batch 9, Loss: 1.0712809562683105\n",
      "Epoch 9, Batch 10, Loss: 1.0676194429397583\n",
      "Epoch 9, Batch 11, Loss: 1.1018011569976807\n",
      "Epoch 9, Batch 12, Loss: 1.1246106624603271\n",
      "Epoch 9, Batch 13, Loss: 1.1061979532241821\n",
      "Epoch 9, Batch 14, Loss: 1.0940649509429932\n",
      "Epoch 9, Batch 15, Loss: 1.123295545578003\n",
      "Epoch 9, Batch 16, Loss: 1.0975697040557861\n",
      "Epoch 9, Batch 17, Loss: 1.0510730743408203\n",
      "Epoch 9, Batch 18, Loss: 1.103416085243225\n",
      "Epoch 9, Batch 19, Loss: 1.057031273841858\n",
      "Epoch 9, Batch 20, Loss: 1.065868854522705\n",
      "Epoch 9, Batch 21, Loss: 1.0750311613082886\n",
      "Epoch 9, Batch 22, Loss: 1.0624094009399414\n",
      "Epoch 9, Batch 23, Loss: 1.0944174528121948\n",
      "Epoch 9, Batch 24, Loss: 1.091860055923462\n",
      "Epoch 9, Batch 25, Loss: 1.0708619356155396\n",
      "Epoch 9, Batch 26, Loss: 1.1169602870941162\n",
      "Epoch 9, Batch 27, Loss: 1.1084134578704834\n",
      "Epoch 9, Batch 28, Loss: 1.1025285720825195\n",
      "Epoch 9, Batch 29, Loss: 1.0872257947921753\n",
      "Epoch 9, Batch 30, Loss: 1.0916708707809448\n",
      "Epoch 9, Batch 31, Loss: 1.072996735572815\n",
      "Epoch 9, Batch 32, Loss: 1.1052401065826416\n",
      "Epoch 9, Batch 33, Loss: 1.1144192218780518\n",
      "Epoch 9, Batch 34, Loss: 1.0848379135131836\n",
      "Epoch 9, Batch 35, Loss: 1.0600254535675049\n",
      "Epoch 9, Batch 36, Loss: 1.0880290269851685\n",
      "Epoch 9, Batch 37, Loss: 1.1037743091583252\n",
      "Epoch 9, Batch 38, Loss: 1.0486629009246826\n",
      "Epoch 9, Batch 39, Loss: 1.0523723363876343\n",
      "Epoch 9, Batch 40, Loss: 1.0770602226257324\n",
      "Epoch 9, Batch 41, Loss: 1.0450735092163086\n",
      "Epoch 9, Batch 42, Loss: 1.067429780960083\n",
      "Epoch 9, Batch 43, Loss: 1.0521341562271118\n",
      "Epoch 9, Batch 44, Loss: 1.069657802581787\n",
      "Epoch 9, Batch 45, Loss: 1.0621967315673828\n",
      "Epoch 9, Batch 46, Loss: 1.0783535242080688\n",
      "Epoch 9, Batch 47, Loss: 1.0850917100906372\n",
      "Epoch 9, Batch 48, Loss: 1.0363333225250244\n",
      "Epoch 9, Batch 49, Loss: 1.0872546434402466\n",
      "Epoch 9, Batch 50, Loss: 1.0623366832733154\n",
      "Epoch 9, Batch 51, Loss: 1.0982061624526978\n",
      "Epoch 9, Batch 52, Loss: 1.091936707496643\n",
      "Epoch 9, Batch 53, Loss: 1.0653845071792603\n",
      "Epoch 9, Batch 54, Loss: 1.056282877922058\n",
      "Epoch 9, Batch 55, Loss: 1.0350067615509033\n",
      "Epoch 9, Batch 56, Loss: 1.0792112350463867\n",
      "Epoch 9, Batch 57, Loss: 1.0846045017242432\n",
      "Epoch 9, Batch 58, Loss: 1.0544626712799072\n",
      "Epoch 9, Batch 59, Loss: 1.0437514781951904\n",
      "Epoch 9, Batch 60, Loss: 1.044745683670044\n",
      "Epoch 9, Batch 61, Loss: 1.0617178678512573\n",
      "Epoch 9, Batch 62, Loss: 1.0666191577911377\n",
      "Epoch 9, Batch 63, Loss: 1.0644091367721558\n",
      "Epoch 9, Batch 64, Loss: 1.027271032333374\n",
      "Epoch 9, Batch 65, Loss: 0.9960252046585083\n",
      "Epoch 9, Batch 66, Loss: 1.0390052795410156\n",
      "Epoch 9, Batch 67, Loss: 1.0638494491577148\n",
      "Epoch 9, Batch 68, Loss: 1.024308204650879\n",
      "Epoch 9, Batch 69, Loss: 1.0490984916687012\n",
      "Epoch 9, Batch 70, Loss: 1.0400722026824951\n",
      "Epoch 9, Batch 71, Loss: 1.0540308952331543\n",
      "Epoch 9, Batch 72, Loss: 1.0252926349639893\n",
      "Epoch 9, Batch 73, Loss: 0.9994330406188965\n",
      "Epoch 9, Batch 74, Loss: 1.040874719619751\n",
      "Epoch 9, Batch 75, Loss: 1.0492429733276367\n",
      "Epoch 9, Batch 76, Loss: 1.0561058521270752\n",
      "Epoch 9, Batch 77, Loss: 0.9986979365348816\n",
      "Epoch 9, Batch 78, Loss: 1.032543659210205\n",
      "Epoch 9, Batch 79, Loss: 1.0362733602523804\n",
      "Epoch 9, Batch 80, Loss: 0.9979796409606934\n",
      "Epoch 9, Batch 81, Loss: 1.0618178844451904\n",
      "Epoch 9, Batch 82, Loss: 1.020822286605835\n",
      "Epoch 9, Batch 83, Loss: 1.040978193283081\n",
      "Epoch 9, Batch 84, Loss: 1.0366837978363037\n",
      "Epoch 9, Batch 85, Loss: 1.011609435081482\n",
      "Epoch 9, Batch 86, Loss: 1.015536904335022\n",
      "Epoch 9, Batch 87, Loss: 1.0056326389312744\n",
      "Epoch 9, Batch 88, Loss: 1.0415154695510864\n",
      "Epoch 9, Batch 89, Loss: 0.9938584566116333\n",
      "Epoch 9, Batch 90, Loss: 1.0351660251617432\n",
      "Epoch 9, Batch 91, Loss: 1.0581624507904053\n",
      "Epoch 9, Batch 92, Loss: 1.0176260471343994\n",
      "Epoch 9, Batch 93, Loss: 1.0720198154449463\n",
      "Epoch 10, Batch 0, Loss: 1.0188599824905396\n",
      "Epoch 10, Batch 1, Loss: 1.0006147623062134\n",
      "Epoch 10, Batch 2, Loss: 1.083100438117981\n",
      "Epoch 10, Batch 3, Loss: 0.9882674217224121\n",
      "Epoch 10, Batch 4, Loss: 1.0393511056900024\n",
      "Epoch 10, Batch 5, Loss: 1.008504867553711\n",
      "Epoch 10, Batch 6, Loss: 1.0331357717514038\n",
      "Epoch 10, Batch 7, Loss: 1.0327969789505005\n",
      "Epoch 10, Batch 8, Loss: 1.0422470569610596\n",
      "Epoch 10, Batch 9, Loss: 0.9900667071342468\n",
      "Epoch 10, Batch 10, Loss: 0.982105553150177\n",
      "Epoch 10, Batch 11, Loss: 1.040893793106079\n",
      "Epoch 10, Batch 12, Loss: 1.0229915380477905\n",
      "Epoch 10, Batch 13, Loss: 1.0077308416366577\n",
      "Epoch 10, Batch 14, Loss: 1.0282409191131592\n",
      "Epoch 10, Batch 15, Loss: 1.0215226411819458\n",
      "Epoch 10, Batch 16, Loss: 1.0167890787124634\n",
      "Epoch 10, Batch 17, Loss: 0.9917728304862976\n",
      "Epoch 10, Batch 18, Loss: 1.008913278579712\n",
      "Epoch 10, Batch 19, Loss: 1.0103662014007568\n",
      "Epoch 10, Batch 20, Loss: 1.0295950174331665\n",
      "Epoch 10, Batch 21, Loss: 1.0515565872192383\n",
      "Epoch 10, Batch 22, Loss: 1.0092025995254517\n",
      "Epoch 10, Batch 23, Loss: 1.0560847520828247\n",
      "Epoch 10, Batch 24, Loss: 0.9985467195510864\n",
      "Epoch 10, Batch 25, Loss: 0.975724995136261\n",
      "Epoch 10, Batch 26, Loss: 1.0271070003509521\n",
      "Epoch 10, Batch 27, Loss: 1.0142123699188232\n",
      "Epoch 10, Batch 28, Loss: 1.042364478111267\n",
      "Epoch 10, Batch 29, Loss: 1.0614469051361084\n",
      "Epoch 10, Batch 30, Loss: 1.0269620418548584\n",
      "Epoch 10, Batch 31, Loss: 0.9918918609619141\n",
      "Epoch 10, Batch 32, Loss: 0.9880872964859009\n",
      "Epoch 10, Batch 33, Loss: 0.9858455657958984\n",
      "Epoch 10, Batch 34, Loss: 1.0239969491958618\n",
      "Epoch 10, Batch 35, Loss: 1.0169514417648315\n",
      "Epoch 10, Batch 36, Loss: 1.027989149093628\n",
      "Epoch 10, Batch 37, Loss: 1.018494963645935\n",
      "Epoch 10, Batch 38, Loss: 0.9995913505554199\n",
      "Epoch 10, Batch 39, Loss: 1.0245591402053833\n",
      "Epoch 10, Batch 40, Loss: 1.022186040878296\n",
      "Epoch 10, Batch 41, Loss: 1.0161850452423096\n",
      "Epoch 10, Batch 42, Loss: 0.9866735339164734\n",
      "Epoch 10, Batch 43, Loss: 1.0084433555603027\n",
      "Epoch 10, Batch 44, Loss: 0.9876850843429565\n",
      "Epoch 10, Batch 45, Loss: 1.0027000904083252\n",
      "Epoch 10, Batch 46, Loss: 1.013743281364441\n",
      "Epoch 10, Batch 47, Loss: 0.975852370262146\n",
      "Epoch 10, Batch 48, Loss: 0.9623994827270508\n",
      "Epoch 10, Batch 49, Loss: 0.9720619320869446\n",
      "Epoch 10, Batch 50, Loss: 1.0001447200775146\n",
      "Epoch 10, Batch 51, Loss: 0.9503893852233887\n",
      "Epoch 10, Batch 52, Loss: 0.9942681193351746\n",
      "Epoch 10, Batch 53, Loss: 0.9651471376419067\n",
      "Epoch 10, Batch 54, Loss: 0.9643624424934387\n",
      "Epoch 10, Batch 55, Loss: 1.016964316368103\n",
      "Epoch 10, Batch 56, Loss: 0.9998960494995117\n",
      "Epoch 10, Batch 57, Loss: 0.9808163642883301\n",
      "Epoch 10, Batch 58, Loss: 0.9798015356063843\n",
      "Epoch 10, Batch 59, Loss: 0.9942300915718079\n",
      "Epoch 10, Batch 60, Loss: 1.0068178176879883\n",
      "Epoch 10, Batch 61, Loss: 1.0151917934417725\n",
      "Epoch 10, Batch 62, Loss: 0.9930254817008972\n",
      "Epoch 10, Batch 63, Loss: 0.9815080761909485\n",
      "Epoch 10, Batch 64, Loss: 0.9720577001571655\n",
      "Epoch 10, Batch 65, Loss: 0.9855844378471375\n",
      "Epoch 10, Batch 66, Loss: 0.9671119451522827\n",
      "Epoch 10, Batch 67, Loss: 0.9532713890075684\n",
      "Epoch 10, Batch 68, Loss: 0.9432905316352844\n",
      "Epoch 10, Batch 69, Loss: 0.9879963994026184\n",
      "Epoch 10, Batch 70, Loss: 0.999011218547821\n",
      "Epoch 10, Batch 71, Loss: 0.9463785886764526\n",
      "Epoch 10, Batch 72, Loss: 0.9963693618774414\n",
      "Epoch 10, Batch 73, Loss: 0.9712160229682922\n",
      "Epoch 10, Batch 74, Loss: 0.9880155324935913\n",
      "Epoch 10, Batch 75, Loss: 0.9636203050613403\n",
      "Epoch 10, Batch 76, Loss: 0.966219425201416\n",
      "Epoch 10, Batch 77, Loss: 0.9528303146362305\n",
      "Epoch 10, Batch 78, Loss: 0.978958785533905\n",
      "Epoch 10, Batch 79, Loss: 0.9496103525161743\n",
      "Epoch 10, Batch 80, Loss: 0.9201083183288574\n",
      "Epoch 10, Batch 81, Loss: 0.9522638320922852\n",
      "Epoch 10, Batch 82, Loss: 0.9918444752693176\n",
      "Epoch 10, Batch 83, Loss: 0.9421764612197876\n",
      "Epoch 10, Batch 84, Loss: 0.92638099193573\n",
      "Epoch 10, Batch 85, Loss: 0.9301745295524597\n",
      "Epoch 10, Batch 86, Loss: 0.9524016380310059\n",
      "Epoch 10, Batch 87, Loss: 0.9358084797859192\n",
      "Epoch 10, Batch 88, Loss: 0.9225303530693054\n",
      "Epoch 10, Batch 89, Loss: 0.9730343818664551\n",
      "Epoch 10, Batch 90, Loss: 0.958267092704773\n",
      "Epoch 10, Batch 91, Loss: 0.957915186882019\n",
      "Epoch 10, Batch 92, Loss: 0.986601710319519\n",
      "Epoch 10, Batch 93, Loss: 0.9731525778770447\n",
      "Epoch 11, Batch 0, Loss: 0.9745360612869263\n",
      "Epoch 11, Batch 1, Loss: 0.9640113115310669\n",
      "Epoch 11, Batch 2, Loss: 0.9549452662467957\n",
      "Epoch 11, Batch 3, Loss: 0.9875563383102417\n",
      "Epoch 11, Batch 4, Loss: 0.9449129104614258\n",
      "Epoch 11, Batch 5, Loss: 0.9331814646720886\n",
      "Epoch 11, Batch 6, Loss: 0.9810657501220703\n",
      "Epoch 11, Batch 7, Loss: 0.9297183156013489\n",
      "Epoch 11, Batch 8, Loss: 0.9883701205253601\n",
      "Epoch 11, Batch 9, Loss: 0.9687789678573608\n",
      "Epoch 11, Batch 10, Loss: 0.9401543736457825\n",
      "Epoch 11, Batch 11, Loss: 0.9606291055679321\n",
      "Epoch 11, Batch 12, Loss: 0.9643359184265137\n",
      "Epoch 11, Batch 13, Loss: 0.9757086634635925\n",
      "Epoch 11, Batch 14, Loss: 0.9359701871871948\n",
      "Epoch 11, Batch 15, Loss: 1.0135607719421387\n",
      "Epoch 11, Batch 16, Loss: 0.9296997785568237\n",
      "Epoch 11, Batch 17, Loss: 0.951683521270752\n",
      "Epoch 11, Batch 18, Loss: 0.9434165954589844\n",
      "Epoch 11, Batch 19, Loss: 0.9395707249641418\n",
      "Epoch 11, Batch 20, Loss: 0.8983175158500671\n",
      "Epoch 11, Batch 21, Loss: 0.9302809834480286\n",
      "Epoch 11, Batch 22, Loss: 0.9147413969039917\n",
      "Epoch 11, Batch 23, Loss: 0.9949175715446472\n",
      "Epoch 11, Batch 24, Loss: 0.9366436004638672\n",
      "Epoch 11, Batch 25, Loss: 0.9376270174980164\n",
      "Epoch 11, Batch 26, Loss: 0.9315738677978516\n",
      "Epoch 11, Batch 27, Loss: 0.9190548062324524\n",
      "Epoch 11, Batch 28, Loss: 0.9489234685897827\n",
      "Epoch 11, Batch 29, Loss: 0.9358488321304321\n",
      "Epoch 11, Batch 30, Loss: 0.9363813400268555\n",
      "Epoch 11, Batch 31, Loss: 0.9332469701766968\n",
      "Epoch 11, Batch 32, Loss: 0.9205042719841003\n",
      "Epoch 11, Batch 33, Loss: 0.9518364071846008\n",
      "Epoch 11, Batch 34, Loss: 0.8971889615058899\n",
      "Epoch 11, Batch 35, Loss: 0.9557805061340332\n",
      "Epoch 11, Batch 36, Loss: 0.9465187788009644\n",
      "Epoch 11, Batch 37, Loss: 0.9443169832229614\n",
      "Epoch 11, Batch 38, Loss: 0.9388526082038879\n",
      "Epoch 11, Batch 39, Loss: 0.9646965861320496\n",
      "Epoch 11, Batch 40, Loss: 0.931102454662323\n",
      "Epoch 11, Batch 41, Loss: 0.9721367955207825\n",
      "Epoch 11, Batch 42, Loss: 0.9409857988357544\n",
      "Epoch 11, Batch 43, Loss: 0.9061442613601685\n",
      "Epoch 11, Batch 44, Loss: 0.9555142521858215\n",
      "Epoch 11, Batch 45, Loss: 0.8841524124145508\n",
      "Epoch 11, Batch 46, Loss: 0.9071447253227234\n",
      "Epoch 11, Batch 47, Loss: 0.933318018913269\n",
      "Epoch 11, Batch 48, Loss: 0.9112340807914734\n",
      "Epoch 11, Batch 49, Loss: 0.9016600847244263\n",
      "Epoch 11, Batch 50, Loss: 0.9596127271652222\n",
      "Epoch 11, Batch 51, Loss: 0.9298707246780396\n",
      "Epoch 11, Batch 52, Loss: 0.9125300645828247\n",
      "Epoch 11, Batch 53, Loss: 0.9659014940261841\n",
      "Epoch 11, Batch 54, Loss: 0.9223541021347046\n",
      "Epoch 11, Batch 55, Loss: 0.9170078039169312\n",
      "Epoch 11, Batch 56, Loss: 0.9174642562866211\n",
      "Epoch 11, Batch 57, Loss: 0.9129495620727539\n",
      "Epoch 11, Batch 58, Loss: 0.9693177938461304\n",
      "Epoch 11, Batch 59, Loss: 0.9056762456893921\n",
      "Epoch 11, Batch 60, Loss: 0.9655211567878723\n",
      "Epoch 11, Batch 61, Loss: 0.878982424736023\n",
      "Epoch 11, Batch 62, Loss: 0.9322415590286255\n",
      "Epoch 11, Batch 63, Loss: 0.8735764622688293\n",
      "Epoch 11, Batch 64, Loss: 0.9260632395744324\n",
      "Epoch 11, Batch 65, Loss: 0.9271723628044128\n",
      "Epoch 11, Batch 66, Loss: 0.9194225072860718\n",
      "Epoch 11, Batch 67, Loss: 0.9408637285232544\n",
      "Epoch 11, Batch 68, Loss: 0.8863376379013062\n",
      "Epoch 11, Batch 69, Loss: 0.9157184362411499\n",
      "Epoch 11, Batch 70, Loss: 0.9081268310546875\n",
      "Epoch 11, Batch 71, Loss: 0.8887197375297546\n",
      "Epoch 11, Batch 72, Loss: 0.9207513928413391\n",
      "Epoch 11, Batch 73, Loss: 0.882624626159668\n",
      "Epoch 11, Batch 74, Loss: 0.9392448663711548\n",
      "Epoch 11, Batch 75, Loss: 0.9038451910018921\n",
      "Epoch 11, Batch 76, Loss: 0.9255921244621277\n",
      "Epoch 11, Batch 77, Loss: 0.9284051060676575\n",
      "Epoch 11, Batch 78, Loss: 0.9214643239974976\n",
      "Epoch 11, Batch 79, Loss: 0.95228511095047\n",
      "Epoch 11, Batch 80, Loss: 0.9317574501037598\n",
      "Epoch 11, Batch 81, Loss: 0.9215044975280762\n",
      "Epoch 11, Batch 82, Loss: 0.9211238622665405\n",
      "Epoch 11, Batch 83, Loss: 0.8999419212341309\n",
      "Epoch 11, Batch 84, Loss: 0.9044643640518188\n",
      "Epoch 11, Batch 85, Loss: 0.9506516456604004\n",
      "Epoch 11, Batch 86, Loss: 0.9195707440376282\n",
      "Epoch 11, Batch 87, Loss: 0.9371275901794434\n",
      "Epoch 11, Batch 88, Loss: 0.9170838594436646\n",
      "Epoch 11, Batch 89, Loss: 0.899980902671814\n",
      "Epoch 11, Batch 90, Loss: 0.906376838684082\n",
      "Epoch 11, Batch 91, Loss: 0.8975332975387573\n",
      "Epoch 11, Batch 92, Loss: 0.8789132237434387\n",
      "Epoch 11, Batch 93, Loss: 0.8433074355125427\n",
      "Epoch 12, Batch 0, Loss: 0.8869717717170715\n",
      "Epoch 12, Batch 1, Loss: 0.9054051637649536\n",
      "Epoch 12, Batch 2, Loss: 0.8710945248603821\n",
      "Epoch 12, Batch 3, Loss: 0.8944611549377441\n",
      "Epoch 12, Batch 4, Loss: 0.9203799962997437\n",
      "Epoch 12, Batch 5, Loss: 0.904891848564148\n",
      "Epoch 12, Batch 6, Loss: 0.8713696599006653\n",
      "Epoch 12, Batch 7, Loss: 0.910651683807373\n",
      "Epoch 12, Batch 8, Loss: 0.9026399850845337\n",
      "Epoch 12, Batch 9, Loss: 0.8892067670822144\n",
      "Epoch 12, Batch 10, Loss: 0.9064241647720337\n",
      "Epoch 12, Batch 11, Loss: 0.9058860540390015\n",
      "Epoch 12, Batch 12, Loss: 0.9354501962661743\n",
      "Epoch 12, Batch 13, Loss: 0.8908720016479492\n",
      "Epoch 12, Batch 14, Loss: 0.9098641276359558\n",
      "Epoch 12, Batch 15, Loss: 0.8779392242431641\n",
      "Epoch 12, Batch 16, Loss: 0.9117963910102844\n",
      "Epoch 12, Batch 17, Loss: 0.924050509929657\n",
      "Epoch 12, Batch 18, Loss: 0.8914419412612915\n",
      "Epoch 12, Batch 19, Loss: 0.8942010998725891\n",
      "Epoch 12, Batch 20, Loss: 0.8964945673942566\n",
      "Epoch 12, Batch 21, Loss: 0.8994103670120239\n",
      "Epoch 12, Batch 22, Loss: 0.9171274900436401\n",
      "Epoch 12, Batch 23, Loss: 0.9247023463249207\n",
      "Epoch 12, Batch 24, Loss: 0.8964633941650391\n",
      "Epoch 12, Batch 25, Loss: 0.8561590313911438\n",
      "Epoch 12, Batch 26, Loss: 0.9110090136528015\n",
      "Epoch 12, Batch 27, Loss: 0.8604477643966675\n",
      "Epoch 12, Batch 28, Loss: 0.8891203999519348\n",
      "Epoch 12, Batch 29, Loss: 0.8788098096847534\n",
      "Epoch 12, Batch 30, Loss: 0.9041241407394409\n",
      "Epoch 12, Batch 31, Loss: 0.8724901080131531\n",
      "Epoch 12, Batch 32, Loss: 0.893875777721405\n",
      "Epoch 12, Batch 33, Loss: 0.899411678314209\n",
      "Epoch 12, Batch 34, Loss: 0.8703041076660156\n",
      "Epoch 12, Batch 35, Loss: 0.9158199429512024\n",
      "Epoch 12, Batch 36, Loss: 0.8880678415298462\n",
      "Epoch 12, Batch 37, Loss: 0.9091191291809082\n",
      "Epoch 12, Batch 38, Loss: 0.8863326907157898\n",
      "Epoch 12, Batch 39, Loss: 0.8837651014328003\n",
      "Epoch 12, Batch 40, Loss: 0.8739112019538879\n",
      "Epoch 12, Batch 41, Loss: 0.8928499221801758\n",
      "Epoch 12, Batch 42, Loss: 0.8649415969848633\n",
      "Epoch 12, Batch 43, Loss: 0.8813176155090332\n",
      "Epoch 12, Batch 44, Loss: 0.8823601603507996\n",
      "Epoch 12, Batch 45, Loss: 0.865568995475769\n",
      "Epoch 12, Batch 46, Loss: 0.8438817262649536\n",
      "Epoch 12, Batch 47, Loss: 0.8693591356277466\n",
      "Epoch 12, Batch 48, Loss: 0.8372737765312195\n",
      "Epoch 12, Batch 49, Loss: 0.8820492029190063\n",
      "Epoch 12, Batch 50, Loss: 0.8537777662277222\n",
      "Epoch 12, Batch 51, Loss: 0.8915746808052063\n",
      "Epoch 12, Batch 52, Loss: 0.8750108480453491\n",
      "Epoch 12, Batch 53, Loss: 0.8826829195022583\n",
      "Epoch 12, Batch 54, Loss: 0.8608660697937012\n",
      "Epoch 12, Batch 55, Loss: 0.8851607441902161\n",
      "Epoch 12, Batch 56, Loss: 0.8767963647842407\n",
      "Epoch 12, Batch 57, Loss: 0.9049884080886841\n",
      "Epoch 12, Batch 58, Loss: 0.8651841878890991\n",
      "Epoch 12, Batch 59, Loss: 0.8347492218017578\n",
      "Epoch 12, Batch 60, Loss: 0.8788058161735535\n",
      "Epoch 12, Batch 61, Loss: 0.8831055760383606\n",
      "Epoch 12, Batch 62, Loss: 0.8597425222396851\n",
      "Epoch 12, Batch 63, Loss: 0.9109261631965637\n",
      "Epoch 12, Batch 64, Loss: 0.8868182897567749\n",
      "Epoch 12, Batch 65, Loss: 0.8706097602844238\n",
      "Epoch 12, Batch 66, Loss: 0.8510859608650208\n",
      "Epoch 12, Batch 67, Loss: 0.8066611289978027\n",
      "Epoch 12, Batch 68, Loss: 0.830168604850769\n",
      "Epoch 12, Batch 69, Loss: 0.8681195378303528\n",
      "Epoch 12, Batch 70, Loss: 0.8904542922973633\n",
      "Epoch 12, Batch 71, Loss: 0.843269944190979\n",
      "Epoch 12, Batch 72, Loss: 0.8737469911575317\n",
      "Epoch 12, Batch 73, Loss: 0.8334782719612122\n",
      "Epoch 12, Batch 74, Loss: 0.8742779493331909\n",
      "Epoch 12, Batch 75, Loss: 0.8698886036872864\n",
      "Epoch 12, Batch 76, Loss: 0.8997119069099426\n",
      "Epoch 12, Batch 77, Loss: 0.8857917785644531\n",
      "Epoch 12, Batch 78, Loss: 0.8831324577331543\n",
      "Epoch 12, Batch 79, Loss: 0.8784372210502625\n",
      "Epoch 12, Batch 80, Loss: 0.8861273527145386\n",
      "Epoch 12, Batch 81, Loss: 0.8659550547599792\n",
      "Epoch 12, Batch 82, Loss: 0.8003721237182617\n",
      "Epoch 12, Batch 83, Loss: 0.8736168742179871\n",
      "Epoch 12, Batch 84, Loss: 0.8614428639411926\n",
      "Epoch 12, Batch 85, Loss: 0.8783062696456909\n",
      "Epoch 12, Batch 86, Loss: 0.8457088470458984\n",
      "Epoch 12, Batch 87, Loss: 0.8450748324394226\n",
      "Epoch 12, Batch 88, Loss: 0.8620957136154175\n",
      "Epoch 12, Batch 89, Loss: 0.8411248326301575\n",
      "Epoch 12, Batch 90, Loss: 0.877302348613739\n",
      "Epoch 12, Batch 91, Loss: 0.8193933367729187\n",
      "Epoch 12, Batch 92, Loss: 0.8391400575637817\n",
      "Epoch 12, Batch 93, Loss: 0.8053066730499268\n",
      "Epoch 13, Batch 0, Loss: 0.8834401369094849\n",
      "Epoch 13, Batch 1, Loss: 0.8265393376350403\n",
      "Epoch 13, Batch 2, Loss: 0.8567935824394226\n",
      "Epoch 13, Batch 3, Loss: 0.8693937063217163\n",
      "Epoch 13, Batch 4, Loss: 0.8504474759101868\n",
      "Epoch 13, Batch 5, Loss: 0.8250589370727539\n",
      "Epoch 13, Batch 6, Loss: 0.8423409461975098\n",
      "Epoch 13, Batch 7, Loss: 0.8637326955795288\n",
      "Epoch 13, Batch 8, Loss: 0.8363898992538452\n",
      "Epoch 13, Batch 9, Loss: 0.8581579923629761\n",
      "Epoch 13, Batch 10, Loss: 0.8499976992607117\n",
      "Epoch 13, Batch 11, Loss: 0.8203738927841187\n",
      "Epoch 13, Batch 12, Loss: 0.8427130579948425\n",
      "Epoch 13, Batch 13, Loss: 0.8216060400009155\n",
      "Epoch 13, Batch 14, Loss: 0.8780468106269836\n",
      "Epoch 13, Batch 15, Loss: 0.8122200965881348\n",
      "Epoch 13, Batch 16, Loss: 0.8505359888076782\n",
      "Epoch 13, Batch 17, Loss: 0.8786662220954895\n",
      "Epoch 13, Batch 18, Loss: 0.8306918144226074\n",
      "Epoch 13, Batch 19, Loss: 0.8481128811836243\n",
      "Epoch 13, Batch 20, Loss: 0.8444636464118958\n",
      "Epoch 13, Batch 21, Loss: 0.8162937164306641\n",
      "Epoch 13, Batch 22, Loss: 0.8502082824707031\n",
      "Epoch 13, Batch 23, Loss: 0.849929690361023\n",
      "Epoch 13, Batch 24, Loss: 0.8565403819084167\n",
      "Epoch 13, Batch 25, Loss: 0.8595870137214661\n",
      "Epoch 13, Batch 26, Loss: 0.8630838394165039\n",
      "Epoch 13, Batch 27, Loss: 0.882800281047821\n",
      "Epoch 13, Batch 28, Loss: 0.7982974648475647\n",
      "Epoch 13, Batch 29, Loss: 0.8367222547531128\n",
      "Epoch 13, Batch 30, Loss: 0.8538374900817871\n",
      "Epoch 13, Batch 31, Loss: 0.831871509552002\n",
      "Epoch 13, Batch 32, Loss: 0.8541997075080872\n",
      "Epoch 13, Batch 33, Loss: 0.8355649709701538\n",
      "Epoch 13, Batch 34, Loss: 0.8138941526412964\n",
      "Epoch 13, Batch 35, Loss: 0.8198326826095581\n",
      "Epoch 13, Batch 36, Loss: 0.8184667825698853\n",
      "Epoch 13, Batch 37, Loss: 0.8264252543449402\n",
      "Epoch 13, Batch 38, Loss: 0.8157743215560913\n",
      "Epoch 13, Batch 39, Loss: 0.8514439463615417\n",
      "Epoch 13, Batch 40, Loss: 0.8571486473083496\n",
      "Epoch 13, Batch 41, Loss: 0.8451560735702515\n",
      "Epoch 13, Batch 42, Loss: 0.8510347604751587\n",
      "Epoch 13, Batch 43, Loss: 0.8083522915840149\n",
      "Epoch 13, Batch 44, Loss: 0.7792069315910339\n",
      "Epoch 13, Batch 45, Loss: 0.822486400604248\n",
      "Epoch 13, Batch 46, Loss: 0.8310409784317017\n",
      "Epoch 13, Batch 47, Loss: 0.817242443561554\n",
      "Epoch 13, Batch 48, Loss: 0.7973654270172119\n",
      "Epoch 13, Batch 49, Loss: 0.8360942006111145\n",
      "Epoch 13, Batch 50, Loss: 0.8661983609199524\n",
      "Epoch 13, Batch 51, Loss: 0.8586406707763672\n",
      "Epoch 13, Batch 52, Loss: 0.8216598629951477\n",
      "Epoch 13, Batch 53, Loss: 0.8335350751876831\n",
      "Epoch 13, Batch 54, Loss: 0.8632847666740417\n",
      "Epoch 13, Batch 55, Loss: 0.8107948303222656\n",
      "Epoch 13, Batch 56, Loss: 0.8293835520744324\n",
      "Epoch 13, Batch 57, Loss: 0.8350849151611328\n",
      "Epoch 13, Batch 58, Loss: 0.8308833837509155\n",
      "Epoch 13, Batch 59, Loss: 0.8214020729064941\n",
      "Epoch 13, Batch 60, Loss: 0.7716876864433289\n",
      "Epoch 13, Batch 61, Loss: 0.8060868382453918\n",
      "Epoch 13, Batch 62, Loss: 0.813286304473877\n",
      "Epoch 13, Batch 63, Loss: 0.8274146318435669\n",
      "Epoch 13, Batch 64, Loss: 0.8231219053268433\n",
      "Epoch 13, Batch 65, Loss: 0.8064136505126953\n",
      "Epoch 13, Batch 66, Loss: 0.8778912425041199\n",
      "Epoch 13, Batch 67, Loss: 0.8320668339729309\n",
      "Epoch 13, Batch 68, Loss: 0.8346788287162781\n",
      "Epoch 13, Batch 69, Loss: 0.7996336221694946\n",
      "Epoch 13, Batch 70, Loss: 0.8110111355781555\n",
      "Epoch 13, Batch 71, Loss: 0.8673350214958191\n",
      "Epoch 13, Batch 72, Loss: 0.8378780484199524\n",
      "Epoch 13, Batch 73, Loss: 0.7940751314163208\n",
      "Epoch 13, Batch 74, Loss: 0.7975940108299255\n",
      "Epoch 13, Batch 75, Loss: 0.8519788980484009\n",
      "Epoch 13, Batch 76, Loss: 0.8060047030448914\n",
      "Epoch 13, Batch 77, Loss: 0.8075369596481323\n",
      "Epoch 13, Batch 78, Loss: 0.8480022549629211\n",
      "Epoch 13, Batch 79, Loss: 0.8251446485519409\n",
      "Epoch 13, Batch 80, Loss: 0.835490882396698\n",
      "Epoch 13, Batch 81, Loss: 0.8134678602218628\n",
      "Epoch 13, Batch 82, Loss: 0.8415945768356323\n",
      "Epoch 13, Batch 83, Loss: 0.8517336845397949\n",
      "Epoch 13, Batch 84, Loss: 0.801937460899353\n",
      "Epoch 13, Batch 85, Loss: 0.8263688087463379\n",
      "Epoch 13, Batch 86, Loss: 0.7735102772712708\n",
      "Epoch 13, Batch 87, Loss: 0.8106359243392944\n",
      "Epoch 13, Batch 88, Loss: 0.8391469717025757\n",
      "Epoch 13, Batch 89, Loss: 0.8237178921699524\n",
      "Epoch 13, Batch 90, Loss: 0.8489488363265991\n",
      "Epoch 13, Batch 91, Loss: 0.8196060061454773\n",
      "Epoch 13, Batch 92, Loss: 0.7906453609466553\n",
      "Epoch 13, Batch 93, Loss: 0.822296679019928\n",
      "Epoch 14, Batch 0, Loss: 0.7965778112411499\n",
      "Epoch 14, Batch 1, Loss: 0.8476698994636536\n",
      "Epoch 14, Batch 2, Loss: 0.7845985889434814\n",
      "Epoch 14, Batch 3, Loss: 0.7917520403862\n",
      "Epoch 14, Batch 4, Loss: 0.7975808382034302\n",
      "Epoch 14, Batch 5, Loss: 0.8045862317085266\n",
      "Epoch 14, Batch 6, Loss: 0.824600875377655\n",
      "Epoch 14, Batch 7, Loss: 0.782082200050354\n",
      "Epoch 14, Batch 8, Loss: 0.764794647693634\n",
      "Epoch 14, Batch 9, Loss: 0.8182792663574219\n",
      "Epoch 14, Batch 10, Loss: 0.7945892810821533\n",
      "Epoch 14, Batch 11, Loss: 0.803112804889679\n",
      "Epoch 14, Batch 12, Loss: 0.8038709759712219\n",
      "Epoch 14, Batch 13, Loss: 0.8364871144294739\n",
      "Epoch 14, Batch 14, Loss: 0.8105390667915344\n",
      "Epoch 14, Batch 15, Loss: 0.8007968068122864\n",
      "Epoch 14, Batch 16, Loss: 0.8165059089660645\n",
      "Epoch 14, Batch 17, Loss: 0.7977687120437622\n",
      "Epoch 14, Batch 18, Loss: 0.787757158279419\n",
      "Epoch 14, Batch 19, Loss: 0.8662654757499695\n",
      "Epoch 14, Batch 20, Loss: 0.8458919525146484\n",
      "Epoch 14, Batch 21, Loss: 0.8349424600601196\n",
      "Epoch 14, Batch 22, Loss: 0.7475124001502991\n",
      "Epoch 14, Batch 23, Loss: 0.8124489784240723\n",
      "Epoch 14, Batch 24, Loss: 0.7566856145858765\n",
      "Epoch 14, Batch 25, Loss: 0.7988317012786865\n",
      "Epoch 14, Batch 26, Loss: 0.8459945917129517\n",
      "Epoch 14, Batch 27, Loss: 0.7566928863525391\n",
      "Epoch 14, Batch 28, Loss: 0.814233660697937\n",
      "Epoch 14, Batch 29, Loss: 0.7782676815986633\n",
      "Epoch 14, Batch 30, Loss: 0.7569018602371216\n",
      "Epoch 14, Batch 31, Loss: 0.7810078263282776\n",
      "Epoch 14, Batch 32, Loss: 0.7746973037719727\n",
      "Epoch 14, Batch 33, Loss: 0.7985420823097229\n",
      "Epoch 14, Batch 34, Loss: 0.7769728302955627\n",
      "Epoch 14, Batch 35, Loss: 0.7889857888221741\n",
      "Epoch 14, Batch 36, Loss: 0.768267035484314\n",
      "Epoch 14, Batch 37, Loss: 0.8206051588058472\n",
      "Epoch 14, Batch 38, Loss: 0.7580487132072449\n",
      "Epoch 14, Batch 39, Loss: 0.7894213199615479\n",
      "Epoch 14, Batch 40, Loss: 0.8139093518257141\n",
      "Epoch 14, Batch 41, Loss: 0.8037753105163574\n",
      "Epoch 14, Batch 42, Loss: 0.8011008501052856\n",
      "Epoch 14, Batch 43, Loss: 0.7760356664657593\n",
      "Epoch 14, Batch 44, Loss: 0.8138307332992554\n",
      "Epoch 14, Batch 45, Loss: 0.7878471612930298\n",
      "Epoch 14, Batch 46, Loss: 0.7798835039138794\n",
      "Epoch 14, Batch 47, Loss: 0.7703870534896851\n",
      "Epoch 14, Batch 48, Loss: 0.806806743144989\n",
      "Epoch 14, Batch 49, Loss: 0.769648015499115\n",
      "Epoch 14, Batch 50, Loss: 0.7851665616035461\n",
      "Epoch 14, Batch 51, Loss: 0.8721475601196289\n",
      "Epoch 14, Batch 52, Loss: 0.8038535118103027\n",
      "Epoch 14, Batch 53, Loss: 0.7933613061904907\n",
      "Epoch 14, Batch 54, Loss: 0.8090420961380005\n",
      "Epoch 14, Batch 55, Loss: 0.8299621343612671\n",
      "Epoch 14, Batch 56, Loss: 0.7664758563041687\n",
      "Epoch 14, Batch 57, Loss: 0.7984833717346191\n",
      "Epoch 14, Batch 58, Loss: 0.8190412521362305\n",
      "Epoch 14, Batch 59, Loss: 0.790469765663147\n",
      "Epoch 14, Batch 60, Loss: 0.8030961751937866\n",
      "Epoch 14, Batch 61, Loss: 0.7789040803909302\n",
      "Epoch 14, Batch 62, Loss: 0.788902997970581\n",
      "Epoch 14, Batch 63, Loss: 0.8335863351821899\n",
      "Epoch 14, Batch 64, Loss: 0.739297091960907\n",
      "Epoch 14, Batch 65, Loss: 0.8031903505325317\n",
      "Epoch 14, Batch 66, Loss: 0.7764530181884766\n",
      "Epoch 14, Batch 67, Loss: 0.7717406153678894\n",
      "Epoch 14, Batch 68, Loss: 0.7724021673202515\n",
      "Epoch 14, Batch 69, Loss: 0.7543959021568298\n",
      "Epoch 14, Batch 70, Loss: 0.7797336578369141\n",
      "Epoch 14, Batch 71, Loss: 0.7952557802200317\n",
      "Epoch 14, Batch 72, Loss: 0.8176621198654175\n",
      "Epoch 14, Batch 73, Loss: 0.8511611819267273\n",
      "Epoch 14, Batch 74, Loss: 0.7591612935066223\n",
      "Epoch 14, Batch 75, Loss: 0.776382327079773\n",
      "Epoch 14, Batch 76, Loss: 0.7762061357498169\n",
      "Epoch 14, Batch 77, Loss: 0.7876865863800049\n",
      "Epoch 14, Batch 78, Loss: 0.7675632238388062\n",
      "Epoch 14, Batch 79, Loss: 0.7847753763198853\n",
      "Epoch 14, Batch 80, Loss: 0.7565447092056274\n",
      "Epoch 14, Batch 81, Loss: 0.8140112161636353\n",
      "Epoch 14, Batch 82, Loss: 0.7576884031295776\n",
      "Epoch 14, Batch 83, Loss: 0.774856448173523\n",
      "Epoch 14, Batch 84, Loss: 0.7598168253898621\n",
      "Epoch 14, Batch 85, Loss: 0.7768670320510864\n",
      "Epoch 14, Batch 86, Loss: 0.7601405382156372\n",
      "Epoch 14, Batch 87, Loss: 0.7660329937934875\n",
      "Epoch 14, Batch 88, Loss: 0.7229669094085693\n",
      "Epoch 14, Batch 89, Loss: 0.8290902376174927\n",
      "Epoch 14, Batch 90, Loss: 0.7943583726882935\n",
      "Epoch 14, Batch 91, Loss: 0.7891013026237488\n",
      "Epoch 14, Batch 92, Loss: 0.7756288051605225\n",
      "Epoch 14, Batch 93, Loss: 0.8420019745826721\n",
      "Epoch 15, Batch 0, Loss: 0.7432996034622192\n",
      "Epoch 15, Batch 1, Loss: 0.7593478560447693\n",
      "Epoch 15, Batch 2, Loss: 0.7739686369895935\n",
      "Epoch 15, Batch 3, Loss: 0.77731853723526\n",
      "Epoch 15, Batch 4, Loss: 0.7543268799781799\n",
      "Epoch 15, Batch 5, Loss: 0.8251535296440125\n",
      "Epoch 15, Batch 6, Loss: 0.7809471487998962\n",
      "Epoch 15, Batch 7, Loss: 0.7766827344894409\n",
      "Epoch 15, Batch 8, Loss: 0.7602006793022156\n",
      "Epoch 15, Batch 9, Loss: 0.7426139116287231\n",
      "Epoch 15, Batch 10, Loss: 0.7502642869949341\n",
      "Epoch 15, Batch 11, Loss: 0.7860388159751892\n",
      "Epoch 15, Batch 12, Loss: 0.7798876166343689\n",
      "Epoch 15, Batch 13, Loss: 0.7382568717002869\n",
      "Epoch 15, Batch 14, Loss: 0.8032193183898926\n",
      "Epoch 15, Batch 15, Loss: 0.7271745800971985\n",
      "Epoch 15, Batch 16, Loss: 0.7535924315452576\n",
      "Epoch 15, Batch 17, Loss: 0.8012657165527344\n",
      "Epoch 15, Batch 18, Loss: 0.7767463326454163\n",
      "Epoch 15, Batch 19, Loss: 0.7611503005027771\n",
      "Epoch 15, Batch 20, Loss: 0.7888356447219849\n",
      "Epoch 15, Batch 21, Loss: 0.822945773601532\n",
      "Epoch 15, Batch 22, Loss: 0.77008056640625\n",
      "Epoch 15, Batch 23, Loss: 0.7446292638778687\n",
      "Epoch 15, Batch 24, Loss: 0.7764958143234253\n",
      "Epoch 15, Batch 25, Loss: 0.7975345849990845\n",
      "Epoch 15, Batch 26, Loss: 0.7652279734611511\n",
      "Epoch 15, Batch 27, Loss: 0.7283183336257935\n",
      "Epoch 15, Batch 28, Loss: 0.7825616598129272\n",
      "Epoch 15, Batch 29, Loss: 0.7651914358139038\n",
      "Epoch 15, Batch 30, Loss: 0.755415678024292\n",
      "Epoch 15, Batch 31, Loss: 0.8204666972160339\n",
      "Epoch 15, Batch 32, Loss: 0.7791893482208252\n",
      "Epoch 15, Batch 33, Loss: 0.7449920177459717\n",
      "Epoch 15, Batch 34, Loss: 0.7274966239929199\n",
      "Epoch 15, Batch 35, Loss: 0.75865238904953\n",
      "Epoch 15, Batch 36, Loss: 0.8019713163375854\n",
      "Epoch 15, Batch 37, Loss: 0.7428228259086609\n",
      "Epoch 15, Batch 38, Loss: 0.7438819408416748\n",
      "Epoch 15, Batch 39, Loss: 0.8236667513847351\n",
      "Epoch 15, Batch 40, Loss: 0.7871893048286438\n",
      "Epoch 15, Batch 41, Loss: 0.7679969072341919\n",
      "Epoch 15, Batch 42, Loss: 0.7539734244346619\n",
      "Epoch 15, Batch 43, Loss: 0.746111273765564\n",
      "Epoch 15, Batch 44, Loss: 0.8105078935623169\n",
      "Epoch 15, Batch 45, Loss: 0.7401958703994751\n",
      "Epoch 15, Batch 46, Loss: 0.7671232223510742\n",
      "Epoch 15, Batch 47, Loss: 0.7807514667510986\n",
      "Epoch 15, Batch 48, Loss: 0.751183807849884\n",
      "Epoch 15, Batch 49, Loss: 0.73885577917099\n",
      "Epoch 15, Batch 50, Loss: 0.7558167576789856\n",
      "Epoch 15, Batch 51, Loss: 0.7425340414047241\n",
      "Epoch 15, Batch 52, Loss: 0.7925281524658203\n",
      "Epoch 15, Batch 53, Loss: 0.7265345454216003\n",
      "Epoch 15, Batch 54, Loss: 0.7469117045402527\n",
      "Epoch 15, Batch 55, Loss: 0.7438755631446838\n",
      "Epoch 15, Batch 56, Loss: 0.7378104329109192\n",
      "Epoch 15, Batch 57, Loss: 0.7341745495796204\n",
      "Epoch 15, Batch 58, Loss: 0.7164390087127686\n",
      "Epoch 15, Batch 59, Loss: 0.7526390552520752\n",
      "Epoch 15, Batch 60, Loss: 0.7663134336471558\n",
      "Epoch 15, Batch 61, Loss: 0.7732944488525391\n",
      "Epoch 15, Batch 62, Loss: 0.7625122666358948\n",
      "Epoch 15, Batch 63, Loss: 0.7485218644142151\n",
      "Epoch 15, Batch 64, Loss: 0.6950543522834778\n",
      "Epoch 15, Batch 65, Loss: 0.7582817673683167\n",
      "Epoch 15, Batch 66, Loss: 0.7377299070358276\n",
      "Epoch 15, Batch 67, Loss: 0.7018789052963257\n",
      "Epoch 15, Batch 68, Loss: 0.7384012937545776\n",
      "Epoch 15, Batch 69, Loss: 0.7791261076927185\n",
      "Epoch 15, Batch 70, Loss: 0.7419193387031555\n",
      "Epoch 15, Batch 71, Loss: 0.7582255601882935\n",
      "Epoch 15, Batch 72, Loss: 0.7719261646270752\n",
      "Epoch 15, Batch 73, Loss: 0.7460623383522034\n",
      "Epoch 15, Batch 74, Loss: 0.7484432458877563\n",
      "Epoch 15, Batch 75, Loss: 0.7222192883491516\n",
      "Epoch 15, Batch 76, Loss: 0.7152532339096069\n",
      "Epoch 15, Batch 77, Loss: 0.7329944372177124\n",
      "Epoch 15, Batch 78, Loss: 0.7734667062759399\n",
      "Epoch 15, Batch 79, Loss: 0.7535416483879089\n",
      "Epoch 15, Batch 80, Loss: 0.707762598991394\n",
      "Epoch 15, Batch 81, Loss: 0.7087554335594177\n",
      "Epoch 15, Batch 82, Loss: 0.7891350984573364\n",
      "Epoch 15, Batch 83, Loss: 0.7794812321662903\n",
      "Epoch 15, Batch 84, Loss: 0.7289952039718628\n",
      "Epoch 15, Batch 85, Loss: 0.7212673425674438\n",
      "Epoch 15, Batch 86, Loss: 0.7622259855270386\n",
      "Epoch 15, Batch 87, Loss: 0.7188631296157837\n",
      "Epoch 15, Batch 88, Loss: 0.7691012024879456\n",
      "Epoch 15, Batch 89, Loss: 0.7468996047973633\n",
      "Epoch 15, Batch 90, Loss: 0.740304172039032\n",
      "Epoch 15, Batch 91, Loss: 0.7480409145355225\n",
      "Epoch 15, Batch 92, Loss: 0.7823614478111267\n",
      "Epoch 15, Batch 93, Loss: 0.7523605823516846\n",
      "Epoch 16, Batch 0, Loss: 0.7921661138534546\n",
      "Epoch 16, Batch 1, Loss: 0.7195237874984741\n",
      "Epoch 16, Batch 2, Loss: 0.7442487478256226\n",
      "Epoch 16, Batch 3, Loss: 0.7071484327316284\n",
      "Epoch 16, Batch 4, Loss: 0.7903565168380737\n",
      "Epoch 16, Batch 5, Loss: 0.7571825981140137\n",
      "Epoch 16, Batch 6, Loss: 0.7624006867408752\n",
      "Epoch 16, Batch 7, Loss: 0.7873269319534302\n",
      "Epoch 16, Batch 8, Loss: 0.7263668775558472\n",
      "Epoch 16, Batch 9, Loss: 0.736794650554657\n",
      "Epoch 16, Batch 10, Loss: 0.744696319103241\n",
      "Epoch 16, Batch 11, Loss: 0.7379415035247803\n",
      "Epoch 16, Batch 12, Loss: 0.7449495196342468\n",
      "Epoch 16, Batch 13, Loss: 0.7388694882392883\n",
      "Epoch 16, Batch 14, Loss: 0.7163702845573425\n",
      "Epoch 16, Batch 15, Loss: 0.7638148665428162\n",
      "Epoch 16, Batch 16, Loss: 0.7444795370101929\n",
      "Epoch 16, Batch 17, Loss: 0.7547508478164673\n",
      "Epoch 16, Batch 18, Loss: 0.7346060872077942\n",
      "Epoch 16, Batch 19, Loss: 0.6920055150985718\n",
      "Epoch 16, Batch 20, Loss: 0.7425089478492737\n",
      "Epoch 16, Batch 21, Loss: 0.7298285365104675\n",
      "Epoch 16, Batch 22, Loss: 0.7478477954864502\n",
      "Epoch 16, Batch 23, Loss: 0.7411708235740662\n",
      "Epoch 16, Batch 24, Loss: 0.7579874396324158\n",
      "Epoch 16, Batch 25, Loss: 0.7964345216751099\n",
      "Epoch 16, Batch 26, Loss: 0.7456099390983582\n",
      "Epoch 16, Batch 27, Loss: 0.7240592241287231\n",
      "Epoch 16, Batch 28, Loss: 0.7112566828727722\n",
      "Epoch 16, Batch 29, Loss: 0.7838472127914429\n",
      "Epoch 16, Batch 30, Loss: 0.6913223266601562\n",
      "Epoch 16, Batch 31, Loss: 0.6943488717079163\n",
      "Epoch 16, Batch 32, Loss: 0.7179277539253235\n",
      "Epoch 16, Batch 33, Loss: 0.7501997947692871\n",
      "Epoch 16, Batch 34, Loss: 0.7663201093673706\n",
      "Epoch 16, Batch 35, Loss: 0.7310056686401367\n",
      "Epoch 16, Batch 36, Loss: 0.7478413581848145\n",
      "Epoch 16, Batch 37, Loss: 0.7654060125350952\n",
      "Epoch 16, Batch 38, Loss: 0.7046301364898682\n",
      "Epoch 16, Batch 39, Loss: 0.7507737874984741\n",
      "Epoch 16, Batch 40, Loss: 0.700760006904602\n",
      "Epoch 16, Batch 41, Loss: 0.7020362615585327\n",
      "Epoch 16, Batch 42, Loss: 0.7319626808166504\n",
      "Epoch 16, Batch 43, Loss: 0.7436152100563049\n",
      "Epoch 16, Batch 44, Loss: 0.7169303894042969\n",
      "Epoch 16, Batch 45, Loss: 0.7121621370315552\n",
      "Epoch 16, Batch 46, Loss: 0.7640671133995056\n",
      "Epoch 16, Batch 47, Loss: 0.7096900939941406\n",
      "Epoch 16, Batch 48, Loss: 0.7147417664527893\n",
      "Epoch 16, Batch 49, Loss: 0.7186468839645386\n",
      "Epoch 16, Batch 50, Loss: 0.7469745874404907\n",
      "Epoch 16, Batch 51, Loss: 0.6761424541473389\n",
      "Epoch 16, Batch 52, Loss: 0.7946785092353821\n",
      "Epoch 16, Batch 53, Loss: 0.7156872153282166\n",
      "Epoch 16, Batch 54, Loss: 0.7292979955673218\n",
      "Epoch 16, Batch 55, Loss: 0.7603122591972351\n",
      "Epoch 16, Batch 56, Loss: 0.6928604245185852\n",
      "Epoch 16, Batch 57, Loss: 0.719950795173645\n",
      "Epoch 16, Batch 58, Loss: 0.7310186624526978\n",
      "Epoch 16, Batch 59, Loss: 0.7325800657272339\n",
      "Epoch 16, Batch 60, Loss: 0.7133224010467529\n",
      "Epoch 16, Batch 61, Loss: 0.6885898113250732\n",
      "Epoch 16, Batch 62, Loss: 0.7142412066459656\n",
      "Epoch 16, Batch 63, Loss: 0.745582103729248\n",
      "Epoch 16, Batch 64, Loss: 0.7292250394821167\n",
      "Epoch 16, Batch 65, Loss: 0.7679508924484253\n",
      "Epoch 16, Batch 66, Loss: 0.7203513979911804\n",
      "Epoch 16, Batch 67, Loss: 0.7233690023422241\n",
      "Epoch 16, Batch 68, Loss: 0.7407418489456177\n",
      "Epoch 16, Batch 69, Loss: 0.699115514755249\n",
      "Epoch 16, Batch 70, Loss: 0.6756972074508667\n",
      "Epoch 16, Batch 71, Loss: 0.7130325436592102\n",
      "Epoch 16, Batch 72, Loss: 0.7339335680007935\n",
      "Epoch 16, Batch 73, Loss: 0.7002135515213013\n",
      "Epoch 16, Batch 74, Loss: 0.6903965473175049\n",
      "Epoch 16, Batch 75, Loss: 0.6885272264480591\n",
      "Epoch 16, Batch 76, Loss: 0.7202433943748474\n",
      "Epoch 16, Batch 77, Loss: 0.7089505791664124\n",
      "Epoch 16, Batch 78, Loss: 0.6699728965759277\n",
      "Epoch 16, Batch 79, Loss: 0.6916111707687378\n",
      "Epoch 16, Batch 80, Loss: 0.6795049905776978\n",
      "Epoch 16, Batch 81, Loss: 0.7077034115791321\n",
      "Epoch 16, Batch 82, Loss: 0.6787713170051575\n",
      "Epoch 16, Batch 83, Loss: 0.71246337890625\n",
      "Epoch 16, Batch 84, Loss: 0.703863263130188\n",
      "Epoch 16, Batch 85, Loss: 0.7258301973342896\n",
      "Epoch 16, Batch 86, Loss: 0.7708474397659302\n",
      "Epoch 16, Batch 87, Loss: 0.7205047607421875\n",
      "Epoch 16, Batch 88, Loss: 0.7225139141082764\n",
      "Epoch 16, Batch 89, Loss: 0.733045220375061\n",
      "Epoch 16, Batch 90, Loss: 0.7044085264205933\n",
      "Epoch 16, Batch 91, Loss: 0.6740004420280457\n",
      "Epoch 16, Batch 92, Loss: 0.7252315878868103\n",
      "Epoch 16, Batch 93, Loss: 0.7043526768684387\n",
      "Epoch 17, Batch 0, Loss: 0.7465463876724243\n",
      "Epoch 17, Batch 1, Loss: 0.6906778812408447\n",
      "Epoch 17, Batch 2, Loss: 0.6903297305107117\n",
      "Epoch 17, Batch 3, Loss: 0.6932851672172546\n",
      "Epoch 17, Batch 4, Loss: 0.7287503480911255\n",
      "Epoch 17, Batch 5, Loss: 0.7169094085693359\n",
      "Epoch 17, Batch 6, Loss: 0.731177806854248\n",
      "Epoch 17, Batch 7, Loss: 0.7269647717475891\n",
      "Epoch 17, Batch 8, Loss: 0.7107610702514648\n",
      "Epoch 17, Batch 9, Loss: 0.7115288376808167\n",
      "Epoch 17, Batch 10, Loss: 0.6989521384239197\n",
      "Epoch 17, Batch 11, Loss: 0.7112025618553162\n",
      "Epoch 17, Batch 12, Loss: 0.6917060613632202\n",
      "Epoch 17, Batch 13, Loss: 0.7215980887413025\n",
      "Epoch 17, Batch 14, Loss: 0.7362619638442993\n",
      "Epoch 17, Batch 15, Loss: 0.6706401705741882\n",
      "Epoch 17, Batch 16, Loss: 0.6771496534347534\n",
      "Epoch 17, Batch 17, Loss: 0.699791431427002\n",
      "Epoch 17, Batch 18, Loss: 0.6471328735351562\n",
      "Epoch 17, Batch 19, Loss: 0.7274421453475952\n",
      "Epoch 17, Batch 20, Loss: 0.7020966410636902\n",
      "Epoch 17, Batch 21, Loss: 0.7026466131210327\n",
      "Epoch 17, Batch 22, Loss: 0.6886654496192932\n",
      "Epoch 17, Batch 23, Loss: 0.7478938102722168\n",
      "Epoch 17, Batch 24, Loss: 0.7128123641014099\n",
      "Epoch 17, Batch 25, Loss: 0.7596846222877502\n",
      "Epoch 17, Batch 26, Loss: 0.7234302759170532\n",
      "Epoch 17, Batch 27, Loss: 0.6599879860877991\n",
      "Epoch 17, Batch 28, Loss: 0.70630943775177\n",
      "Epoch 17, Batch 29, Loss: 0.6796575784683228\n",
      "Epoch 17, Batch 30, Loss: 0.7652915716171265\n",
      "Epoch 17, Batch 31, Loss: 0.7243384122848511\n",
      "Epoch 17, Batch 32, Loss: 0.6748046875\n",
      "Epoch 17, Batch 33, Loss: 0.7048680186271667\n",
      "Epoch 17, Batch 34, Loss: 0.6948113441467285\n",
      "Epoch 17, Batch 35, Loss: 0.69623202085495\n",
      "Epoch 17, Batch 36, Loss: 0.6765505075454712\n",
      "Epoch 17, Batch 37, Loss: 0.6822801232337952\n",
      "Epoch 17, Batch 38, Loss: 0.6913549304008484\n",
      "Epoch 17, Batch 39, Loss: 0.7574648261070251\n",
      "Epoch 17, Batch 40, Loss: 0.6967542767524719\n",
      "Epoch 17, Batch 41, Loss: 0.6603510975837708\n",
      "Epoch 17, Batch 42, Loss: 0.723340630531311\n",
      "Epoch 17, Batch 43, Loss: 0.7111402750015259\n",
      "Epoch 17, Batch 44, Loss: 0.6869160532951355\n",
      "Epoch 17, Batch 45, Loss: 0.7208579778671265\n",
      "Epoch 17, Batch 46, Loss: 0.7204843759536743\n",
      "Epoch 17, Batch 47, Loss: 0.7266363501548767\n",
      "Epoch 17, Batch 48, Loss: 0.7022392749786377\n",
      "Epoch 17, Batch 49, Loss: 0.6972781419754028\n",
      "Epoch 17, Batch 50, Loss: 0.7267482280731201\n",
      "Epoch 17, Batch 51, Loss: 0.7002319097518921\n",
      "Epoch 17, Batch 52, Loss: 0.7191764712333679\n",
      "Epoch 17, Batch 53, Loss: 0.7042571306228638\n",
      "Epoch 17, Batch 54, Loss: 0.6940773129463196\n",
      "Epoch 17, Batch 55, Loss: 0.7105117440223694\n",
      "Epoch 17, Batch 56, Loss: 0.6782740354537964\n",
      "Epoch 17, Batch 57, Loss: 0.6825588345527649\n",
      "Epoch 17, Batch 58, Loss: 0.7406296133995056\n",
      "Epoch 17, Batch 59, Loss: 0.7264123558998108\n",
      "Epoch 17, Batch 60, Loss: 0.6878817677497864\n",
      "Epoch 17, Batch 61, Loss: 0.7018063068389893\n",
      "Epoch 17, Batch 62, Loss: 0.7014888525009155\n",
      "Epoch 17, Batch 63, Loss: 0.7118224501609802\n",
      "Epoch 17, Batch 64, Loss: 0.6312231421470642\n",
      "Epoch 17, Batch 65, Loss: 0.6686227917671204\n",
      "Epoch 17, Batch 66, Loss: 0.6489710211753845\n",
      "Epoch 17, Batch 67, Loss: 0.680959939956665\n",
      "Epoch 17, Batch 68, Loss: 0.6665435433387756\n",
      "Epoch 17, Batch 69, Loss: 0.6997879147529602\n",
      "Epoch 17, Batch 70, Loss: 0.712649405002594\n",
      "Epoch 17, Batch 71, Loss: 0.6877604722976685\n",
      "Epoch 17, Batch 72, Loss: 0.7138421535491943\n",
      "Epoch 17, Batch 73, Loss: 0.7133737802505493\n",
      "Epoch 17, Batch 74, Loss: 0.70878666639328\n",
      "Epoch 17, Batch 75, Loss: 0.7198089361190796\n",
      "Epoch 17, Batch 76, Loss: 0.6953698992729187\n",
      "Epoch 17, Batch 77, Loss: 0.6849855780601501\n",
      "Epoch 17, Batch 78, Loss: 0.6916670799255371\n",
      "Epoch 17, Batch 79, Loss: 0.6426308155059814\n",
      "Epoch 17, Batch 80, Loss: 0.7158039212226868\n",
      "Epoch 17, Batch 81, Loss: 0.7254548668861389\n",
      "Epoch 17, Batch 82, Loss: 0.6630285382270813\n",
      "Epoch 17, Batch 83, Loss: 0.6771810054779053\n",
      "Epoch 17, Batch 84, Loss: 0.6972661018371582\n",
      "Epoch 17, Batch 85, Loss: 0.6990816593170166\n",
      "Epoch 17, Batch 86, Loss: 0.7115368247032166\n",
      "Epoch 17, Batch 87, Loss: 0.6573829650878906\n",
      "Epoch 17, Batch 88, Loss: 0.6650098562240601\n",
      "Epoch 17, Batch 89, Loss: 0.7190725207328796\n",
      "Epoch 17, Batch 90, Loss: 0.705885648727417\n",
      "Epoch 17, Batch 91, Loss: 0.7221608757972717\n",
      "Epoch 17, Batch 92, Loss: 0.6739950776100159\n",
      "Epoch 17, Batch 93, Loss: 0.6448403000831604\n",
      "Epoch 18, Batch 0, Loss: 0.673293948173523\n",
      "Epoch 18, Batch 1, Loss: 0.7055517435073853\n",
      "Epoch 18, Batch 2, Loss: 0.6393877863883972\n",
      "Epoch 18, Batch 3, Loss: 0.657438337802887\n",
      "Epoch 18, Batch 4, Loss: 0.6846206188201904\n",
      "Epoch 18, Batch 5, Loss: 0.6997376084327698\n",
      "Epoch 18, Batch 6, Loss: 0.6683833599090576\n",
      "Epoch 18, Batch 7, Loss: 0.6759465932846069\n",
      "Epoch 18, Batch 8, Loss: 0.6884445548057556\n",
      "Epoch 18, Batch 9, Loss: 0.693199098110199\n",
      "Epoch 18, Batch 10, Loss: 0.6793724894523621\n",
      "Epoch 18, Batch 11, Loss: 0.6740294694900513\n",
      "Epoch 18, Batch 12, Loss: 0.7148964405059814\n",
      "Epoch 18, Batch 13, Loss: 0.6954003572463989\n",
      "Epoch 18, Batch 14, Loss: 0.6516428589820862\n",
      "Epoch 18, Batch 15, Loss: 0.6583573818206787\n",
      "Epoch 18, Batch 16, Loss: 0.6850758790969849\n",
      "Epoch 18, Batch 17, Loss: 0.6633232235908508\n",
      "Epoch 18, Batch 18, Loss: 0.6752001047134399\n",
      "Epoch 18, Batch 19, Loss: 0.6771935224533081\n",
      "Epoch 18, Batch 20, Loss: 0.6773995757102966\n",
      "Epoch 18, Batch 21, Loss: 0.7346210479736328\n",
      "Epoch 18, Batch 22, Loss: 0.7083107233047485\n",
      "Epoch 18, Batch 23, Loss: 0.6677502393722534\n",
      "Epoch 18, Batch 24, Loss: 0.6997671127319336\n",
      "Epoch 18, Batch 25, Loss: 0.712510883808136\n",
      "Epoch 18, Batch 26, Loss: 0.6674774885177612\n",
      "Epoch 18, Batch 27, Loss: 0.6908650398254395\n",
      "Epoch 18, Batch 28, Loss: 0.7283912897109985\n",
      "Epoch 18, Batch 29, Loss: 0.6787559986114502\n",
      "Epoch 18, Batch 30, Loss: 0.6984679698944092\n",
      "Epoch 18, Batch 31, Loss: 0.6213758587837219\n",
      "Epoch 18, Batch 32, Loss: 0.6663998365402222\n",
      "Epoch 18, Batch 33, Loss: 0.6547229290008545\n",
      "Epoch 18, Batch 34, Loss: 0.6843851804733276\n",
      "Epoch 18, Batch 35, Loss: 0.6773781180381775\n",
      "Epoch 18, Batch 36, Loss: 0.6865490674972534\n",
      "Epoch 18, Batch 37, Loss: 0.675167977809906\n",
      "Epoch 18, Batch 38, Loss: 0.6822836399078369\n",
      "Epoch 18, Batch 39, Loss: 0.6696635484695435\n",
      "Epoch 18, Batch 40, Loss: 0.6951884627342224\n",
      "Epoch 18, Batch 41, Loss: 0.6465222239494324\n",
      "Epoch 18, Batch 42, Loss: 0.6863428354263306\n",
      "Epoch 18, Batch 43, Loss: 0.6972970366477966\n",
      "Epoch 18, Batch 44, Loss: 0.6984007954597473\n",
      "Epoch 18, Batch 45, Loss: 0.6925516128540039\n",
      "Epoch 18, Batch 46, Loss: 0.6837893724441528\n",
      "Epoch 18, Batch 47, Loss: 0.6331727504730225\n",
      "Epoch 18, Batch 48, Loss: 0.6618284583091736\n",
      "Epoch 18, Batch 49, Loss: 0.6557800769805908\n",
      "Epoch 18, Batch 50, Loss: 0.6759063601493835\n",
      "Epoch 18, Batch 51, Loss: 0.6769049167633057\n",
      "Epoch 18, Batch 52, Loss: 0.6856186389923096\n",
      "Epoch 18, Batch 53, Loss: 0.6530444025993347\n",
      "Epoch 18, Batch 54, Loss: 0.7029098272323608\n",
      "Epoch 18, Batch 55, Loss: 0.6790774464607239\n",
      "Epoch 18, Batch 56, Loss: 0.6533735990524292\n",
      "Epoch 18, Batch 57, Loss: 0.635104775428772\n",
      "Epoch 18, Batch 58, Loss: 0.6665683388710022\n",
      "Epoch 18, Batch 59, Loss: 0.6725701093673706\n",
      "Epoch 18, Batch 60, Loss: 0.7060006856918335\n",
      "Epoch 18, Batch 61, Loss: 0.6791907548904419\n",
      "Epoch 18, Batch 62, Loss: 0.7012201547622681\n",
      "Epoch 18, Batch 63, Loss: 0.6627107262611389\n",
      "Epoch 18, Batch 64, Loss: 0.634842038154602\n",
      "Epoch 18, Batch 65, Loss: 0.6899697184562683\n",
      "Epoch 18, Batch 66, Loss: 0.688400149345398\n",
      "Epoch 18, Batch 67, Loss: 0.6561497449874878\n",
      "Epoch 18, Batch 68, Loss: 0.697438657283783\n",
      "Epoch 18, Batch 69, Loss: 0.6788662672042847\n",
      "Epoch 18, Batch 70, Loss: 0.6566551923751831\n",
      "Epoch 18, Batch 71, Loss: 0.7485549449920654\n",
      "Epoch 18, Batch 72, Loss: 0.6285925507545471\n",
      "Epoch 18, Batch 73, Loss: 0.6892824769020081\n",
      "Epoch 18, Batch 74, Loss: 0.648807168006897\n",
      "Epoch 18, Batch 75, Loss: 0.637651801109314\n",
      "Epoch 18, Batch 76, Loss: 0.6683324575424194\n",
      "Epoch 18, Batch 77, Loss: 0.6349411606788635\n",
      "Epoch 18, Batch 78, Loss: 0.6652088165283203\n",
      "Epoch 18, Batch 79, Loss: 0.6743547916412354\n",
      "Epoch 18, Batch 80, Loss: 0.6727666258811951\n",
      "Epoch 18, Batch 81, Loss: 0.6434717774391174\n",
      "Epoch 18, Batch 82, Loss: 0.724064290523529\n",
      "Epoch 18, Batch 83, Loss: 0.6694498062133789\n",
      "Epoch 18, Batch 84, Loss: 0.667894721031189\n",
      "Epoch 18, Batch 85, Loss: 0.6603561043739319\n",
      "Epoch 18, Batch 86, Loss: 0.7005094289779663\n",
      "Epoch 18, Batch 87, Loss: 0.7030788660049438\n",
      "Epoch 18, Batch 88, Loss: 0.6961160898208618\n",
      "Epoch 18, Batch 89, Loss: 0.6770991086959839\n",
      "Epoch 18, Batch 90, Loss: 0.629719078540802\n",
      "Epoch 18, Batch 91, Loss: 0.6799434423446655\n",
      "Epoch 18, Batch 92, Loss: 0.6740174889564514\n",
      "Epoch 18, Batch 93, Loss: 0.6565653085708618\n",
      "Epoch 19, Batch 0, Loss: 0.6089321374893188\n",
      "Epoch 19, Batch 1, Loss: 0.7025779485702515\n",
      "Epoch 19, Batch 2, Loss: 0.6403822302818298\n",
      "Epoch 19, Batch 3, Loss: 0.6572862267494202\n",
      "Epoch 19, Batch 4, Loss: 0.688652753829956\n",
      "Epoch 19, Batch 5, Loss: 0.601952850818634\n",
      "Epoch 19, Batch 6, Loss: 0.6678668856620789\n",
      "Epoch 19, Batch 7, Loss: 0.6513159871101379\n",
      "Epoch 19, Batch 8, Loss: 0.6484471559524536\n",
      "Epoch 19, Batch 9, Loss: 0.6582838892936707\n",
      "Epoch 19, Batch 10, Loss: 0.6906625032424927\n",
      "Epoch 19, Batch 11, Loss: 0.6700596809387207\n",
      "Epoch 19, Batch 12, Loss: 0.6703132390975952\n",
      "Epoch 19, Batch 13, Loss: 0.6303945779800415\n",
      "Epoch 19, Batch 14, Loss: 0.722343385219574\n",
      "Epoch 19, Batch 15, Loss: 0.6924632787704468\n",
      "Epoch 19, Batch 16, Loss: 0.6385085582733154\n",
      "Epoch 19, Batch 17, Loss: 0.6688462495803833\n",
      "Epoch 19, Batch 18, Loss: 0.6646681427955627\n",
      "Epoch 19, Batch 19, Loss: 0.6160489320755005\n",
      "Epoch 19, Batch 20, Loss: 0.6678355932235718\n",
      "Epoch 19, Batch 21, Loss: 0.6709480285644531\n",
      "Epoch 19, Batch 22, Loss: 0.6527318358421326\n",
      "Epoch 19, Batch 23, Loss: 0.6479143500328064\n",
      "Epoch 19, Batch 24, Loss: 0.6841734051704407\n",
      "Epoch 19, Batch 25, Loss: 0.6607958078384399\n",
      "Epoch 19, Batch 26, Loss: 0.6820818185806274\n",
      "Epoch 19, Batch 27, Loss: 0.6749289035797119\n",
      "Epoch 19, Batch 28, Loss: 0.6221097111701965\n",
      "Epoch 19, Batch 29, Loss: 0.6846427917480469\n",
      "Epoch 19, Batch 30, Loss: 0.6413754820823669\n",
      "Epoch 19, Batch 31, Loss: 0.6600576639175415\n",
      "Epoch 19, Batch 32, Loss: 0.6409394145011902\n",
      "Epoch 19, Batch 33, Loss: 0.6847235560417175\n",
      "Epoch 19, Batch 34, Loss: 0.6314481496810913\n",
      "Epoch 19, Batch 35, Loss: 0.6464934945106506\n",
      "Epoch 19, Batch 36, Loss: 0.6321991086006165\n",
      "Epoch 19, Batch 37, Loss: 0.6489453911781311\n",
      "Epoch 19, Batch 38, Loss: 0.6579190492630005\n",
      "Epoch 19, Batch 39, Loss: 0.6692489385604858\n",
      "Epoch 19, Batch 40, Loss: 0.6333144903182983\n",
      "Epoch 19, Batch 41, Loss: 0.6211462020874023\n",
      "Epoch 19, Batch 42, Loss: 0.6392127275466919\n",
      "Epoch 19, Batch 43, Loss: 0.6945594549179077\n",
      "Epoch 19, Batch 44, Loss: 0.6352145075798035\n",
      "Epoch 19, Batch 45, Loss: 0.6332039833068848\n",
      "Epoch 19, Batch 46, Loss: 0.6686371564865112\n",
      "Epoch 19, Batch 47, Loss: 0.6464315056800842\n",
      "Epoch 19, Batch 48, Loss: 0.6763930916786194\n",
      "Epoch 19, Batch 49, Loss: 0.678622305393219\n",
      "Epoch 19, Batch 50, Loss: 0.7083961963653564\n",
      "Epoch 19, Batch 51, Loss: 0.6738561987876892\n",
      "Epoch 19, Batch 52, Loss: 0.6382163763046265\n",
      "Epoch 19, Batch 53, Loss: 0.6675512194633484\n",
      "Epoch 19, Batch 54, Loss: 0.6509239077568054\n",
      "Epoch 19, Batch 55, Loss: 0.6792657971382141\n",
      "Epoch 19, Batch 56, Loss: 0.6270154714584351\n",
      "Epoch 19, Batch 57, Loss: 0.6436696648597717\n",
      "Epoch 19, Batch 58, Loss: 0.6613401174545288\n",
      "Epoch 19, Batch 59, Loss: 0.6327757835388184\n",
      "Epoch 19, Batch 60, Loss: 0.6797687411308289\n",
      "Epoch 19, Batch 61, Loss: 0.6569409370422363\n",
      "Epoch 19, Batch 62, Loss: 0.5949596166610718\n",
      "Epoch 19, Batch 63, Loss: 0.6465461254119873\n",
      "Epoch 19, Batch 64, Loss: 0.6226381063461304\n",
      "Epoch 19, Batch 65, Loss: 0.6361622214317322\n",
      "Epoch 19, Batch 66, Loss: 0.6335850954055786\n",
      "Epoch 19, Batch 67, Loss: 0.6403183341026306\n",
      "Epoch 19, Batch 68, Loss: 0.6631176471710205\n",
      "Epoch 19, Batch 69, Loss: 0.6654626131057739\n",
      "Epoch 19, Batch 70, Loss: 0.6821498274803162\n",
      "Epoch 19, Batch 71, Loss: 0.6366828083992004\n",
      "Epoch 19, Batch 72, Loss: 0.6760209798812866\n",
      "Epoch 19, Batch 73, Loss: 0.5994296669960022\n",
      "Epoch 19, Batch 74, Loss: 0.5982152223587036\n",
      "Epoch 19, Batch 75, Loss: 0.6587774753570557\n",
      "Epoch 19, Batch 76, Loss: 0.6497945785522461\n",
      "Epoch 19, Batch 77, Loss: 0.6114569902420044\n",
      "Epoch 19, Batch 78, Loss: 0.689071774482727\n",
      "Epoch 19, Batch 79, Loss: 0.6013270616531372\n",
      "Epoch 19, Batch 80, Loss: 0.7104629874229431\n",
      "Epoch 19, Batch 81, Loss: 0.6524204611778259\n",
      "Epoch 19, Batch 82, Loss: 0.65297532081604\n",
      "Epoch 19, Batch 83, Loss: 0.6277522444725037\n",
      "Epoch 19, Batch 84, Loss: 0.6540640592575073\n",
      "Epoch 19, Batch 85, Loss: 0.6663866639137268\n",
      "Epoch 19, Batch 86, Loss: 0.6315349340438843\n",
      "Epoch 19, Batch 87, Loss: 0.6848664283752441\n",
      "Epoch 19, Batch 88, Loss: 0.6702483296394348\n",
      "Epoch 19, Batch 89, Loss: 0.6411229372024536\n",
      "Epoch 19, Batch 90, Loss: 0.6973673105239868\n",
      "Epoch 19, Batch 91, Loss: 0.6912904977798462\n",
      "Epoch 19, Batch 92, Loss: 0.649571418762207\n",
      "Epoch 19, Batch 93, Loss: 0.6264844536781311\n",
      "Epoch 20, Batch 0, Loss: 0.6630512475967407\n",
      "Epoch 20, Batch 1, Loss: 0.6377271413803101\n",
      "Epoch 20, Batch 2, Loss: 0.6465065479278564\n",
      "Epoch 20, Batch 3, Loss: 0.6653242111206055\n",
      "Epoch 20, Batch 4, Loss: 0.6632398366928101\n",
      "Epoch 20, Batch 5, Loss: 0.648878276348114\n",
      "Epoch 20, Batch 6, Loss: 0.6457017660140991\n",
      "Epoch 20, Batch 7, Loss: 0.6745352745056152\n",
      "Epoch 20, Batch 8, Loss: 0.6364501118659973\n",
      "Epoch 20, Batch 9, Loss: 0.6311907768249512\n",
      "Epoch 20, Batch 10, Loss: 0.6802207231521606\n",
      "Epoch 20, Batch 11, Loss: 0.6334019303321838\n",
      "Epoch 20, Batch 12, Loss: 0.6416046619415283\n",
      "Epoch 20, Batch 13, Loss: 0.6079952120780945\n",
      "Epoch 20, Batch 14, Loss: 0.6445838212966919\n",
      "Epoch 20, Batch 15, Loss: 0.6409304738044739\n",
      "Epoch 20, Batch 16, Loss: 0.6414964199066162\n",
      "Epoch 20, Batch 17, Loss: 0.5960344076156616\n",
      "Epoch 20, Batch 18, Loss: 0.6000153422355652\n",
      "Epoch 20, Batch 19, Loss: 0.6533011198043823\n",
      "Epoch 20, Batch 20, Loss: 0.6618658304214478\n",
      "Epoch 20, Batch 21, Loss: 0.6053293347358704\n",
      "Epoch 20, Batch 22, Loss: 0.6132265329360962\n",
      "Epoch 20, Batch 23, Loss: 0.6585550308227539\n",
      "Epoch 20, Batch 24, Loss: 0.6334809064865112\n",
      "Epoch 20, Batch 25, Loss: 0.6264157295227051\n",
      "Epoch 20, Batch 26, Loss: 0.6356252431869507\n",
      "Epoch 20, Batch 27, Loss: 0.6343475580215454\n",
      "Epoch 20, Batch 28, Loss: 0.6399496793746948\n",
      "Epoch 20, Batch 29, Loss: 0.6624221801757812\n",
      "Epoch 20, Batch 30, Loss: 0.600590705871582\n",
      "Epoch 20, Batch 31, Loss: 0.6663859486579895\n",
      "Epoch 20, Batch 32, Loss: 0.6448424458503723\n",
      "Epoch 20, Batch 33, Loss: 0.6275641918182373\n",
      "Epoch 20, Batch 34, Loss: 0.6927772164344788\n",
      "Epoch 20, Batch 35, Loss: 0.6856807470321655\n",
      "Epoch 20, Batch 36, Loss: 0.6020166873931885\n",
      "Epoch 20, Batch 37, Loss: 0.6272513270378113\n",
      "Epoch 20, Batch 38, Loss: 0.6398367881774902\n",
      "Epoch 20, Batch 39, Loss: 0.640498697757721\n",
      "Epoch 20, Batch 40, Loss: 0.6378260850906372\n",
      "Epoch 20, Batch 41, Loss: 0.6380398273468018\n",
      "Epoch 20, Batch 42, Loss: 0.6344918012619019\n",
      "Epoch 20, Batch 43, Loss: 0.6464928984642029\n",
      "Epoch 20, Batch 44, Loss: 0.6214441061019897\n",
      "Epoch 20, Batch 45, Loss: 0.6704623699188232\n",
      "Epoch 20, Batch 46, Loss: 0.6466233134269714\n",
      "Epoch 20, Batch 47, Loss: 0.6350095868110657\n",
      "Epoch 20, Batch 48, Loss: 0.6635870933532715\n",
      "Epoch 20, Batch 49, Loss: 0.6310505867004395\n",
      "Epoch 20, Batch 50, Loss: 0.678253173828125\n",
      "Epoch 20, Batch 51, Loss: 0.6412712335586548\n",
      "Epoch 20, Batch 52, Loss: 0.5842772126197815\n",
      "Epoch 20, Batch 53, Loss: 0.6002297401428223\n",
      "Epoch 20, Batch 54, Loss: 0.6604279279708862\n",
      "Epoch 20, Batch 55, Loss: 0.625015914440155\n",
      "Epoch 20, Batch 56, Loss: 0.6278480291366577\n",
      "Epoch 20, Batch 57, Loss: 0.6382081508636475\n",
      "Epoch 20, Batch 58, Loss: 0.6786497235298157\n",
      "Epoch 20, Batch 59, Loss: 0.6272976994514465\n",
      "Epoch 20, Batch 60, Loss: 0.6298103928565979\n",
      "Epoch 20, Batch 61, Loss: 0.6472364664077759\n",
      "Epoch 20, Batch 62, Loss: 0.6095144152641296\n",
      "Epoch 20, Batch 63, Loss: 0.6180251836776733\n",
      "Epoch 20, Batch 64, Loss: 0.628049373626709\n",
      "Epoch 20, Batch 65, Loss: 0.6490527391433716\n",
      "Epoch 20, Batch 66, Loss: 0.6051999926567078\n",
      "Epoch 20, Batch 67, Loss: 0.632351279258728\n",
      "Epoch 20, Batch 68, Loss: 0.6547654271125793\n",
      "Epoch 20, Batch 69, Loss: 0.6306949257850647\n",
      "Epoch 20, Batch 70, Loss: 0.6385699510574341\n",
      "Epoch 20, Batch 71, Loss: 0.6167501211166382\n",
      "Epoch 20, Batch 72, Loss: 0.617232620716095\n",
      "Epoch 20, Batch 73, Loss: 0.5974301099777222\n",
      "Epoch 20, Batch 74, Loss: 0.6552062034606934\n",
      "Epoch 20, Batch 75, Loss: 0.600712776184082\n",
      "Epoch 20, Batch 76, Loss: 0.6121786236763\n",
      "Epoch 20, Batch 77, Loss: 0.6489360928535461\n",
      "Epoch 20, Batch 78, Loss: 0.621614396572113\n",
      "Epoch 20, Batch 79, Loss: 0.5770242214202881\n",
      "Epoch 20, Batch 80, Loss: 0.614834725856781\n",
      "Epoch 20, Batch 81, Loss: 0.6533368229866028\n",
      "Epoch 20, Batch 82, Loss: 0.6146360039710999\n",
      "Epoch 20, Batch 83, Loss: 0.6497825384140015\n",
      "Epoch 20, Batch 84, Loss: 0.6482665538787842\n",
      "Epoch 20, Batch 85, Loss: 0.5734434127807617\n",
      "Epoch 20, Batch 86, Loss: 0.6332051157951355\n",
      "Epoch 20, Batch 87, Loss: 0.6177430748939514\n",
      "Epoch 20, Batch 88, Loss: 0.5995883345603943\n",
      "Epoch 20, Batch 89, Loss: 0.6730020046234131\n",
      "Epoch 20, Batch 90, Loss: 0.6466675996780396\n",
      "Epoch 20, Batch 91, Loss: 0.5923230051994324\n",
      "Epoch 20, Batch 92, Loss: 0.6070548295974731\n",
      "Epoch 20, Batch 93, Loss: 0.6958615183830261\n",
      "Epoch 21, Batch 0, Loss: 0.6630125045776367\n",
      "Epoch 21, Batch 1, Loss: 0.5901144742965698\n",
      "Epoch 21, Batch 2, Loss: 0.6403641104698181\n",
      "Epoch 21, Batch 3, Loss: 0.6237828135490417\n",
      "Epoch 21, Batch 4, Loss: 0.6510323882102966\n",
      "Epoch 21, Batch 5, Loss: 0.5960749387741089\n",
      "Epoch 21, Batch 6, Loss: 0.5994964838027954\n",
      "Epoch 21, Batch 7, Loss: 0.6088494062423706\n",
      "Epoch 21, Batch 8, Loss: 0.5786207318305969\n",
      "Epoch 21, Batch 9, Loss: 0.6081486940383911\n",
      "Epoch 21, Batch 10, Loss: 0.6312935948371887\n",
      "Epoch 21, Batch 11, Loss: 0.6044436097145081\n",
      "Epoch 21, Batch 12, Loss: 0.611059844493866\n",
      "Epoch 21, Batch 13, Loss: 0.6398441195487976\n",
      "Epoch 21, Batch 14, Loss: 0.6135808825492859\n",
      "Epoch 21, Batch 15, Loss: 0.5632126927375793\n",
      "Epoch 21, Batch 16, Loss: 0.6196871995925903\n",
      "Epoch 21, Batch 17, Loss: 0.630165696144104\n",
      "Epoch 21, Batch 18, Loss: 0.6382032632827759\n",
      "Epoch 21, Batch 19, Loss: 0.658960223197937\n",
      "Epoch 21, Batch 20, Loss: 0.5787644982337952\n",
      "Epoch 21, Batch 21, Loss: 0.6553710103034973\n",
      "Epoch 21, Batch 22, Loss: 0.6221047639846802\n",
      "Epoch 21, Batch 23, Loss: 0.6063514351844788\n",
      "Epoch 21, Batch 24, Loss: 0.6092931032180786\n",
      "Epoch 21, Batch 25, Loss: 0.6367532014846802\n",
      "Epoch 21, Batch 26, Loss: 0.6375105381011963\n",
      "Epoch 21, Batch 27, Loss: 0.6597640514373779\n",
      "Epoch 21, Batch 28, Loss: 0.6047691106796265\n",
      "Epoch 21, Batch 29, Loss: 0.6666476726531982\n",
      "Epoch 21, Batch 30, Loss: 0.6252012848854065\n",
      "Epoch 21, Batch 31, Loss: 0.6044759154319763\n",
      "Epoch 21, Batch 32, Loss: 0.6029540300369263\n",
      "Epoch 21, Batch 33, Loss: 0.6308812499046326\n",
      "Epoch 21, Batch 34, Loss: 0.5741480588912964\n",
      "Epoch 21, Batch 35, Loss: 0.6160531640052795\n",
      "Epoch 21, Batch 36, Loss: 0.6106147766113281\n",
      "Epoch 21, Batch 37, Loss: 0.5985020399093628\n",
      "Epoch 21, Batch 38, Loss: 0.638018786907196\n",
      "Epoch 21, Batch 39, Loss: 0.6149667501449585\n",
      "Epoch 21, Batch 40, Loss: 0.6223373413085938\n",
      "Epoch 21, Batch 41, Loss: 0.676727831363678\n",
      "Epoch 21, Batch 42, Loss: 0.604836642742157\n",
      "Epoch 21, Batch 43, Loss: 0.6087150573730469\n",
      "Epoch 21, Batch 44, Loss: 0.6339215040206909\n",
      "Epoch 21, Batch 45, Loss: 0.693540632724762\n",
      "Epoch 21, Batch 46, Loss: 0.6414182186126709\n",
      "Epoch 21, Batch 47, Loss: 0.6244975328445435\n",
      "Epoch 21, Batch 48, Loss: 0.6119357347488403\n",
      "Epoch 21, Batch 49, Loss: 0.608967661857605\n",
      "Epoch 21, Batch 50, Loss: 0.6375104784965515\n",
      "Epoch 21, Batch 51, Loss: 0.5993771553039551\n",
      "Epoch 21, Batch 52, Loss: 0.6316730976104736\n",
      "Epoch 21, Batch 53, Loss: 0.5954948663711548\n",
      "Epoch 21, Batch 54, Loss: 0.6407533288002014\n",
      "Epoch 21, Batch 55, Loss: 0.6082994341850281\n",
      "Epoch 21, Batch 56, Loss: 0.6085865497589111\n",
      "Epoch 21, Batch 57, Loss: 0.6131112575531006\n",
      "Epoch 21, Batch 58, Loss: 0.5813974142074585\n",
      "Epoch 21, Batch 59, Loss: 0.6325639486312866\n",
      "Epoch 21, Batch 60, Loss: 0.5915358662605286\n",
      "Epoch 21, Batch 61, Loss: 0.630436360836029\n",
      "Epoch 21, Batch 62, Loss: 0.6195425391197205\n",
      "Epoch 21, Batch 63, Loss: 0.5870625376701355\n",
      "Epoch 21, Batch 64, Loss: 0.5981330871582031\n",
      "Epoch 21, Batch 65, Loss: 0.6450431942939758\n",
      "Epoch 21, Batch 66, Loss: 0.5935086607933044\n",
      "Epoch 21, Batch 67, Loss: 0.607872486114502\n",
      "Epoch 21, Batch 68, Loss: 0.626338005065918\n",
      "Epoch 21, Batch 69, Loss: 0.5920639038085938\n",
      "Epoch 21, Batch 70, Loss: 0.628948986530304\n",
      "Epoch 21, Batch 71, Loss: 0.6149060726165771\n",
      "Epoch 21, Batch 72, Loss: 0.611667275428772\n",
      "Epoch 21, Batch 73, Loss: 0.6024013757705688\n",
      "Epoch 21, Batch 74, Loss: 0.6437121629714966\n",
      "Epoch 21, Batch 75, Loss: 0.632649838924408\n",
      "Epoch 21, Batch 76, Loss: 0.585473895072937\n",
      "Epoch 21, Batch 77, Loss: 0.5803757905960083\n",
      "Epoch 21, Batch 78, Loss: 0.6179323196411133\n",
      "Epoch 21, Batch 79, Loss: 0.642276406288147\n",
      "Epoch 21, Batch 80, Loss: 0.6297163367271423\n",
      "Epoch 21, Batch 81, Loss: 0.6346909999847412\n",
      "Epoch 21, Batch 82, Loss: 0.6312075853347778\n",
      "Epoch 21, Batch 83, Loss: 0.6001044511795044\n",
      "Epoch 21, Batch 84, Loss: 0.6096809506416321\n",
      "Epoch 21, Batch 85, Loss: 0.5739103555679321\n",
      "Epoch 21, Batch 86, Loss: 0.6302646398544312\n",
      "Epoch 21, Batch 87, Loss: 0.5957727432250977\n",
      "Epoch 21, Batch 88, Loss: 0.6321888566017151\n",
      "Epoch 21, Batch 89, Loss: 0.6278955936431885\n",
      "Epoch 21, Batch 90, Loss: 0.6260537505149841\n",
      "Epoch 21, Batch 91, Loss: 0.5752864480018616\n",
      "Epoch 21, Batch 92, Loss: 0.6036060452461243\n",
      "Epoch 21, Batch 93, Loss: 0.6069464087486267\n",
      "Epoch 22, Batch 0, Loss: 0.6438487768173218\n",
      "Epoch 22, Batch 1, Loss: 0.5993943214416504\n",
      "Epoch 22, Batch 2, Loss: 0.6164339780807495\n",
      "Epoch 22, Batch 3, Loss: 0.6147588491439819\n",
      "Epoch 22, Batch 4, Loss: 0.6267178654670715\n",
      "Epoch 22, Batch 5, Loss: 0.5967443585395813\n",
      "Epoch 22, Batch 6, Loss: 0.5857168436050415\n",
      "Epoch 22, Batch 7, Loss: 0.6282273530960083\n",
      "Epoch 22, Batch 8, Loss: 0.628247857093811\n",
      "Epoch 22, Batch 9, Loss: 0.5889230966567993\n",
      "Epoch 22, Batch 10, Loss: 0.607343316078186\n",
      "Epoch 22, Batch 11, Loss: 0.5698882341384888\n",
      "Epoch 22, Batch 12, Loss: 0.6224230527877808\n",
      "Epoch 22, Batch 13, Loss: 0.5858654379844666\n",
      "Epoch 22, Batch 14, Loss: 0.6118298768997192\n",
      "Epoch 22, Batch 15, Loss: 0.5892717242240906\n",
      "Epoch 22, Batch 16, Loss: 0.6233824491500854\n",
      "Epoch 22, Batch 17, Loss: 0.6404527425765991\n",
      "Epoch 22, Batch 18, Loss: 0.6058053374290466\n",
      "Epoch 22, Batch 19, Loss: 0.6481192111968994\n",
      "Epoch 22, Batch 20, Loss: 0.6399562954902649\n",
      "Epoch 22, Batch 21, Loss: 0.5586915612220764\n",
      "Epoch 22, Batch 22, Loss: 0.602529764175415\n",
      "Epoch 22, Batch 23, Loss: 0.641717255115509\n",
      "Epoch 22, Batch 24, Loss: 0.619245171546936\n",
      "Epoch 22, Batch 25, Loss: 0.5973648428916931\n",
      "Epoch 22, Batch 26, Loss: 0.624129056930542\n",
      "Epoch 22, Batch 27, Loss: 0.6100786328315735\n",
      "Epoch 22, Batch 28, Loss: 0.5240853428840637\n",
      "Epoch 22, Batch 29, Loss: 0.5996541976928711\n",
      "Epoch 22, Batch 30, Loss: 0.6199471950531006\n",
      "Epoch 22, Batch 31, Loss: 0.6025891900062561\n",
      "Epoch 22, Batch 32, Loss: 0.5893794894218445\n",
      "Epoch 22, Batch 33, Loss: 0.5761400461196899\n",
      "Epoch 22, Batch 34, Loss: 0.6166349649429321\n",
      "Epoch 22, Batch 35, Loss: 0.6423448324203491\n",
      "Epoch 22, Batch 36, Loss: 0.5928089022636414\n",
      "Epoch 22, Batch 37, Loss: 0.616265594959259\n",
      "Epoch 22, Batch 38, Loss: 0.6091598868370056\n",
      "Epoch 22, Batch 39, Loss: 0.6021618843078613\n",
      "Epoch 22, Batch 40, Loss: 0.6547091603279114\n",
      "Epoch 22, Batch 41, Loss: 0.5971438884735107\n",
      "Epoch 22, Batch 42, Loss: 0.5986995100975037\n",
      "Epoch 22, Batch 43, Loss: 0.5685847997665405\n",
      "Epoch 22, Batch 44, Loss: 0.5773715972900391\n",
      "Epoch 22, Batch 45, Loss: 0.6035096049308777\n",
      "Epoch 22, Batch 46, Loss: 0.5920190215110779\n",
      "Epoch 22, Batch 47, Loss: 0.6013301014900208\n",
      "Epoch 22, Batch 48, Loss: 0.6260455846786499\n",
      "Epoch 22, Batch 49, Loss: 0.6040361523628235\n",
      "Epoch 22, Batch 50, Loss: 0.619037926197052\n",
      "Epoch 22, Batch 51, Loss: 0.6207035779953003\n",
      "Epoch 22, Batch 52, Loss: 0.5635160803794861\n",
      "Epoch 22, Batch 53, Loss: 0.6147345304489136\n",
      "Epoch 22, Batch 54, Loss: 0.6268708109855652\n",
      "Epoch 22, Batch 55, Loss: 0.5528150796890259\n",
      "Epoch 22, Batch 56, Loss: 0.6370726823806763\n",
      "Epoch 22, Batch 57, Loss: 0.5544780492782593\n",
      "Epoch 22, Batch 58, Loss: 0.6248890161514282\n",
      "Epoch 22, Batch 59, Loss: 0.6201754808425903\n",
      "Epoch 22, Batch 60, Loss: 0.5753934383392334\n",
      "Epoch 22, Batch 61, Loss: 0.5873191952705383\n",
      "Epoch 22, Batch 62, Loss: 0.598495364189148\n",
      "Epoch 22, Batch 63, Loss: 0.6326006054878235\n",
      "Epoch 22, Batch 64, Loss: 0.6142361760139465\n",
      "Epoch 22, Batch 65, Loss: 0.6417695879936218\n",
      "Epoch 22, Batch 66, Loss: 0.601152777671814\n",
      "Epoch 22, Batch 67, Loss: 0.6092833280563354\n",
      "Epoch 22, Batch 68, Loss: 0.6137612462043762\n",
      "Epoch 22, Batch 69, Loss: 0.608947217464447\n",
      "Epoch 22, Batch 70, Loss: 0.6034026145935059\n",
      "Epoch 22, Batch 71, Loss: 0.5731470584869385\n",
      "Epoch 22, Batch 72, Loss: 0.5939011573791504\n",
      "Epoch 22, Batch 73, Loss: 0.5939489006996155\n",
      "Epoch 22, Batch 74, Loss: 0.5713474750518799\n",
      "Epoch 22, Batch 75, Loss: 0.5933881998062134\n",
      "Epoch 22, Batch 76, Loss: 0.5804833173751831\n",
      "Epoch 22, Batch 77, Loss: 0.5754721164703369\n",
      "Epoch 22, Batch 78, Loss: 0.6113439202308655\n",
      "Epoch 22, Batch 79, Loss: 0.6003653407096863\n",
      "Epoch 22, Batch 80, Loss: 0.5919045805931091\n",
      "Epoch 22, Batch 81, Loss: 0.5525519251823425\n",
      "Epoch 22, Batch 82, Loss: 0.6082316040992737\n",
      "Epoch 22, Batch 83, Loss: 0.5616816282272339\n",
      "Epoch 22, Batch 84, Loss: 0.6032924056053162\n",
      "Epoch 22, Batch 85, Loss: 0.5694354772567749\n",
      "Epoch 22, Batch 86, Loss: 0.6079895496368408\n",
      "Epoch 22, Batch 87, Loss: 0.5924336314201355\n",
      "Epoch 22, Batch 88, Loss: 0.5863202214241028\n",
      "Epoch 22, Batch 89, Loss: 0.5576932430267334\n",
      "Epoch 22, Batch 90, Loss: 0.6187118291854858\n",
      "Epoch 22, Batch 91, Loss: 0.5487827062606812\n",
      "Epoch 22, Batch 92, Loss: 0.5749028325080872\n",
      "Epoch 22, Batch 93, Loss: 0.6381080150604248\n",
      "Epoch 23, Batch 0, Loss: 0.5641478300094604\n",
      "Epoch 23, Batch 1, Loss: 0.5672025084495544\n",
      "Epoch 23, Batch 2, Loss: 0.6017826199531555\n",
      "Epoch 23, Batch 3, Loss: 0.5637468099594116\n",
      "Epoch 23, Batch 4, Loss: 0.6596742272377014\n",
      "Epoch 23, Batch 5, Loss: 0.6077470779418945\n",
      "Epoch 23, Batch 6, Loss: 0.5710517168045044\n",
      "Epoch 23, Batch 7, Loss: 0.636337161064148\n",
      "Epoch 23, Batch 8, Loss: 0.6172398328781128\n",
      "Epoch 23, Batch 9, Loss: 0.5633935928344727\n",
      "Epoch 23, Batch 10, Loss: 0.5584447383880615\n",
      "Epoch 23, Batch 11, Loss: 0.604833722114563\n",
      "Epoch 23, Batch 12, Loss: 0.5844534635543823\n",
      "Epoch 23, Batch 13, Loss: 0.5790604948997498\n",
      "Epoch 23, Batch 14, Loss: 0.5837987065315247\n",
      "Epoch 23, Batch 15, Loss: 0.5688042044639587\n",
      "Epoch 23, Batch 16, Loss: 0.6173051595687866\n",
      "Epoch 23, Batch 17, Loss: 0.5802199244499207\n",
      "Epoch 23, Batch 18, Loss: 0.5975489616394043\n",
      "Epoch 23, Batch 19, Loss: 0.6328821778297424\n",
      "Epoch 23, Batch 20, Loss: 0.6243771910667419\n",
      "Epoch 23, Batch 21, Loss: 0.5825613737106323\n",
      "Epoch 23, Batch 22, Loss: 0.5994106531143188\n",
      "Epoch 23, Batch 23, Loss: 0.5843247175216675\n",
      "Epoch 23, Batch 24, Loss: 0.62823086977005\n",
      "Epoch 23, Batch 25, Loss: 0.6098594665527344\n",
      "Epoch 23, Batch 26, Loss: 0.5807417631149292\n",
      "Epoch 23, Batch 27, Loss: 0.6494548916816711\n",
      "Epoch 23, Batch 28, Loss: 0.5966176390647888\n",
      "Epoch 23, Batch 29, Loss: 0.5878416299819946\n",
      "Epoch 23, Batch 30, Loss: 0.5868659615516663\n",
      "Epoch 23, Batch 31, Loss: 0.5574242472648621\n",
      "Epoch 23, Batch 32, Loss: 0.6081458330154419\n",
      "Epoch 23, Batch 33, Loss: 0.6080519556999207\n",
      "Epoch 23, Batch 34, Loss: 0.5943268537521362\n",
      "Epoch 23, Batch 35, Loss: 0.5812996625900269\n",
      "Epoch 23, Batch 36, Loss: 0.5721116662025452\n",
      "Epoch 23, Batch 37, Loss: 0.5830584764480591\n",
      "Epoch 23, Batch 38, Loss: 0.5686503052711487\n",
      "Epoch 23, Batch 39, Loss: 0.6183355450630188\n",
      "Epoch 23, Batch 40, Loss: 0.5346004962921143\n",
      "Epoch 23, Batch 41, Loss: 0.571528434753418\n",
      "Epoch 23, Batch 42, Loss: 0.5333539247512817\n",
      "Epoch 23, Batch 43, Loss: 0.5725680589675903\n",
      "Epoch 23, Batch 44, Loss: 0.6061097383499146\n",
      "Epoch 23, Batch 45, Loss: 0.5364767909049988\n",
      "Epoch 23, Batch 46, Loss: 0.5484355688095093\n",
      "Epoch 23, Batch 47, Loss: 0.5778660774230957\n",
      "Epoch 23, Batch 48, Loss: 0.6467217206954956\n",
      "Epoch 23, Batch 49, Loss: 0.6360852122306824\n",
      "Epoch 23, Batch 50, Loss: 0.6041198372840881\n",
      "Epoch 23, Batch 51, Loss: 0.5587518215179443\n",
      "Epoch 23, Batch 52, Loss: 0.5662323236465454\n",
      "Epoch 23, Batch 53, Loss: 0.594176709651947\n",
      "Epoch 23, Batch 54, Loss: 0.5352723002433777\n",
      "Epoch 23, Batch 55, Loss: 0.5252140164375305\n",
      "Epoch 23, Batch 56, Loss: 0.6117004156112671\n",
      "Epoch 23, Batch 57, Loss: 0.622978925704956\n",
      "Epoch 23, Batch 58, Loss: 0.6365573406219482\n",
      "Epoch 23, Batch 59, Loss: 0.5768960118293762\n",
      "Epoch 23, Batch 60, Loss: 0.539239764213562\n",
      "Epoch 23, Batch 61, Loss: 0.6189661622047424\n",
      "Epoch 23, Batch 62, Loss: 0.59964519739151\n",
      "Epoch 23, Batch 63, Loss: 0.5889550447463989\n",
      "Epoch 23, Batch 64, Loss: 0.61753910779953\n",
      "Epoch 23, Batch 65, Loss: 0.5767704844474792\n",
      "Epoch 23, Batch 66, Loss: 0.6080473065376282\n",
      "Epoch 23, Batch 67, Loss: 0.554117739200592\n",
      "Epoch 23, Batch 68, Loss: 0.5871824622154236\n",
      "Epoch 23, Batch 69, Loss: 0.6093662977218628\n",
      "Epoch 23, Batch 70, Loss: 0.5711433291435242\n",
      "Epoch 23, Batch 71, Loss: 0.5922929048538208\n",
      "Epoch 23, Batch 72, Loss: 0.5745631456375122\n",
      "Epoch 23, Batch 73, Loss: 0.5201572179794312\n",
      "Epoch 23, Batch 74, Loss: 0.5920571684837341\n",
      "Epoch 23, Batch 75, Loss: 0.6273971199989319\n",
      "Epoch 23, Batch 76, Loss: 0.5556820034980774\n",
      "Epoch 23, Batch 77, Loss: 0.5455186367034912\n",
      "Epoch 23, Batch 78, Loss: 0.5348551273345947\n",
      "Epoch 23, Batch 79, Loss: 0.6313345432281494\n",
      "Epoch 23, Batch 80, Loss: 0.572535514831543\n",
      "Epoch 23, Batch 81, Loss: 0.630276083946228\n",
      "Epoch 23, Batch 82, Loss: 0.5709782838821411\n",
      "Epoch 23, Batch 83, Loss: 0.605581521987915\n",
      "Epoch 23, Batch 84, Loss: 0.5586715936660767\n",
      "Epoch 23, Batch 85, Loss: 0.5918399095535278\n",
      "Epoch 23, Batch 86, Loss: 0.5786363482475281\n",
      "Epoch 23, Batch 87, Loss: 0.6200629472732544\n",
      "Epoch 23, Batch 88, Loss: 0.5643284916877747\n",
      "Epoch 23, Batch 89, Loss: 0.5765973925590515\n",
      "Epoch 23, Batch 90, Loss: 0.5638216137886047\n",
      "Epoch 23, Batch 91, Loss: 0.579094648361206\n",
      "Epoch 23, Batch 92, Loss: 0.5706185102462769\n",
      "Epoch 23, Batch 93, Loss: 0.5655059218406677\n",
      "Epoch 24, Batch 0, Loss: 0.6032987236976624\n",
      "Epoch 24, Batch 1, Loss: 0.5960831046104431\n",
      "Epoch 24, Batch 2, Loss: 0.5399405360221863\n",
      "Epoch 24, Batch 3, Loss: 0.6190603971481323\n",
      "Epoch 24, Batch 4, Loss: 0.5900530815124512\n",
      "Epoch 24, Batch 5, Loss: 0.6093674898147583\n",
      "Epoch 24, Batch 6, Loss: 0.5819004774093628\n",
      "Epoch 24, Batch 7, Loss: 0.606495201587677\n",
      "Epoch 24, Batch 8, Loss: 0.5704748034477234\n",
      "Epoch 24, Batch 9, Loss: 0.5975065231323242\n",
      "Epoch 24, Batch 10, Loss: 0.6033288836479187\n",
      "Epoch 24, Batch 11, Loss: 0.6091428399085999\n",
      "Epoch 24, Batch 12, Loss: 0.5263758897781372\n",
      "Epoch 24, Batch 13, Loss: 0.5885747671127319\n",
      "Epoch 24, Batch 14, Loss: 0.5530892610549927\n",
      "Epoch 24, Batch 15, Loss: 0.5950305461883545\n",
      "Epoch 24, Batch 16, Loss: 0.6162436008453369\n",
      "Epoch 24, Batch 17, Loss: 0.5733472108840942\n",
      "Epoch 24, Batch 18, Loss: 0.5503728985786438\n",
      "Epoch 24, Batch 19, Loss: 0.5371348261833191\n",
      "Epoch 24, Batch 20, Loss: 0.5592163801193237\n",
      "Epoch 24, Batch 21, Loss: 0.5873531103134155\n",
      "Epoch 24, Batch 22, Loss: 0.5476846098899841\n",
      "Epoch 24, Batch 23, Loss: 0.6304494738578796\n",
      "Epoch 24, Batch 24, Loss: 0.58118736743927\n",
      "Epoch 24, Batch 25, Loss: 0.5985592007637024\n",
      "Epoch 24, Batch 26, Loss: 0.5781908631324768\n",
      "Epoch 24, Batch 27, Loss: 0.5821799635887146\n",
      "Epoch 24, Batch 28, Loss: 0.5231716632843018\n",
      "Epoch 24, Batch 29, Loss: 0.5443353652954102\n",
      "Epoch 24, Batch 30, Loss: 0.587682843208313\n",
      "Epoch 24, Batch 31, Loss: 0.574201226234436\n",
      "Epoch 24, Batch 32, Loss: 0.5731862783432007\n",
      "Epoch 24, Batch 33, Loss: 0.593830943107605\n",
      "Epoch 24, Batch 34, Loss: 0.5803939700126648\n",
      "Epoch 24, Batch 35, Loss: 0.5405882000923157\n",
      "Epoch 24, Batch 36, Loss: 0.5974563360214233\n",
      "Epoch 24, Batch 37, Loss: 0.6093703508377075\n",
      "Epoch 24, Batch 38, Loss: 0.5793323516845703\n",
      "Epoch 24, Batch 39, Loss: 0.6192386150360107\n",
      "Epoch 24, Batch 40, Loss: 0.5287712216377258\n",
      "Epoch 24, Batch 41, Loss: 0.5785395503044128\n",
      "Epoch 24, Batch 42, Loss: 0.5739802122116089\n",
      "Epoch 24, Batch 43, Loss: 0.5431557297706604\n",
      "Epoch 24, Batch 44, Loss: 0.5761634707450867\n",
      "Epoch 24, Batch 45, Loss: 0.5761843919754028\n",
      "Epoch 24, Batch 46, Loss: 0.525185763835907\n",
      "Epoch 24, Batch 47, Loss: 0.5758044123649597\n",
      "Epoch 24, Batch 48, Loss: 0.6022858023643494\n",
      "Epoch 24, Batch 49, Loss: 0.5988548994064331\n",
      "Epoch 24, Batch 50, Loss: 0.5884074568748474\n",
      "Epoch 24, Batch 51, Loss: 0.5650287866592407\n",
      "Epoch 24, Batch 52, Loss: 0.5918604135513306\n",
      "Epoch 24, Batch 53, Loss: 0.5552576780319214\n",
      "Epoch 24, Batch 54, Loss: 0.5663236379623413\n",
      "Epoch 24, Batch 55, Loss: 0.5920799970626831\n",
      "Epoch 24, Batch 56, Loss: 0.5695713758468628\n",
      "Epoch 24, Batch 57, Loss: 0.6353965997695923\n",
      "Epoch 24, Batch 58, Loss: 0.5504500269889832\n",
      "Epoch 24, Batch 59, Loss: 0.6005080938339233\n",
      "Epoch 24, Batch 60, Loss: 0.5491955280303955\n",
      "Epoch 24, Batch 61, Loss: 0.5997150540351868\n",
      "Epoch 24, Batch 62, Loss: 0.5912793278694153\n",
      "Epoch 24, Batch 63, Loss: 0.541110634803772\n",
      "Epoch 24, Batch 64, Loss: 0.5934544801712036\n",
      "Epoch 24, Batch 65, Loss: 0.569983184337616\n",
      "Epoch 24, Batch 66, Loss: 0.6128220558166504\n",
      "Epoch 24, Batch 67, Loss: 0.5769888758659363\n",
      "Epoch 24, Batch 68, Loss: 0.5803749561309814\n",
      "Epoch 24, Batch 69, Loss: 0.5946153998374939\n",
      "Epoch 24, Batch 70, Loss: 0.568798840045929\n",
      "Epoch 24, Batch 71, Loss: 0.5711507201194763\n",
      "Epoch 24, Batch 72, Loss: 0.5998954176902771\n",
      "Epoch 24, Batch 73, Loss: 0.5425595045089722\n",
      "Epoch 24, Batch 74, Loss: 0.5293270349502563\n",
      "Epoch 24, Batch 75, Loss: 0.5264315605163574\n",
      "Epoch 24, Batch 76, Loss: 0.554521918296814\n",
      "Epoch 24, Batch 77, Loss: 0.5600838661193848\n",
      "Epoch 24, Batch 78, Loss: 0.5983474850654602\n",
      "Epoch 24, Batch 79, Loss: 0.4787956774234772\n",
      "Epoch 24, Batch 80, Loss: 0.5429823398590088\n",
      "Epoch 24, Batch 81, Loss: 0.5469812154769897\n",
      "Epoch 24, Batch 82, Loss: 0.5541064739227295\n",
      "Epoch 24, Batch 83, Loss: 0.5549366474151611\n",
      "Epoch 24, Batch 84, Loss: 0.5615574717521667\n",
      "Epoch 24, Batch 85, Loss: 0.5946276187896729\n",
      "Epoch 24, Batch 86, Loss: 0.6063938140869141\n",
      "Epoch 24, Batch 87, Loss: 0.5078085064888\n",
      "Epoch 24, Batch 88, Loss: 0.564237117767334\n",
      "Epoch 24, Batch 89, Loss: 0.5233715772628784\n",
      "Epoch 24, Batch 90, Loss: 0.5701097846031189\n",
      "Epoch 24, Batch 91, Loss: 0.5565820336341858\n",
      "Epoch 24, Batch 92, Loss: 0.5838724374771118\n",
      "Epoch 24, Batch 93, Loss: 0.5462185740470886\n",
      "Epoch 25, Batch 0, Loss: 0.5905656814575195\n",
      "Epoch 25, Batch 1, Loss: 0.5195015072822571\n",
      "Epoch 25, Batch 2, Loss: 0.5348861813545227\n",
      "Epoch 25, Batch 3, Loss: 0.5742365717887878\n",
      "Epoch 25, Batch 4, Loss: 0.5778413414955139\n",
      "Epoch 25, Batch 5, Loss: 0.5397917628288269\n",
      "Epoch 25, Batch 6, Loss: 0.5636440515518188\n",
      "Epoch 25, Batch 7, Loss: 0.549372136592865\n",
      "Epoch 25, Batch 8, Loss: 0.6018944978713989\n",
      "Epoch 25, Batch 9, Loss: 0.5925469398498535\n",
      "Epoch 25, Batch 10, Loss: 0.5792690515518188\n",
      "Epoch 25, Batch 11, Loss: 0.5733763575553894\n",
      "Epoch 25, Batch 12, Loss: 0.5545095205307007\n",
      "Epoch 25, Batch 13, Loss: 0.5697740316390991\n",
      "Epoch 25, Batch 14, Loss: 0.5616587400436401\n",
      "Epoch 25, Batch 15, Loss: 0.5647715330123901\n",
      "Epoch 25, Batch 16, Loss: 0.5380105376243591\n",
      "Epoch 25, Batch 17, Loss: 0.5702204704284668\n",
      "Epoch 25, Batch 18, Loss: 0.5821173787117004\n",
      "Epoch 25, Batch 19, Loss: 0.5373340845108032\n",
      "Epoch 25, Batch 20, Loss: 0.590633749961853\n",
      "Epoch 25, Batch 21, Loss: 0.5618426203727722\n",
      "Epoch 25, Batch 22, Loss: 0.5517878532409668\n",
      "Epoch 25, Batch 23, Loss: 0.5649248957633972\n",
      "Epoch 25, Batch 24, Loss: 0.5415393710136414\n",
      "Epoch 25, Batch 25, Loss: 0.5887668132781982\n",
      "Epoch 25, Batch 26, Loss: 0.591774582862854\n",
      "Epoch 25, Batch 27, Loss: 0.5855449438095093\n",
      "Epoch 25, Batch 28, Loss: 0.5688419938087463\n",
      "Epoch 25, Batch 29, Loss: 0.5723134875297546\n",
      "Epoch 25, Batch 30, Loss: 0.5425865054130554\n",
      "Epoch 25, Batch 31, Loss: 0.5259566903114319\n",
      "Epoch 25, Batch 32, Loss: 0.5691589713096619\n",
      "Epoch 25, Batch 33, Loss: 0.588117241859436\n",
      "Epoch 25, Batch 34, Loss: 0.5467050075531006\n",
      "Epoch 25, Batch 35, Loss: 0.5718095898628235\n",
      "Epoch 25, Batch 36, Loss: 0.5295459032058716\n",
      "Epoch 25, Batch 37, Loss: 0.5628910660743713\n",
      "Epoch 25, Batch 38, Loss: 0.5616024732589722\n",
      "Epoch 25, Batch 39, Loss: 0.5523984432220459\n",
      "Epoch 25, Batch 40, Loss: 0.5749650597572327\n",
      "Epoch 25, Batch 41, Loss: 0.5506706833839417\n",
      "Epoch 25, Batch 42, Loss: 0.6016303300857544\n",
      "Epoch 25, Batch 43, Loss: 0.5503915548324585\n",
      "Epoch 25, Batch 44, Loss: 0.5622378587722778\n",
      "Epoch 25, Batch 45, Loss: 0.5548872351646423\n",
      "Epoch 25, Batch 46, Loss: 0.6063212752342224\n",
      "Epoch 25, Batch 47, Loss: 0.5821561217308044\n",
      "Epoch 25, Batch 48, Loss: 0.5429385900497437\n",
      "Epoch 25, Batch 49, Loss: 0.5602215528488159\n",
      "Epoch 25, Batch 50, Loss: 0.5755115151405334\n",
      "Epoch 25, Batch 51, Loss: 0.5535310506820679\n",
      "Epoch 25, Batch 52, Loss: 0.5465161800384521\n",
      "Epoch 25, Batch 53, Loss: 0.5585027933120728\n",
      "Epoch 25, Batch 54, Loss: 0.5447970628738403\n",
      "Epoch 25, Batch 55, Loss: 0.5611885786056519\n",
      "Epoch 25, Batch 56, Loss: 0.5793486833572388\n",
      "Epoch 25, Batch 57, Loss: 0.5469199419021606\n",
      "Epoch 25, Batch 58, Loss: 0.556772768497467\n",
      "Epoch 25, Batch 59, Loss: 0.5196938514709473\n",
      "Epoch 25, Batch 60, Loss: 0.5754914879798889\n",
      "Epoch 25, Batch 61, Loss: 0.5383629202842712\n",
      "Epoch 25, Batch 62, Loss: 0.5996593832969666\n",
      "Epoch 25, Batch 63, Loss: 0.5801029801368713\n",
      "Epoch 25, Batch 64, Loss: 0.5706315040588379\n",
      "Epoch 25, Batch 65, Loss: 0.5483708381652832\n",
      "Epoch 25, Batch 66, Loss: 0.5698064565658569\n",
      "Epoch 25, Batch 67, Loss: 0.54417884349823\n",
      "Epoch 25, Batch 68, Loss: 0.5719661116600037\n",
      "Epoch 25, Batch 69, Loss: 0.5277594923973083\n",
      "Epoch 25, Batch 70, Loss: 0.5319943428039551\n",
      "Epoch 25, Batch 71, Loss: 0.5623143315315247\n",
      "Epoch 25, Batch 72, Loss: 0.576449990272522\n",
      "Epoch 25, Batch 73, Loss: 0.5722747445106506\n",
      "Epoch 25, Batch 74, Loss: 0.5496707558631897\n",
      "Epoch 25, Batch 75, Loss: 0.5211561918258667\n",
      "Epoch 25, Batch 76, Loss: 0.5533109903335571\n",
      "Epoch 25, Batch 77, Loss: 0.5843223929405212\n",
      "Epoch 25, Batch 78, Loss: 0.5702797174453735\n",
      "Epoch 25, Batch 79, Loss: 0.5389605760574341\n",
      "Epoch 25, Batch 80, Loss: 0.5244208574295044\n",
      "Epoch 25, Batch 81, Loss: 0.6027019619941711\n",
      "Epoch 25, Batch 82, Loss: 0.5987781286239624\n",
      "Epoch 25, Batch 83, Loss: 0.5527900457382202\n",
      "Epoch 25, Batch 84, Loss: 0.5691672563552856\n",
      "Epoch 25, Batch 85, Loss: 0.6008066534996033\n",
      "Epoch 25, Batch 86, Loss: 0.531403660774231\n",
      "Epoch 25, Batch 87, Loss: 0.5749756693840027\n",
      "Epoch 25, Batch 88, Loss: 0.5372032523155212\n",
      "Epoch 25, Batch 89, Loss: 0.5357614755630493\n",
      "Epoch 25, Batch 90, Loss: 0.5653396248817444\n",
      "Epoch 25, Batch 91, Loss: 0.5452908277511597\n",
      "Epoch 25, Batch 92, Loss: 0.504319965839386\n",
      "Epoch 25, Batch 93, Loss: 0.5298007130622864\n",
      "Epoch 26, Batch 0, Loss: 0.5717678070068359\n",
      "Epoch 26, Batch 1, Loss: 0.5384706258773804\n",
      "Epoch 26, Batch 2, Loss: 0.5266423225402832\n",
      "Epoch 26, Batch 3, Loss: 0.5682380795478821\n",
      "Epoch 26, Batch 4, Loss: 0.5699982047080994\n",
      "Epoch 26, Batch 5, Loss: 0.567346453666687\n",
      "Epoch 26, Batch 6, Loss: 0.5650070309638977\n",
      "Epoch 26, Batch 7, Loss: 0.5322756767272949\n",
      "Epoch 26, Batch 8, Loss: 0.5434341430664062\n",
      "Epoch 26, Batch 9, Loss: 0.5638325810432434\n",
      "Epoch 26, Batch 10, Loss: 0.5128885507583618\n",
      "Epoch 26, Batch 11, Loss: 0.5705229043960571\n",
      "Epoch 26, Batch 12, Loss: 0.5899511575698853\n",
      "Epoch 26, Batch 13, Loss: 0.5346062183380127\n",
      "Epoch 26, Batch 14, Loss: 0.5508946180343628\n",
      "Epoch 26, Batch 15, Loss: 0.543428897857666\n",
      "Epoch 26, Batch 16, Loss: 0.6094526052474976\n",
      "Epoch 26, Batch 17, Loss: 0.5487537980079651\n",
      "Epoch 26, Batch 18, Loss: 0.5129877328872681\n",
      "Epoch 26, Batch 19, Loss: 0.5768393278121948\n",
      "Epoch 26, Batch 20, Loss: 0.6071047782897949\n",
      "Epoch 26, Batch 21, Loss: 0.54777592420578\n",
      "Epoch 26, Batch 22, Loss: 0.5617495179176331\n",
      "Epoch 26, Batch 23, Loss: 0.5848135948181152\n",
      "Epoch 26, Batch 24, Loss: 0.5111074447631836\n",
      "Epoch 26, Batch 25, Loss: 0.5431410670280457\n",
      "Epoch 26, Batch 26, Loss: 0.552348256111145\n",
      "Epoch 26, Batch 27, Loss: 0.59201580286026\n",
      "Epoch 26, Batch 28, Loss: 0.5003766417503357\n",
      "Epoch 26, Batch 29, Loss: 0.5068645477294922\n",
      "Epoch 26, Batch 30, Loss: 0.5784610509872437\n",
      "Epoch 26, Batch 31, Loss: 0.5367664694786072\n",
      "Epoch 26, Batch 32, Loss: 0.5394669771194458\n",
      "Epoch 26, Batch 33, Loss: 0.5494022965431213\n",
      "Epoch 26, Batch 34, Loss: 0.5215397477149963\n",
      "Epoch 26, Batch 35, Loss: 0.531560480594635\n",
      "Epoch 26, Batch 36, Loss: 0.5787800550460815\n",
      "Epoch 26, Batch 37, Loss: 0.49994558095932007\n",
      "Epoch 26, Batch 38, Loss: 0.579979419708252\n",
      "Epoch 26, Batch 39, Loss: 0.5577352046966553\n",
      "Epoch 26, Batch 40, Loss: 0.5899413824081421\n",
      "Epoch 26, Batch 41, Loss: 0.5467264652252197\n",
      "Epoch 26, Batch 42, Loss: 0.5101064443588257\n",
      "Epoch 26, Batch 43, Loss: 0.532345712184906\n",
      "Epoch 26, Batch 44, Loss: 0.5589918494224548\n",
      "Epoch 26, Batch 45, Loss: 0.5609220266342163\n",
      "Epoch 26, Batch 46, Loss: 0.5236015319824219\n",
      "Epoch 26, Batch 47, Loss: 0.568710446357727\n",
      "Epoch 26, Batch 48, Loss: 0.5240484476089478\n",
      "Epoch 26, Batch 49, Loss: 0.5261314511299133\n",
      "Epoch 26, Batch 50, Loss: 0.553751528263092\n",
      "Epoch 26, Batch 51, Loss: 0.5713773369789124\n",
      "Epoch 26, Batch 52, Loss: 0.47633615136146545\n",
      "Epoch 26, Batch 53, Loss: 0.5507843494415283\n",
      "Epoch 26, Batch 54, Loss: 0.5617412328720093\n",
      "Epoch 26, Batch 55, Loss: 0.6216802597045898\n",
      "Epoch 26, Batch 56, Loss: 0.5553438663482666\n",
      "Epoch 26, Batch 57, Loss: 0.5422850847244263\n",
      "Epoch 26, Batch 58, Loss: 0.5556203126907349\n",
      "Epoch 26, Batch 59, Loss: 0.5702947378158569\n",
      "Epoch 26, Batch 60, Loss: 0.5673354864120483\n",
      "Epoch 26, Batch 61, Loss: 0.5477331876754761\n",
      "Epoch 26, Batch 62, Loss: 0.5444983839988708\n",
      "Epoch 26, Batch 63, Loss: 0.5463012456893921\n",
      "Epoch 26, Batch 64, Loss: 0.5513423681259155\n",
      "Epoch 26, Batch 65, Loss: 0.5346247553825378\n",
      "Epoch 26, Batch 66, Loss: 0.5531222820281982\n",
      "Epoch 26, Batch 67, Loss: 0.5396299362182617\n",
      "Epoch 26, Batch 68, Loss: 0.5317467451095581\n",
      "Epoch 26, Batch 69, Loss: 0.507644534111023\n",
      "Epoch 26, Batch 70, Loss: 0.5448415875434875\n",
      "Epoch 26, Batch 71, Loss: 0.5665932893753052\n",
      "Epoch 26, Batch 72, Loss: 0.5391696691513062\n",
      "Epoch 26, Batch 73, Loss: 0.5677890777587891\n",
      "Epoch 26, Batch 74, Loss: 0.5487448573112488\n",
      "Epoch 26, Batch 75, Loss: 0.5543683767318726\n",
      "Epoch 26, Batch 76, Loss: 0.541812539100647\n",
      "Epoch 26, Batch 77, Loss: 0.611125648021698\n",
      "Epoch 26, Batch 78, Loss: 0.559904932975769\n",
      "Epoch 26, Batch 79, Loss: 0.5297390222549438\n",
      "Epoch 26, Batch 80, Loss: 0.5448419451713562\n",
      "Epoch 26, Batch 81, Loss: 0.5168830752372742\n",
      "Epoch 26, Batch 82, Loss: 0.5561391115188599\n",
      "Epoch 26, Batch 83, Loss: 0.5288439989089966\n",
      "Epoch 26, Batch 84, Loss: 0.5294132828712463\n",
      "Epoch 26, Batch 85, Loss: 0.5469252467155457\n",
      "Epoch 26, Batch 86, Loss: 0.5649515390396118\n",
      "Epoch 26, Batch 87, Loss: 0.5261685252189636\n",
      "Epoch 26, Batch 88, Loss: 0.5565448999404907\n",
      "Epoch 26, Batch 89, Loss: 0.5458349585533142\n",
      "Epoch 26, Batch 90, Loss: 0.550409734249115\n",
      "Epoch 26, Batch 91, Loss: 0.5334880948066711\n",
      "Epoch 26, Batch 92, Loss: 0.5610862970352173\n",
      "Epoch 26, Batch 93, Loss: 0.5272069573402405\n",
      "Epoch 27, Batch 0, Loss: 0.5673810243606567\n",
      "Epoch 27, Batch 1, Loss: 0.5503760576248169\n",
      "Epoch 27, Batch 2, Loss: 0.5247892737388611\n",
      "Epoch 27, Batch 3, Loss: 0.5677137970924377\n",
      "Epoch 27, Batch 4, Loss: 0.49256983399391174\n",
      "Epoch 27, Batch 5, Loss: 0.5282343626022339\n",
      "Epoch 27, Batch 6, Loss: 0.5476037263870239\n",
      "Epoch 27, Batch 7, Loss: 0.55189049243927\n",
      "Epoch 27, Batch 8, Loss: 0.5147839784622192\n",
      "Epoch 27, Batch 9, Loss: 0.5599827766418457\n",
      "Epoch 27, Batch 10, Loss: 0.5231866836547852\n",
      "Epoch 27, Batch 11, Loss: 0.5028946995735168\n",
      "Epoch 27, Batch 12, Loss: 0.5181999802589417\n",
      "Epoch 27, Batch 13, Loss: 0.5265778303146362\n",
      "Epoch 27, Batch 14, Loss: 0.47334519028663635\n",
      "Epoch 27, Batch 15, Loss: 0.5193026065826416\n",
      "Epoch 27, Batch 16, Loss: 0.5403841733932495\n",
      "Epoch 27, Batch 17, Loss: 0.5363470315933228\n",
      "Epoch 27, Batch 18, Loss: 0.4694681167602539\n",
      "Epoch 27, Batch 19, Loss: 0.5381194949150085\n",
      "Epoch 27, Batch 20, Loss: 0.5766777396202087\n",
      "Epoch 27, Batch 21, Loss: 0.6047941446304321\n",
      "Epoch 27, Batch 22, Loss: 0.5719823837280273\n",
      "Epoch 27, Batch 23, Loss: 0.5677865743637085\n",
      "Epoch 27, Batch 24, Loss: 0.5894158482551575\n",
      "Epoch 27, Batch 25, Loss: 0.5096954703330994\n",
      "Epoch 27, Batch 26, Loss: 0.5374401807785034\n",
      "Epoch 27, Batch 27, Loss: 0.5195296406745911\n",
      "Epoch 27, Batch 28, Loss: 0.5201250910758972\n",
      "Epoch 27, Batch 29, Loss: 0.5485919117927551\n",
      "Epoch 27, Batch 30, Loss: 0.5317906141281128\n",
      "Epoch 27, Batch 31, Loss: 0.5069445967674255\n",
      "Epoch 27, Batch 32, Loss: 0.5268950462341309\n",
      "Epoch 27, Batch 33, Loss: 0.5359290838241577\n",
      "Epoch 27, Batch 34, Loss: 0.5993987917900085\n",
      "Epoch 27, Batch 35, Loss: 0.533068835735321\n",
      "Epoch 27, Batch 36, Loss: 0.6181103587150574\n",
      "Epoch 27, Batch 37, Loss: 0.538236677646637\n",
      "Epoch 27, Batch 38, Loss: 0.5704285502433777\n",
      "Epoch 27, Batch 39, Loss: 0.5083428621292114\n",
      "Epoch 27, Batch 40, Loss: 0.5545851588249207\n",
      "Epoch 27, Batch 41, Loss: 0.5490061044692993\n",
      "Epoch 27, Batch 42, Loss: 0.4984317719936371\n",
      "Epoch 27, Batch 43, Loss: 0.5353242754936218\n",
      "Epoch 27, Batch 44, Loss: 0.5160459280014038\n",
      "Epoch 27, Batch 45, Loss: 0.5529307723045349\n",
      "Epoch 27, Batch 46, Loss: 0.5295392870903015\n",
      "Epoch 27, Batch 47, Loss: 0.5518649220466614\n",
      "Epoch 27, Batch 48, Loss: 0.5256022810935974\n",
      "Epoch 27, Batch 49, Loss: 0.5222352147102356\n",
      "Epoch 27, Batch 50, Loss: 0.5512027740478516\n",
      "Epoch 27, Batch 51, Loss: 0.5036096572875977\n",
      "Epoch 27, Batch 52, Loss: 0.5167056322097778\n",
      "Epoch 27, Batch 53, Loss: 0.5896763801574707\n",
      "Epoch 27, Batch 54, Loss: 0.5164631605148315\n",
      "Epoch 27, Batch 55, Loss: 0.5265787839889526\n",
      "Epoch 27, Batch 56, Loss: 0.5434105396270752\n",
      "Epoch 27, Batch 57, Loss: 0.5985913872718811\n",
      "Epoch 27, Batch 58, Loss: 0.5613701939582825\n",
      "Epoch 27, Batch 59, Loss: 0.60284423828125\n",
      "Epoch 27, Batch 60, Loss: 0.5358787775039673\n",
      "Epoch 27, Batch 61, Loss: 0.536522626876831\n",
      "Epoch 27, Batch 62, Loss: 0.5862740278244019\n",
      "Epoch 27, Batch 63, Loss: 0.5478715896606445\n",
      "Epoch 27, Batch 64, Loss: 0.5077962875366211\n",
      "Epoch 27, Batch 65, Loss: 0.5692169666290283\n",
      "Epoch 27, Batch 66, Loss: 0.5583263635635376\n",
      "Epoch 27, Batch 67, Loss: 0.5486540794372559\n",
      "Epoch 27, Batch 68, Loss: 0.501136839389801\n",
      "Epoch 27, Batch 69, Loss: 0.5471361875534058\n",
      "Epoch 27, Batch 70, Loss: 0.542972981929779\n",
      "Epoch 27, Batch 71, Loss: 0.5171653628349304\n",
      "Epoch 27, Batch 72, Loss: 0.5212926268577576\n",
      "Epoch 27, Batch 73, Loss: 0.5355141758918762\n",
      "Epoch 27, Batch 74, Loss: 0.5081900358200073\n",
      "Epoch 27, Batch 75, Loss: 0.5389522910118103\n",
      "Epoch 27, Batch 76, Loss: 0.5290793180465698\n",
      "Epoch 27, Batch 77, Loss: 0.5352115035057068\n",
      "Epoch 27, Batch 78, Loss: 0.5130791068077087\n",
      "Epoch 27, Batch 79, Loss: 0.5456449389457703\n",
      "Epoch 27, Batch 80, Loss: 0.5231571197509766\n",
      "Epoch 27, Batch 81, Loss: 0.5725807547569275\n",
      "Epoch 27, Batch 82, Loss: 0.5409426093101501\n",
      "Epoch 27, Batch 83, Loss: 0.5492825508117676\n",
      "Epoch 27, Batch 84, Loss: 0.5613501071929932\n",
      "Epoch 27, Batch 85, Loss: 0.5217137932777405\n",
      "Epoch 27, Batch 86, Loss: 0.5373278260231018\n",
      "Epoch 27, Batch 87, Loss: 0.5405885577201843\n",
      "Epoch 27, Batch 88, Loss: 0.5259133577346802\n",
      "Epoch 27, Batch 89, Loss: 0.5356305837631226\n",
      "Epoch 27, Batch 90, Loss: 0.5349943041801453\n",
      "Epoch 27, Batch 91, Loss: 0.56812584400177\n",
      "Epoch 27, Batch 92, Loss: 0.5244625806808472\n",
      "Epoch 27, Batch 93, Loss: 0.4689679741859436\n",
      "Epoch 28, Batch 0, Loss: 0.540252149105072\n",
      "Epoch 28, Batch 1, Loss: 0.5468822717666626\n",
      "Epoch 28, Batch 2, Loss: 0.5571702122688293\n",
      "Epoch 28, Batch 3, Loss: 0.5523101687431335\n",
      "Epoch 28, Batch 4, Loss: 0.5178285837173462\n",
      "Epoch 28, Batch 5, Loss: 0.5546677112579346\n",
      "Epoch 28, Batch 6, Loss: 0.56353360414505\n",
      "Epoch 28, Batch 7, Loss: 0.574180006980896\n",
      "Epoch 28, Batch 8, Loss: 0.5348937511444092\n",
      "Epoch 28, Batch 9, Loss: 0.5542956590652466\n",
      "Epoch 28, Batch 10, Loss: 0.5119383335113525\n",
      "Epoch 28, Batch 11, Loss: 0.5320384502410889\n",
      "Epoch 28, Batch 12, Loss: 0.5705691576004028\n",
      "Epoch 28, Batch 13, Loss: 0.5115887522697449\n",
      "Epoch 28, Batch 14, Loss: 0.5562762022018433\n",
      "Epoch 28, Batch 15, Loss: 0.49587708711624146\n",
      "Epoch 28, Batch 16, Loss: 0.5696781277656555\n",
      "Epoch 28, Batch 17, Loss: 0.5408790111541748\n",
      "Epoch 28, Batch 18, Loss: 0.5208626389503479\n",
      "Epoch 28, Batch 19, Loss: 0.5396919846534729\n",
      "Epoch 28, Batch 20, Loss: 0.5586878061294556\n",
      "Epoch 28, Batch 21, Loss: 0.5158433318138123\n",
      "Epoch 28, Batch 22, Loss: 0.5407334566116333\n",
      "Epoch 28, Batch 23, Loss: 0.5267682075500488\n",
      "Epoch 28, Batch 24, Loss: 0.5311561226844788\n",
      "Epoch 28, Batch 25, Loss: 0.5460736751556396\n",
      "Epoch 28, Batch 26, Loss: 0.5781944990158081\n",
      "Epoch 28, Batch 27, Loss: 0.5491331815719604\n",
      "Epoch 28, Batch 28, Loss: 0.5628315210342407\n",
      "Epoch 28, Batch 29, Loss: 0.523737370967865\n",
      "Epoch 28, Batch 30, Loss: 0.5429851412773132\n",
      "Epoch 28, Batch 31, Loss: 0.4846498966217041\n",
      "Epoch 28, Batch 32, Loss: 0.570679783821106\n",
      "Epoch 28, Batch 33, Loss: 0.5089972019195557\n",
      "Epoch 28, Batch 34, Loss: 0.5352905988693237\n",
      "Epoch 28, Batch 35, Loss: 0.5015087723731995\n",
      "Epoch 28, Batch 36, Loss: 0.5087984800338745\n",
      "Epoch 28, Batch 37, Loss: 0.49782595038414\n",
      "Epoch 28, Batch 38, Loss: 0.5209652781486511\n",
      "Epoch 28, Batch 39, Loss: 0.5892108082771301\n",
      "Epoch 28, Batch 40, Loss: 0.5454329252243042\n",
      "Epoch 28, Batch 41, Loss: 0.52627032995224\n",
      "Epoch 28, Batch 42, Loss: 0.5324744582176208\n",
      "Epoch 28, Batch 43, Loss: 0.4892570972442627\n",
      "Epoch 28, Batch 44, Loss: 0.515121340751648\n",
      "Epoch 28, Batch 45, Loss: 0.47458964586257935\n",
      "Epoch 28, Batch 46, Loss: 0.49962034821510315\n",
      "Epoch 28, Batch 47, Loss: 0.5599602460861206\n",
      "Epoch 28, Batch 48, Loss: 0.5297858119010925\n",
      "Epoch 28, Batch 49, Loss: 0.5284481048583984\n",
      "Epoch 28, Batch 50, Loss: 0.5282440781593323\n",
      "Epoch 28, Batch 51, Loss: 0.5287656188011169\n",
      "Epoch 28, Batch 52, Loss: 0.485839307308197\n",
      "Epoch 28, Batch 53, Loss: 0.529166579246521\n",
      "Epoch 28, Batch 54, Loss: 0.48774948716163635\n",
      "Epoch 28, Batch 55, Loss: 0.5199109315872192\n",
      "Epoch 28, Batch 56, Loss: 0.5495795607566833\n",
      "Epoch 28, Batch 57, Loss: 0.5495591163635254\n",
      "Epoch 28, Batch 58, Loss: 0.494687020778656\n",
      "Epoch 28, Batch 59, Loss: 0.5219022035598755\n",
      "Epoch 28, Batch 60, Loss: 0.5802005529403687\n",
      "Epoch 28, Batch 61, Loss: 0.5483813285827637\n",
      "Epoch 28, Batch 62, Loss: 0.5467198491096497\n",
      "Epoch 28, Batch 63, Loss: 0.5384975075721741\n",
      "Epoch 28, Batch 64, Loss: 0.5312005281448364\n",
      "Epoch 28, Batch 65, Loss: 0.48724231123924255\n",
      "Epoch 28, Batch 66, Loss: 0.47420063614845276\n",
      "Epoch 28, Batch 67, Loss: 0.5693809986114502\n",
      "Epoch 28, Batch 68, Loss: 0.49446576833724976\n",
      "Epoch 28, Batch 69, Loss: 0.537447988986969\n",
      "Epoch 28, Batch 70, Loss: 0.5030220746994019\n",
      "Epoch 28, Batch 71, Loss: 0.5182741284370422\n",
      "Epoch 28, Batch 72, Loss: 0.5254080891609192\n",
      "Epoch 28, Batch 73, Loss: 0.5066579580307007\n",
      "Epoch 28, Batch 74, Loss: 0.5523408651351929\n",
      "Epoch 28, Batch 75, Loss: 0.5142789483070374\n",
      "Epoch 28, Batch 76, Loss: 0.5275989770889282\n",
      "Epoch 28, Batch 77, Loss: 0.5119768381118774\n",
      "Epoch 28, Batch 78, Loss: 0.5012596845626831\n",
      "Epoch 28, Batch 79, Loss: 0.49840229749679565\n",
      "Epoch 28, Batch 80, Loss: 0.5298357009887695\n",
      "Epoch 28, Batch 81, Loss: 0.5300165414810181\n",
      "Epoch 28, Batch 82, Loss: 0.5212923288345337\n",
      "Epoch 28, Batch 83, Loss: 0.5181736946105957\n",
      "Epoch 28, Batch 84, Loss: 0.4702339172363281\n",
      "Epoch 28, Batch 85, Loss: 0.5561575293540955\n",
      "Epoch 28, Batch 86, Loss: 0.4927842617034912\n",
      "Epoch 28, Batch 87, Loss: 0.521171510219574\n",
      "Epoch 28, Batch 88, Loss: 0.5047329664230347\n",
      "Epoch 28, Batch 89, Loss: 0.525147557258606\n",
      "Epoch 28, Batch 90, Loss: 0.5493930578231812\n",
      "Epoch 28, Batch 91, Loss: 0.53961181640625\n",
      "Epoch 28, Batch 92, Loss: 0.5272658467292786\n",
      "Epoch 28, Batch 93, Loss: 0.5524110794067383\n",
      "Epoch 29, Batch 0, Loss: 0.5086936950683594\n",
      "Epoch 29, Batch 1, Loss: 0.5265846252441406\n",
      "Epoch 29, Batch 2, Loss: 0.4949261248111725\n",
      "Epoch 29, Batch 3, Loss: 0.554613471031189\n",
      "Epoch 29, Batch 4, Loss: 0.5318334102630615\n",
      "Epoch 29, Batch 5, Loss: 0.5047951936721802\n",
      "Epoch 29, Batch 6, Loss: 0.5171007513999939\n",
      "Epoch 29, Batch 7, Loss: 0.5333586931228638\n",
      "Epoch 29, Batch 8, Loss: 0.5183708071708679\n",
      "Epoch 29, Batch 9, Loss: 0.5535344481468201\n",
      "Epoch 29, Batch 10, Loss: 0.5572217702865601\n",
      "Epoch 29, Batch 11, Loss: 0.49169421195983887\n",
      "Epoch 29, Batch 12, Loss: 0.539917528629303\n",
      "Epoch 29, Batch 13, Loss: 0.5639264583587646\n",
      "Epoch 29, Batch 14, Loss: 0.5652776956558228\n",
      "Epoch 29, Batch 15, Loss: 0.5362157821655273\n",
      "Epoch 29, Batch 16, Loss: 0.5060948133468628\n",
      "Epoch 29, Batch 17, Loss: 0.5189013481140137\n",
      "Epoch 29, Batch 18, Loss: 0.538541853427887\n",
      "Epoch 29, Batch 19, Loss: 0.48278507590293884\n",
      "Epoch 29, Batch 20, Loss: 0.5455275774002075\n",
      "Epoch 29, Batch 21, Loss: 0.5531379580497742\n",
      "Epoch 29, Batch 22, Loss: 0.5277436971664429\n",
      "Epoch 29, Batch 23, Loss: 0.6015211939811707\n",
      "Epoch 29, Batch 24, Loss: 0.5230064988136292\n",
      "Epoch 29, Batch 25, Loss: 0.5128769874572754\n",
      "Epoch 29, Batch 26, Loss: 0.5176883935928345\n",
      "Epoch 29, Batch 27, Loss: 0.5032451152801514\n",
      "Epoch 29, Batch 28, Loss: 0.523852527141571\n",
      "Epoch 29, Batch 29, Loss: 0.5027515292167664\n",
      "Epoch 29, Batch 30, Loss: 0.5392312407493591\n",
      "Epoch 29, Batch 31, Loss: 0.4865923523902893\n",
      "Epoch 29, Batch 32, Loss: 0.500749409198761\n",
      "Epoch 29, Batch 33, Loss: 0.5483290553092957\n",
      "Epoch 29, Batch 34, Loss: 0.5310638546943665\n",
      "Epoch 29, Batch 35, Loss: 0.4918239116668701\n",
      "Epoch 29, Batch 36, Loss: 0.5513622164726257\n",
      "Epoch 29, Batch 37, Loss: 0.5281800031661987\n",
      "Epoch 29, Batch 38, Loss: 0.5743305087089539\n",
      "Epoch 29, Batch 39, Loss: 0.5010830760002136\n",
      "Epoch 29, Batch 40, Loss: 0.5188263058662415\n",
      "Epoch 29, Batch 41, Loss: 0.5328917503356934\n",
      "Epoch 29, Batch 42, Loss: 0.5125154256820679\n",
      "Epoch 29, Batch 43, Loss: 0.5009455680847168\n",
      "Epoch 29, Batch 44, Loss: 0.5195983648300171\n",
      "Epoch 29, Batch 45, Loss: 0.5131036043167114\n",
      "Epoch 29, Batch 46, Loss: 0.5360064506530762\n",
      "Epoch 29, Batch 47, Loss: 0.5039092898368835\n",
      "Epoch 29, Batch 48, Loss: 0.5031818747520447\n",
      "Epoch 29, Batch 49, Loss: 0.5067859292030334\n",
      "Epoch 29, Batch 50, Loss: 0.5250856876373291\n",
      "Epoch 29, Batch 51, Loss: 0.5281393527984619\n",
      "Epoch 29, Batch 52, Loss: 0.4844209551811218\n",
      "Epoch 29, Batch 53, Loss: 0.4681755602359772\n",
      "Epoch 29, Batch 54, Loss: 0.5216793417930603\n",
      "Epoch 29, Batch 55, Loss: 0.46968474984169006\n",
      "Epoch 29, Batch 56, Loss: 0.5461646318435669\n",
      "Epoch 29, Batch 57, Loss: 0.4937260150909424\n",
      "Epoch 29, Batch 58, Loss: 0.4969521164894104\n",
      "Epoch 29, Batch 59, Loss: 0.502874493598938\n",
      "Epoch 29, Batch 60, Loss: 0.5235140323638916\n",
      "Epoch 29, Batch 61, Loss: 0.5808973908424377\n",
      "Epoch 29, Batch 62, Loss: 0.5215134024620056\n",
      "Epoch 29, Batch 63, Loss: 0.486616849899292\n",
      "Epoch 29, Batch 64, Loss: 0.5118558406829834\n",
      "Epoch 29, Batch 65, Loss: 0.5050086379051208\n",
      "Epoch 29, Batch 66, Loss: 0.4972880780696869\n",
      "Epoch 29, Batch 67, Loss: 0.5112878084182739\n",
      "Epoch 29, Batch 68, Loss: 0.4774724543094635\n",
      "Epoch 29, Batch 69, Loss: 0.5348154306411743\n",
      "Epoch 29, Batch 70, Loss: 0.4996029734611511\n",
      "Epoch 29, Batch 71, Loss: 0.5316438674926758\n",
      "Epoch 29, Batch 72, Loss: 0.47366705536842346\n",
      "Epoch 29, Batch 73, Loss: 0.49614429473876953\n",
      "Epoch 29, Batch 74, Loss: 0.5412876605987549\n",
      "Epoch 29, Batch 75, Loss: 0.5121762752532959\n",
      "Epoch 29, Batch 76, Loss: 0.5079382061958313\n",
      "Epoch 29, Batch 77, Loss: 0.5421870350837708\n",
      "Epoch 29, Batch 78, Loss: 0.5325137972831726\n",
      "Epoch 29, Batch 79, Loss: 0.5170291662216187\n",
      "Epoch 29, Batch 80, Loss: 0.5213083028793335\n",
      "Epoch 29, Batch 81, Loss: 0.548617422580719\n",
      "Epoch 29, Batch 82, Loss: 0.5388628244400024\n",
      "Epoch 29, Batch 83, Loss: 0.5293594598770142\n",
      "Epoch 29, Batch 84, Loss: 0.47662878036499023\n",
      "Epoch 29, Batch 85, Loss: 0.5520933866500854\n",
      "Epoch 29, Batch 86, Loss: 0.5085312128067017\n",
      "Epoch 29, Batch 87, Loss: 0.4674372673034668\n",
      "Epoch 29, Batch 88, Loss: 0.5098186731338501\n",
      "Epoch 29, Batch 89, Loss: 0.5135800242424011\n",
      "Epoch 29, Batch 90, Loss: 0.5140180587768555\n",
      "Epoch 29, Batch 91, Loss: 0.5057954788208008\n",
      "Epoch 29, Batch 92, Loss: 0.6111786365509033\n",
      "Epoch 29, Batch 93, Loss: 0.4609256386756897\n",
      "Epoch 30, Batch 0, Loss: 0.49890413880348206\n",
      "Epoch 30, Batch 1, Loss: 0.5624691843986511\n",
      "Epoch 30, Batch 2, Loss: 0.5491200685501099\n",
      "Epoch 30, Batch 3, Loss: 0.5539806485176086\n",
      "Epoch 30, Batch 4, Loss: 0.5031927824020386\n",
      "Epoch 30, Batch 5, Loss: 0.5065774917602539\n",
      "Epoch 30, Batch 6, Loss: 0.5788615942001343\n",
      "Epoch 30, Batch 7, Loss: 0.524552583694458\n",
      "Epoch 30, Batch 8, Loss: 0.5168370604515076\n",
      "Epoch 30, Batch 9, Loss: 0.5339841246604919\n",
      "Epoch 30, Batch 10, Loss: 0.5124934315681458\n",
      "Epoch 30, Batch 11, Loss: 0.45633459091186523\n",
      "Epoch 30, Batch 12, Loss: 0.48093128204345703\n",
      "Epoch 30, Batch 13, Loss: 0.4895087778568268\n",
      "Epoch 30, Batch 14, Loss: 0.5022867321968079\n",
      "Epoch 30, Batch 15, Loss: 0.4959411025047302\n",
      "Epoch 30, Batch 16, Loss: 0.5293905138969421\n",
      "Epoch 30, Batch 17, Loss: 0.5418649315834045\n",
      "Epoch 30, Batch 18, Loss: 0.538365364074707\n",
      "Epoch 30, Batch 19, Loss: 0.49562352895736694\n",
      "Epoch 30, Batch 20, Loss: 0.46524709463119507\n",
      "Epoch 30, Batch 21, Loss: 0.4908965229988098\n",
      "Epoch 30, Batch 22, Loss: 0.4869254529476166\n",
      "Epoch 30, Batch 23, Loss: 0.49353480339050293\n",
      "Epoch 30, Batch 24, Loss: 0.52342689037323\n",
      "Epoch 30, Batch 25, Loss: 0.5415818095207214\n",
      "Epoch 30, Batch 26, Loss: 0.46552830934524536\n",
      "Epoch 30, Batch 27, Loss: 0.5338801145553589\n",
      "Epoch 30, Batch 28, Loss: 0.4744928777217865\n",
      "Epoch 30, Batch 29, Loss: 0.5113561153411865\n",
      "Epoch 30, Batch 30, Loss: 0.48553794622421265\n",
      "Epoch 30, Batch 31, Loss: 0.5392735004425049\n",
      "Epoch 30, Batch 32, Loss: 0.5077590942382812\n",
      "Epoch 30, Batch 33, Loss: 0.5250328779220581\n",
      "Epoch 30, Batch 34, Loss: 0.5729661583900452\n",
      "Epoch 30, Batch 35, Loss: 0.4933433532714844\n",
      "Epoch 30, Batch 36, Loss: 0.5122167468070984\n",
      "Epoch 30, Batch 37, Loss: 0.4868832528591156\n",
      "Epoch 30, Batch 38, Loss: 0.5359081625938416\n",
      "Epoch 30, Batch 39, Loss: 0.5088924765586853\n",
      "Epoch 30, Batch 40, Loss: 0.4946518540382385\n",
      "Epoch 30, Batch 41, Loss: 0.4962151050567627\n",
      "Epoch 30, Batch 42, Loss: 0.4921579957008362\n",
      "Epoch 30, Batch 43, Loss: 0.49463868141174316\n",
      "Epoch 30, Batch 44, Loss: 0.5150186419487\n",
      "Epoch 30, Batch 45, Loss: 0.5145794153213501\n",
      "Epoch 30, Batch 46, Loss: 0.5777735114097595\n",
      "Epoch 30, Batch 47, Loss: 0.5029962062835693\n",
      "Epoch 30, Batch 48, Loss: 0.5480840802192688\n",
      "Epoch 30, Batch 49, Loss: 0.512714684009552\n",
      "Epoch 30, Batch 50, Loss: 0.5343335866928101\n",
      "Epoch 30, Batch 51, Loss: 0.4478805959224701\n",
      "Epoch 30, Batch 52, Loss: 0.4599105715751648\n",
      "Epoch 30, Batch 53, Loss: 0.5280544757843018\n",
      "Epoch 30, Batch 54, Loss: 0.5520039796829224\n",
      "Epoch 30, Batch 55, Loss: 0.5032100081443787\n",
      "Epoch 30, Batch 56, Loss: 0.4841460585594177\n",
      "Epoch 30, Batch 57, Loss: 0.5695537328720093\n",
      "Epoch 30, Batch 58, Loss: 0.4854029715061188\n",
      "Epoch 30, Batch 59, Loss: 0.5008957386016846\n",
      "Epoch 30, Batch 60, Loss: 0.4911348223686218\n",
      "Epoch 30, Batch 61, Loss: 0.5240501165390015\n",
      "Epoch 30, Batch 62, Loss: 0.5119568705558777\n",
      "Epoch 30, Batch 63, Loss: 0.5276424288749695\n",
      "Epoch 30, Batch 64, Loss: 0.46986451745033264\n",
      "Epoch 30, Batch 65, Loss: 0.5070498585700989\n",
      "Epoch 30, Batch 66, Loss: 0.49454206228256226\n",
      "Epoch 30, Batch 67, Loss: 0.44102174043655396\n",
      "Epoch 30, Batch 68, Loss: 0.5245154500007629\n",
      "Epoch 30, Batch 69, Loss: 0.5275208950042725\n",
      "Epoch 30, Batch 70, Loss: 0.552885890007019\n",
      "Epoch 30, Batch 71, Loss: 0.509915292263031\n",
      "Epoch 30, Batch 72, Loss: 0.5096480846405029\n",
      "Epoch 30, Batch 73, Loss: 0.49347132444381714\n",
      "Epoch 30, Batch 74, Loss: 0.4997246265411377\n",
      "Epoch 30, Batch 75, Loss: 0.5180114507675171\n",
      "Epoch 30, Batch 76, Loss: 0.5082463026046753\n",
      "Epoch 30, Batch 77, Loss: 0.4797116219997406\n",
      "Epoch 30, Batch 78, Loss: 0.5039955377578735\n",
      "Epoch 30, Batch 79, Loss: 0.5018479824066162\n",
      "Epoch 30, Batch 80, Loss: 0.5192695260047913\n",
      "Epoch 30, Batch 81, Loss: 0.5391448736190796\n",
      "Epoch 30, Batch 82, Loss: 0.49250882863998413\n",
      "Epoch 30, Batch 83, Loss: 0.4926687180995941\n",
      "Epoch 30, Batch 84, Loss: 0.5282869338989258\n",
      "Epoch 30, Batch 85, Loss: 0.550360381603241\n",
      "Epoch 30, Batch 86, Loss: 0.49781346321105957\n",
      "Epoch 30, Batch 87, Loss: 0.5862303376197815\n",
      "Epoch 30, Batch 88, Loss: 0.500747799873352\n",
      "Epoch 30, Batch 89, Loss: 0.5196345448493958\n",
      "Epoch 30, Batch 90, Loss: 0.4816281199455261\n",
      "Epoch 30, Batch 91, Loss: 0.5432115197181702\n",
      "Epoch 30, Batch 92, Loss: 0.49059829115867615\n",
      "Epoch 30, Batch 93, Loss: 0.47311365604400635\n",
      "Epoch 31, Batch 0, Loss: 0.4970005452632904\n",
      "Epoch 31, Batch 1, Loss: 0.5953220725059509\n",
      "Epoch 31, Batch 2, Loss: 0.45426830649375916\n",
      "Epoch 31, Batch 3, Loss: 0.5350472331047058\n",
      "Epoch 31, Batch 4, Loss: 0.5446189045906067\n",
      "Epoch 31, Batch 5, Loss: 0.5247754454612732\n",
      "Epoch 31, Batch 6, Loss: 0.5016360282897949\n",
      "Epoch 31, Batch 7, Loss: 0.4948791563510895\n",
      "Epoch 31, Batch 8, Loss: 0.49870210886001587\n",
      "Epoch 31, Batch 9, Loss: 0.4870116710662842\n",
      "Epoch 31, Batch 10, Loss: 0.5239850282669067\n",
      "Epoch 31, Batch 11, Loss: 0.4882528781890869\n",
      "Epoch 31, Batch 12, Loss: 0.5536586046218872\n",
      "Epoch 31, Batch 13, Loss: 0.4820640981197357\n",
      "Epoch 31, Batch 14, Loss: 0.477285236120224\n",
      "Epoch 31, Batch 15, Loss: 0.49723881483078003\n",
      "Epoch 31, Batch 16, Loss: 0.49843424558639526\n",
      "Epoch 31, Batch 17, Loss: 0.517768919467926\n",
      "Epoch 31, Batch 18, Loss: 0.5251703858375549\n",
      "Epoch 31, Batch 19, Loss: 0.43181830644607544\n",
      "Epoch 31, Batch 20, Loss: 0.5221315622329712\n",
      "Epoch 31, Batch 21, Loss: 0.49177107214927673\n",
      "Epoch 31, Batch 22, Loss: 0.47642651200294495\n",
      "Epoch 31, Batch 23, Loss: 0.49933212995529175\n",
      "Epoch 31, Batch 24, Loss: 0.49247926473617554\n",
      "Epoch 31, Batch 25, Loss: 0.5229891538619995\n",
      "Epoch 31, Batch 26, Loss: 0.4976939558982849\n",
      "Epoch 31, Batch 27, Loss: 0.47101861238479614\n",
      "Epoch 31, Batch 28, Loss: 0.5284218788146973\n",
      "Epoch 31, Batch 29, Loss: 0.48482927680015564\n",
      "Epoch 31, Batch 30, Loss: 0.4972623884677887\n",
      "Epoch 31, Batch 31, Loss: 0.5043524503707886\n",
      "Epoch 31, Batch 32, Loss: 0.5303218364715576\n",
      "Epoch 31, Batch 33, Loss: 0.44736582040786743\n",
      "Epoch 31, Batch 34, Loss: 0.4552825391292572\n",
      "Epoch 31, Batch 35, Loss: 0.4779287874698639\n",
      "Epoch 31, Batch 36, Loss: 0.4900732636451721\n",
      "Epoch 31, Batch 37, Loss: 0.48576170206069946\n",
      "Epoch 31, Batch 38, Loss: 0.5299199819564819\n",
      "Epoch 31, Batch 39, Loss: 0.5131666660308838\n",
      "Epoch 31, Batch 40, Loss: 0.4630756974220276\n",
      "Epoch 31, Batch 41, Loss: 0.5003376007080078\n",
      "Epoch 31, Batch 42, Loss: 0.48361262679100037\n",
      "Epoch 31, Batch 43, Loss: 0.504767119884491\n",
      "Epoch 31, Batch 44, Loss: 0.4866052567958832\n",
      "Epoch 31, Batch 45, Loss: 0.46146589517593384\n",
      "Epoch 31, Batch 46, Loss: 0.48746442794799805\n",
      "Epoch 31, Batch 47, Loss: 0.5095530152320862\n",
      "Epoch 31, Batch 48, Loss: 0.44688886404037476\n",
      "Epoch 31, Batch 49, Loss: 0.4717143177986145\n",
      "Epoch 31, Batch 50, Loss: 0.5198501348495483\n",
      "Epoch 31, Batch 51, Loss: 0.5262261629104614\n",
      "Epoch 31, Batch 52, Loss: 0.5054596066474915\n",
      "Epoch 31, Batch 53, Loss: 0.49299532175064087\n",
      "Epoch 31, Batch 54, Loss: 0.46777433156967163\n",
      "Epoch 31, Batch 55, Loss: 0.5462237596511841\n",
      "Epoch 31, Batch 56, Loss: 0.550041675567627\n",
      "Epoch 31, Batch 57, Loss: 0.5140712857246399\n",
      "Epoch 31, Batch 58, Loss: 0.5131958723068237\n",
      "Epoch 31, Batch 59, Loss: 0.5030146241188049\n",
      "Epoch 31, Batch 60, Loss: 0.5309491157531738\n",
      "Epoch 31, Batch 61, Loss: 0.5476900339126587\n",
      "Epoch 31, Batch 62, Loss: 0.5405560731887817\n",
      "Epoch 31, Batch 63, Loss: 0.4717560410499573\n",
      "Epoch 31, Batch 64, Loss: 0.4999772012233734\n",
      "Epoch 31, Batch 65, Loss: 0.5175255537033081\n",
      "Epoch 31, Batch 66, Loss: 0.5092856287956238\n",
      "Epoch 31, Batch 67, Loss: 0.4795372486114502\n",
      "Epoch 31, Batch 68, Loss: 0.4933445453643799\n",
      "Epoch 31, Batch 69, Loss: 0.4815552830696106\n",
      "Epoch 31, Batch 70, Loss: 0.5231095552444458\n",
      "Epoch 31, Batch 71, Loss: 0.5256434679031372\n",
      "Epoch 31, Batch 72, Loss: 0.5748883485794067\n",
      "Epoch 31, Batch 73, Loss: 0.5265921950340271\n",
      "Epoch 31, Batch 74, Loss: 0.5718311071395874\n",
      "Epoch 31, Batch 75, Loss: 0.4987289011478424\n",
      "Epoch 31, Batch 76, Loss: 0.49682196974754333\n",
      "Epoch 31, Batch 77, Loss: 0.4610331058502197\n",
      "Epoch 31, Batch 78, Loss: 0.5031772255897522\n",
      "Epoch 31, Batch 79, Loss: 0.48956790566444397\n",
      "Epoch 31, Batch 80, Loss: 0.47337254881858826\n",
      "Epoch 31, Batch 81, Loss: 0.5040196776390076\n",
      "Epoch 31, Batch 82, Loss: 0.5132679343223572\n",
      "Epoch 31, Batch 83, Loss: 0.5523322820663452\n",
      "Epoch 31, Batch 84, Loss: 0.5363636612892151\n",
      "Epoch 31, Batch 85, Loss: 0.5418078899383545\n",
      "Epoch 31, Batch 86, Loss: 0.47166380286216736\n",
      "Epoch 31, Batch 87, Loss: 0.46380820870399475\n",
      "Epoch 31, Batch 88, Loss: 0.5249707698822021\n",
      "Epoch 31, Batch 89, Loss: 0.4588530659675598\n",
      "Epoch 31, Batch 90, Loss: 0.49875718355178833\n",
      "Epoch 31, Batch 91, Loss: 0.5232861042022705\n",
      "Epoch 31, Batch 92, Loss: 0.5008509755134583\n",
      "Epoch 31, Batch 93, Loss: 0.49665531516075134\n",
      "Epoch 32, Batch 0, Loss: 0.5134305357933044\n",
      "Epoch 32, Batch 1, Loss: 0.4901668429374695\n",
      "Epoch 32, Batch 2, Loss: 0.5017700791358948\n",
      "Epoch 32, Batch 3, Loss: 0.5109219551086426\n",
      "Epoch 32, Batch 4, Loss: 0.5205048322677612\n",
      "Epoch 32, Batch 5, Loss: 0.4662650227546692\n",
      "Epoch 32, Batch 6, Loss: 0.4686688780784607\n",
      "Epoch 32, Batch 7, Loss: 0.4946669042110443\n",
      "Epoch 32, Batch 8, Loss: 0.5069493055343628\n",
      "Epoch 32, Batch 9, Loss: 0.5316339135169983\n",
      "Epoch 32, Batch 10, Loss: 0.48408645391464233\n",
      "Epoch 32, Batch 11, Loss: 0.5423246622085571\n",
      "Epoch 32, Batch 12, Loss: 0.44696134328842163\n",
      "Epoch 32, Batch 13, Loss: 0.4507218301296234\n",
      "Epoch 32, Batch 14, Loss: 0.4859072268009186\n",
      "Epoch 32, Batch 15, Loss: 0.5104078650474548\n",
      "Epoch 32, Batch 16, Loss: 0.4717985987663269\n",
      "Epoch 32, Batch 17, Loss: 0.48927760124206543\n",
      "Epoch 32, Batch 18, Loss: 0.49545741081237793\n",
      "Epoch 32, Batch 19, Loss: 0.5499692559242249\n",
      "Epoch 32, Batch 20, Loss: 0.4543534815311432\n",
      "Epoch 32, Batch 21, Loss: 0.5303210020065308\n",
      "Epoch 32, Batch 22, Loss: 0.4876154363155365\n",
      "Epoch 32, Batch 23, Loss: 0.4696083068847656\n",
      "Epoch 32, Batch 24, Loss: 0.471109002828598\n",
      "Epoch 32, Batch 25, Loss: 0.5095182061195374\n",
      "Epoch 32, Batch 26, Loss: 0.5369683504104614\n",
      "Epoch 32, Batch 27, Loss: 0.45308607816696167\n",
      "Epoch 32, Batch 28, Loss: 0.5121435523033142\n",
      "Epoch 32, Batch 29, Loss: 0.514865517616272\n",
      "Epoch 32, Batch 30, Loss: 0.48351478576660156\n",
      "Epoch 32, Batch 31, Loss: 0.46904411911964417\n",
      "Epoch 32, Batch 32, Loss: 0.5048527121543884\n",
      "Epoch 32, Batch 33, Loss: 0.4883590638637543\n",
      "Epoch 32, Batch 34, Loss: 0.5342510342597961\n",
      "Epoch 32, Batch 35, Loss: 0.5061370730400085\n",
      "Epoch 32, Batch 36, Loss: 0.4681726098060608\n",
      "Epoch 32, Batch 37, Loss: 0.4881402552127838\n",
      "Epoch 32, Batch 38, Loss: 0.49941372871398926\n",
      "Epoch 32, Batch 39, Loss: 0.5076506733894348\n",
      "Epoch 32, Batch 40, Loss: 0.5069831013679504\n",
      "Epoch 32, Batch 41, Loss: 0.5050944685935974\n",
      "Epoch 32, Batch 42, Loss: 0.44487738609313965\n",
      "Epoch 32, Batch 43, Loss: 0.5155929923057556\n",
      "Epoch 32, Batch 44, Loss: 0.48419007658958435\n",
      "Epoch 32, Batch 45, Loss: 0.5119849443435669\n",
      "Epoch 32, Batch 46, Loss: 0.5028440356254578\n",
      "Epoch 32, Batch 47, Loss: 0.5250177979469299\n",
      "Epoch 32, Batch 48, Loss: 0.4751780927181244\n",
      "Epoch 32, Batch 49, Loss: 0.5206729769706726\n",
      "Epoch 32, Batch 50, Loss: 0.5469841361045837\n",
      "Epoch 32, Batch 51, Loss: 0.4705241620540619\n",
      "Epoch 32, Batch 52, Loss: 0.43563541769981384\n",
      "Epoch 32, Batch 53, Loss: 0.47303372621536255\n",
      "Epoch 32, Batch 54, Loss: 0.5258384943008423\n",
      "Epoch 32, Batch 55, Loss: 0.5023384094238281\n",
      "Epoch 32, Batch 56, Loss: 0.4948928952217102\n",
      "Epoch 32, Batch 57, Loss: 0.5434213876724243\n",
      "Epoch 32, Batch 58, Loss: 0.5412160158157349\n",
      "Epoch 32, Batch 59, Loss: 0.48203596472740173\n",
      "Epoch 32, Batch 60, Loss: 0.5517397522926331\n",
      "Epoch 32, Batch 61, Loss: 0.4998099207878113\n",
      "Epoch 32, Batch 62, Loss: 0.4977615773677826\n",
      "Epoch 32, Batch 63, Loss: 0.5049279928207397\n",
      "Epoch 32, Batch 64, Loss: 0.48988503217697144\n",
      "Epoch 32, Batch 65, Loss: 0.4920892119407654\n",
      "Epoch 32, Batch 66, Loss: 0.4877929091453552\n",
      "Epoch 32, Batch 67, Loss: 0.4660029411315918\n",
      "Epoch 32, Batch 68, Loss: 0.4710003733634949\n",
      "Epoch 32, Batch 69, Loss: 0.49269843101501465\n",
      "Epoch 32, Batch 70, Loss: 0.5044175386428833\n",
      "Epoch 32, Batch 71, Loss: 0.5279356241226196\n",
      "Epoch 32, Batch 72, Loss: 0.5195131897926331\n",
      "Epoch 32, Batch 73, Loss: 0.45239853858947754\n",
      "Epoch 32, Batch 74, Loss: 0.530029833316803\n",
      "Epoch 32, Batch 75, Loss: 0.5269374847412109\n",
      "Epoch 32, Batch 76, Loss: 0.49855536222457886\n",
      "Epoch 32, Batch 77, Loss: 0.4927694797515869\n",
      "Epoch 32, Batch 78, Loss: 0.513064980506897\n",
      "Epoch 32, Batch 79, Loss: 0.51032555103302\n",
      "Epoch 32, Batch 80, Loss: 0.4629875719547272\n",
      "Epoch 32, Batch 81, Loss: 0.4385281503200531\n",
      "Epoch 32, Batch 82, Loss: 0.4827798008918762\n",
      "Epoch 32, Batch 83, Loss: 0.486456960439682\n",
      "Epoch 32, Batch 84, Loss: 0.5095530152320862\n",
      "Epoch 32, Batch 85, Loss: 0.5198003649711609\n",
      "Epoch 32, Batch 86, Loss: 0.4890265464782715\n",
      "Epoch 32, Batch 87, Loss: 0.4511594772338867\n",
      "Epoch 32, Batch 88, Loss: 0.4481891989707947\n",
      "Epoch 32, Batch 89, Loss: 0.510227620601654\n",
      "Epoch 32, Batch 90, Loss: 0.4830266535282135\n",
      "Epoch 32, Batch 91, Loss: 0.4506900906562805\n",
      "Epoch 32, Batch 92, Loss: 0.5075615644454956\n",
      "Epoch 32, Batch 93, Loss: 0.4998630881309509\n",
      "Epoch 33, Batch 0, Loss: 0.49298223853111267\n",
      "Epoch 33, Batch 1, Loss: 0.5462852716445923\n",
      "Epoch 33, Batch 2, Loss: 0.4803082048892975\n",
      "Epoch 33, Batch 3, Loss: 0.4498435854911804\n",
      "Epoch 33, Batch 4, Loss: 0.4492105543613434\n",
      "Epoch 33, Batch 5, Loss: 0.5111684203147888\n",
      "Epoch 33, Batch 6, Loss: 0.4655393064022064\n",
      "Epoch 33, Batch 7, Loss: 0.5465410351753235\n",
      "Epoch 33, Batch 8, Loss: 0.4697481095790863\n",
      "Epoch 33, Batch 9, Loss: 0.4771559238433838\n",
      "Epoch 33, Batch 10, Loss: 0.4499776363372803\n",
      "Epoch 33, Batch 11, Loss: 0.48413461446762085\n",
      "Epoch 33, Batch 12, Loss: 0.4507741928100586\n",
      "Epoch 33, Batch 13, Loss: 0.4856094717979431\n",
      "Epoch 33, Batch 14, Loss: 0.49993452429771423\n",
      "Epoch 33, Batch 15, Loss: 0.5356361865997314\n",
      "Epoch 33, Batch 16, Loss: 0.4964021146297455\n",
      "Epoch 33, Batch 17, Loss: 0.4912641644477844\n",
      "Epoch 33, Batch 18, Loss: 0.4447433352470398\n",
      "Epoch 33, Batch 19, Loss: 0.4943822920322418\n",
      "Epoch 33, Batch 20, Loss: 0.4838680624961853\n",
      "Epoch 33, Batch 21, Loss: 0.45267969369888306\n",
      "Epoch 33, Batch 22, Loss: 0.5104093551635742\n",
      "Epoch 33, Batch 23, Loss: 0.4416796565055847\n",
      "Epoch 33, Batch 24, Loss: 0.5274573564529419\n",
      "Epoch 33, Batch 25, Loss: 0.49647483229637146\n",
      "Epoch 33, Batch 26, Loss: 0.45903104543685913\n",
      "Epoch 33, Batch 27, Loss: 0.48076382279396057\n",
      "Epoch 33, Batch 28, Loss: 0.5147120952606201\n",
      "Epoch 33, Batch 29, Loss: 0.5164569020271301\n",
      "Epoch 33, Batch 30, Loss: 0.4813729226589203\n",
      "Epoch 33, Batch 31, Loss: 0.48513442277908325\n",
      "Epoch 33, Batch 32, Loss: 0.5584498047828674\n",
      "Epoch 33, Batch 33, Loss: 0.49529677629470825\n",
      "Epoch 33, Batch 34, Loss: 0.46509259939193726\n",
      "Epoch 33, Batch 35, Loss: 0.49109482765197754\n",
      "Epoch 33, Batch 36, Loss: 0.4898896813392639\n",
      "Epoch 33, Batch 37, Loss: 0.45980730652809143\n",
      "Epoch 33, Batch 38, Loss: 0.5542839169502258\n",
      "Epoch 33, Batch 39, Loss: 0.46141499280929565\n",
      "Epoch 33, Batch 40, Loss: 0.4487520754337311\n",
      "Epoch 33, Batch 41, Loss: 0.4653783440589905\n",
      "Epoch 33, Batch 42, Loss: 0.4492731988430023\n",
      "Epoch 33, Batch 43, Loss: 0.45859432220458984\n",
      "Epoch 33, Batch 44, Loss: 0.477891743183136\n",
      "Epoch 33, Batch 45, Loss: 0.47097358107566833\n",
      "Epoch 33, Batch 46, Loss: 0.49841246008872986\n",
      "Epoch 33, Batch 47, Loss: 0.5313331484794617\n",
      "Epoch 33, Batch 48, Loss: 0.48128294944763184\n",
      "Epoch 33, Batch 49, Loss: 0.4846072793006897\n",
      "Epoch 33, Batch 50, Loss: 0.4694384038448334\n",
      "Epoch 33, Batch 51, Loss: 0.4902215898036957\n",
      "Epoch 33, Batch 52, Loss: 0.5149250626564026\n",
      "Epoch 33, Batch 53, Loss: 0.49131232500076294\n",
      "Epoch 33, Batch 54, Loss: 0.5098894834518433\n",
      "Epoch 33, Batch 55, Loss: 0.502324104309082\n",
      "Epoch 33, Batch 56, Loss: 0.426229864358902\n",
      "Epoch 33, Batch 57, Loss: 0.521124005317688\n",
      "Epoch 33, Batch 58, Loss: 0.5070255994796753\n",
      "Epoch 33, Batch 59, Loss: 0.4975631833076477\n",
      "Epoch 33, Batch 60, Loss: 0.5139438509941101\n",
      "Epoch 33, Batch 61, Loss: 0.4916456639766693\n",
      "Epoch 33, Batch 62, Loss: 0.5305042266845703\n",
      "Epoch 33, Batch 63, Loss: 0.4996469020843506\n",
      "Epoch 33, Batch 64, Loss: 0.5016772150993347\n",
      "Epoch 33, Batch 65, Loss: 0.4907650053501129\n",
      "Epoch 33, Batch 66, Loss: 0.5155356526374817\n",
      "Epoch 33, Batch 67, Loss: 0.48936089873313904\n",
      "Epoch 33, Batch 68, Loss: 0.4982072412967682\n",
      "Epoch 33, Batch 69, Loss: 0.45831409096717834\n",
      "Epoch 33, Batch 70, Loss: 0.5227144360542297\n",
      "Epoch 33, Batch 71, Loss: 0.4739317297935486\n",
      "Epoch 33, Batch 72, Loss: 0.49754977226257324\n",
      "Epoch 33, Batch 73, Loss: 0.4851323664188385\n",
      "Epoch 33, Batch 74, Loss: 0.4747803807258606\n",
      "Epoch 33, Batch 75, Loss: 0.47973427176475525\n",
      "Epoch 33, Batch 76, Loss: 0.44303184747695923\n",
      "Epoch 33, Batch 77, Loss: 0.5194271802902222\n",
      "Epoch 33, Batch 78, Loss: 0.47907429933547974\n",
      "Epoch 33, Batch 79, Loss: 0.4822167456150055\n",
      "Epoch 33, Batch 80, Loss: 0.5069840550422668\n",
      "Epoch 33, Batch 81, Loss: 0.4739600121974945\n",
      "Epoch 33, Batch 82, Loss: 0.5327538251876831\n",
      "Epoch 33, Batch 83, Loss: 0.4612561762332916\n",
      "Epoch 33, Batch 84, Loss: 0.4979148805141449\n",
      "Epoch 33, Batch 85, Loss: 0.47988539934158325\n",
      "Epoch 33, Batch 86, Loss: 0.45896750688552856\n",
      "Epoch 33, Batch 87, Loss: 0.48615193367004395\n",
      "Epoch 33, Batch 88, Loss: 0.48273199796676636\n",
      "Epoch 33, Batch 89, Loss: 0.48858070373535156\n",
      "Epoch 33, Batch 90, Loss: 0.5304459929466248\n",
      "Epoch 33, Batch 91, Loss: 0.4794696271419525\n",
      "Epoch 33, Batch 92, Loss: 0.5012922286987305\n",
      "Epoch 33, Batch 93, Loss: 0.45178645849227905\n",
      "Epoch 34, Batch 0, Loss: 0.4682416021823883\n",
      "Epoch 34, Batch 1, Loss: 0.4938647150993347\n",
      "Epoch 34, Batch 2, Loss: 0.5078166723251343\n",
      "Epoch 34, Batch 3, Loss: 0.494838148355484\n",
      "Epoch 34, Batch 4, Loss: 0.4783676266670227\n",
      "Epoch 34, Batch 5, Loss: 0.48561733961105347\n",
      "Epoch 34, Batch 6, Loss: 0.4565432071685791\n",
      "Epoch 34, Batch 7, Loss: 0.5248253345489502\n",
      "Epoch 34, Batch 8, Loss: 0.4916117787361145\n",
      "Epoch 34, Batch 9, Loss: 0.482287734746933\n",
      "Epoch 34, Batch 10, Loss: 0.4916878342628479\n",
      "Epoch 34, Batch 11, Loss: 0.45109033584594727\n",
      "Epoch 34, Batch 12, Loss: 0.4692140519618988\n",
      "Epoch 34, Batch 13, Loss: 0.47630730271339417\n",
      "Epoch 34, Batch 14, Loss: 0.4676763415336609\n",
      "Epoch 34, Batch 15, Loss: 0.4443483352661133\n",
      "Epoch 34, Batch 16, Loss: 0.4681212902069092\n",
      "Epoch 34, Batch 17, Loss: 0.544049859046936\n",
      "Epoch 34, Batch 18, Loss: 0.4876555800437927\n",
      "Epoch 34, Batch 19, Loss: 0.5102737545967102\n",
      "Epoch 34, Batch 20, Loss: 0.4973520338535309\n",
      "Epoch 34, Batch 21, Loss: 0.4432523846626282\n",
      "Epoch 34, Batch 22, Loss: 0.4876917004585266\n",
      "Epoch 34, Batch 23, Loss: 0.45373836159706116\n",
      "Epoch 34, Batch 24, Loss: 0.4770326614379883\n",
      "Epoch 34, Batch 25, Loss: 0.456200510263443\n",
      "Epoch 34, Batch 26, Loss: 0.47592049837112427\n",
      "Epoch 34, Batch 27, Loss: 0.4792911112308502\n",
      "Epoch 34, Batch 28, Loss: 0.49656933546066284\n",
      "Epoch 34, Batch 29, Loss: 0.4740488529205322\n",
      "Epoch 34, Batch 30, Loss: 0.551197350025177\n",
      "Epoch 34, Batch 31, Loss: 0.49441003799438477\n",
      "Epoch 34, Batch 32, Loss: 0.4761168956756592\n",
      "Epoch 34, Batch 33, Loss: 0.4914931356906891\n",
      "Epoch 34, Batch 34, Loss: 0.4960075318813324\n",
      "Epoch 34, Batch 35, Loss: 0.47137874364852905\n",
      "Epoch 34, Batch 36, Loss: 0.5216525197029114\n",
      "Epoch 34, Batch 37, Loss: 0.5021229982376099\n",
      "Epoch 34, Batch 38, Loss: 0.4450158178806305\n",
      "Epoch 34, Batch 39, Loss: 0.5509178638458252\n",
      "Epoch 34, Batch 40, Loss: 0.48366624116897583\n",
      "Epoch 34, Batch 41, Loss: 0.5000383257865906\n",
      "Epoch 34, Batch 42, Loss: 0.4805740416049957\n",
      "Epoch 34, Batch 43, Loss: 0.4830099046230316\n",
      "Epoch 34, Batch 44, Loss: 0.5411282777786255\n",
      "Epoch 34, Batch 45, Loss: 0.5050079822540283\n",
      "Epoch 34, Batch 46, Loss: 0.48723751306533813\n",
      "Epoch 34, Batch 47, Loss: 0.4745032787322998\n",
      "Epoch 34, Batch 48, Loss: 0.5121108293533325\n",
      "Epoch 34, Batch 49, Loss: 0.4364171028137207\n",
      "Epoch 34, Batch 50, Loss: 0.5064085721969604\n",
      "Epoch 34, Batch 51, Loss: 0.43864670395851135\n",
      "Epoch 34, Batch 52, Loss: 0.5002129077911377\n",
      "Epoch 34, Batch 53, Loss: 0.4833374619483948\n",
      "Epoch 34, Batch 54, Loss: 0.5108309388160706\n",
      "Epoch 34, Batch 55, Loss: 0.46568378806114197\n",
      "Epoch 34, Batch 56, Loss: 0.479006826877594\n",
      "Epoch 34, Batch 57, Loss: 0.48806723952293396\n",
      "Epoch 34, Batch 58, Loss: 0.4612807631492615\n",
      "Epoch 34, Batch 59, Loss: 0.5025773048400879\n",
      "Epoch 34, Batch 60, Loss: 0.4947582185268402\n",
      "Epoch 34, Batch 61, Loss: 0.48874515295028687\n",
      "Epoch 34, Batch 62, Loss: 0.4654177725315094\n",
      "Epoch 34, Batch 63, Loss: 0.5313242673873901\n",
      "Epoch 34, Batch 64, Loss: 0.4495859146118164\n",
      "Epoch 34, Batch 65, Loss: 0.43914395570755005\n",
      "Epoch 34, Batch 66, Loss: 0.47392648458480835\n",
      "Epoch 34, Batch 67, Loss: 0.4751034677028656\n",
      "Epoch 34, Batch 68, Loss: 0.4295489192008972\n",
      "Epoch 34, Batch 69, Loss: 0.5277419090270996\n",
      "Epoch 34, Batch 70, Loss: 0.5017287135124207\n",
      "Epoch 34, Batch 71, Loss: 0.43654027581214905\n",
      "Epoch 34, Batch 72, Loss: 0.49441298842430115\n",
      "Epoch 34, Batch 73, Loss: 0.47957843542099\n",
      "Epoch 34, Batch 74, Loss: 0.46038103103637695\n",
      "Epoch 34, Batch 75, Loss: 0.5142781734466553\n",
      "Epoch 34, Batch 76, Loss: 0.43157142400741577\n",
      "Epoch 34, Batch 77, Loss: 0.4489806592464447\n",
      "Epoch 34, Batch 78, Loss: 0.5077095627784729\n",
      "Epoch 34, Batch 79, Loss: 0.43954262137413025\n",
      "Epoch 34, Batch 80, Loss: 0.4956400394439697\n",
      "Epoch 34, Batch 81, Loss: 0.489058256149292\n",
      "Epoch 34, Batch 82, Loss: 0.45332565903663635\n",
      "Epoch 34, Batch 83, Loss: 0.5019696950912476\n",
      "Epoch 34, Batch 84, Loss: 0.45567265152931213\n",
      "Epoch 34, Batch 85, Loss: 0.48196840286254883\n",
      "Epoch 34, Batch 86, Loss: 0.4393587112426758\n",
      "Epoch 34, Batch 87, Loss: 0.4698338508605957\n",
      "Epoch 34, Batch 88, Loss: 0.46997570991516113\n",
      "Epoch 34, Batch 89, Loss: 0.48253995180130005\n",
      "Epoch 34, Batch 90, Loss: 0.4532264769077301\n",
      "Epoch 34, Batch 91, Loss: 0.45778027176856995\n",
      "Epoch 34, Batch 92, Loss: 0.49870243668556213\n",
      "Epoch 34, Batch 93, Loss: 0.4929264485836029\n",
      "Epoch 35, Batch 0, Loss: 0.4702598452568054\n",
      "Epoch 35, Batch 1, Loss: 0.48885220289230347\n",
      "Epoch 35, Batch 2, Loss: 0.5120316743850708\n",
      "Epoch 35, Batch 3, Loss: 0.47644343972206116\n",
      "Epoch 35, Batch 4, Loss: 0.43361133337020874\n",
      "Epoch 35, Batch 5, Loss: 0.47456973791122437\n",
      "Epoch 35, Batch 6, Loss: 0.46712881326675415\n",
      "Epoch 35, Batch 7, Loss: 0.4554666578769684\n",
      "Epoch 35, Batch 8, Loss: 0.5483123660087585\n",
      "Epoch 35, Batch 9, Loss: 0.483599990606308\n",
      "Epoch 35, Batch 10, Loss: 0.5052043199539185\n",
      "Epoch 35, Batch 11, Loss: 0.4375001788139343\n",
      "Epoch 35, Batch 12, Loss: 0.44344550371170044\n",
      "Epoch 35, Batch 13, Loss: 0.46854233741760254\n",
      "Epoch 35, Batch 14, Loss: 0.4507599472999573\n",
      "Epoch 35, Batch 15, Loss: 0.49613475799560547\n",
      "Epoch 35, Batch 16, Loss: 0.4792853891849518\n",
      "Epoch 35, Batch 17, Loss: 0.5341536998748779\n",
      "Epoch 35, Batch 18, Loss: 0.44947490096092224\n",
      "Epoch 35, Batch 19, Loss: 0.4921949505805969\n",
      "Epoch 35, Batch 20, Loss: 0.4706389307975769\n",
      "Epoch 35, Batch 21, Loss: 0.5140162706375122\n",
      "Epoch 35, Batch 22, Loss: 0.48278865218162537\n",
      "Epoch 35, Batch 23, Loss: 0.4963292181491852\n",
      "Epoch 35, Batch 24, Loss: 0.4803914427757263\n",
      "Epoch 35, Batch 25, Loss: 0.4115304946899414\n",
      "Epoch 35, Batch 26, Loss: 0.48790305852890015\n",
      "Epoch 35, Batch 27, Loss: 0.4304184317588806\n",
      "Epoch 35, Batch 28, Loss: 0.45864564180374146\n",
      "Epoch 35, Batch 29, Loss: 0.5064846277236938\n",
      "Epoch 35, Batch 30, Loss: 0.45563381910324097\n",
      "Epoch 35, Batch 31, Loss: 0.4808412194252014\n",
      "Epoch 35, Batch 32, Loss: 0.4454623758792877\n",
      "Epoch 35, Batch 33, Loss: 0.4922401010990143\n",
      "Epoch 35, Batch 34, Loss: 0.4514741003513336\n",
      "Epoch 35, Batch 35, Loss: 0.49074244499206543\n",
      "Epoch 35, Batch 36, Loss: 0.46061262488365173\n",
      "Epoch 35, Batch 37, Loss: 0.49814867973327637\n",
      "Epoch 35, Batch 38, Loss: 0.47449296712875366\n",
      "Epoch 35, Batch 39, Loss: 0.519170880317688\n",
      "Epoch 35, Batch 40, Loss: 0.4588879644870758\n",
      "Epoch 35, Batch 41, Loss: 0.49542108178138733\n",
      "Epoch 35, Batch 42, Loss: 0.46361619234085083\n",
      "Epoch 35, Batch 43, Loss: 0.42341169714927673\n",
      "Epoch 35, Batch 44, Loss: 0.5247867703437805\n",
      "Epoch 35, Batch 45, Loss: 0.4272626042366028\n",
      "Epoch 35, Batch 46, Loss: 0.5034706592559814\n",
      "Epoch 35, Batch 47, Loss: 0.46350955963134766\n",
      "Epoch 35, Batch 48, Loss: 0.49694308638572693\n",
      "Epoch 35, Batch 49, Loss: 0.4489607810974121\n",
      "Epoch 35, Batch 50, Loss: 0.4840467572212219\n",
      "Epoch 35, Batch 51, Loss: 0.47843822836875916\n",
      "Epoch 35, Batch 52, Loss: 0.4853289723396301\n",
      "Epoch 35, Batch 53, Loss: 0.44894498586654663\n",
      "Epoch 35, Batch 54, Loss: 0.4622512757778168\n",
      "Epoch 35, Batch 55, Loss: 0.521246075630188\n",
      "Epoch 35, Batch 56, Loss: 0.49037790298461914\n",
      "Epoch 35, Batch 57, Loss: 0.5122426748275757\n",
      "Epoch 35, Batch 58, Loss: 0.5150995254516602\n",
      "Epoch 35, Batch 59, Loss: 0.44853872060775757\n",
      "Epoch 35, Batch 60, Loss: 0.431910902261734\n",
      "Epoch 35, Batch 61, Loss: 0.4819074273109436\n",
      "Epoch 35, Batch 62, Loss: 0.47474607825279236\n",
      "Epoch 35, Batch 63, Loss: 0.46705514192581177\n",
      "Epoch 35, Batch 64, Loss: 0.49994659423828125\n",
      "Epoch 35, Batch 65, Loss: 0.47239822149276733\n",
      "Epoch 35, Batch 66, Loss: 0.4471666216850281\n",
      "Epoch 35, Batch 67, Loss: 0.4831274151802063\n",
      "Epoch 35, Batch 68, Loss: 0.4843052327632904\n",
      "Epoch 35, Batch 69, Loss: 0.48488593101501465\n",
      "Epoch 35, Batch 70, Loss: 0.4354579448699951\n",
      "Epoch 35, Batch 71, Loss: 0.463205486536026\n",
      "Epoch 35, Batch 72, Loss: 0.42993345856666565\n",
      "Epoch 35, Batch 73, Loss: 0.4969368577003479\n",
      "Epoch 35, Batch 74, Loss: 0.47732019424438477\n",
      "Epoch 35, Batch 75, Loss: 0.48102807998657227\n",
      "Epoch 35, Batch 76, Loss: 0.4916110634803772\n",
      "Epoch 35, Batch 77, Loss: 0.4671756327152252\n",
      "Epoch 35, Batch 78, Loss: 0.45940619707107544\n",
      "Epoch 35, Batch 79, Loss: 0.48020997643470764\n",
      "Epoch 35, Batch 80, Loss: 0.4795047640800476\n",
      "Epoch 35, Batch 81, Loss: 0.49940529465675354\n",
      "Epoch 35, Batch 82, Loss: 0.48692983388900757\n",
      "Epoch 35, Batch 83, Loss: 0.5280452370643616\n",
      "Epoch 35, Batch 84, Loss: 0.4742555022239685\n",
      "Epoch 35, Batch 85, Loss: 0.46530142426490784\n",
      "Epoch 35, Batch 86, Loss: 0.44291678071022034\n",
      "Epoch 35, Batch 87, Loss: 0.4876323640346527\n",
      "Epoch 35, Batch 88, Loss: 0.4397539496421814\n",
      "Epoch 35, Batch 89, Loss: 0.4343712329864502\n",
      "Epoch 35, Batch 90, Loss: 0.4731338620185852\n",
      "Epoch 35, Batch 91, Loss: 0.4941704273223877\n",
      "Epoch 35, Batch 92, Loss: 0.49402347207069397\n",
      "Epoch 35, Batch 93, Loss: 0.47613561153411865\n",
      "Epoch 36, Batch 0, Loss: 0.44839268922805786\n",
      "Epoch 36, Batch 1, Loss: 0.5175079107284546\n",
      "Epoch 36, Batch 2, Loss: 0.4530055522918701\n",
      "Epoch 36, Batch 3, Loss: 0.4819668233394623\n",
      "Epoch 36, Batch 4, Loss: 0.4548339247703552\n",
      "Epoch 36, Batch 5, Loss: 0.5239721536636353\n",
      "Epoch 36, Batch 6, Loss: 0.5446965098381042\n",
      "Epoch 36, Batch 7, Loss: 0.5038643479347229\n",
      "Epoch 36, Batch 8, Loss: 0.49992743134498596\n",
      "Epoch 36, Batch 9, Loss: 0.47225990891456604\n",
      "Epoch 36, Batch 10, Loss: 0.4627855718135834\n",
      "Epoch 36, Batch 11, Loss: 0.5091453790664673\n",
      "Epoch 36, Batch 12, Loss: 0.45141324400901794\n",
      "Epoch 36, Batch 13, Loss: 0.4697282314300537\n",
      "Epoch 36, Batch 14, Loss: 0.5120480060577393\n",
      "Epoch 36, Batch 15, Loss: 0.45979198813438416\n",
      "Epoch 36, Batch 16, Loss: 0.45451074838638306\n",
      "Epoch 36, Batch 17, Loss: 0.4576227068901062\n",
      "Epoch 36, Batch 18, Loss: 0.5278398394584656\n",
      "Epoch 36, Batch 19, Loss: 0.4658586084842682\n",
      "Epoch 36, Batch 20, Loss: 0.4687115550041199\n",
      "Epoch 36, Batch 21, Loss: 0.45810508728027344\n",
      "Epoch 36, Batch 22, Loss: 0.46495842933654785\n",
      "Epoch 36, Batch 23, Loss: 0.4572408199310303\n",
      "Epoch 36, Batch 24, Loss: 0.4988744258880615\n",
      "Epoch 36, Batch 25, Loss: 0.45077601075172424\n",
      "Epoch 36, Batch 26, Loss: 0.5465325117111206\n",
      "Epoch 36, Batch 27, Loss: 0.46535325050354004\n",
      "Epoch 36, Batch 28, Loss: 0.4716883599758148\n",
      "Epoch 36, Batch 29, Loss: 0.4608200490474701\n",
      "Epoch 36, Batch 30, Loss: 0.5205509662628174\n",
      "Epoch 36, Batch 31, Loss: 0.45903944969177246\n",
      "Epoch 36, Batch 32, Loss: 0.44813498854637146\n",
      "Epoch 36, Batch 33, Loss: 0.47062522172927856\n",
      "Epoch 36, Batch 34, Loss: 0.4347633421421051\n",
      "Epoch 36, Batch 35, Loss: 0.44864994287490845\n",
      "Epoch 36, Batch 36, Loss: 0.4572397768497467\n",
      "Epoch 36, Batch 37, Loss: 0.4819274842739105\n",
      "Epoch 36, Batch 38, Loss: 0.4610669016838074\n",
      "Epoch 36, Batch 39, Loss: 0.46607232093811035\n",
      "Epoch 36, Batch 40, Loss: 0.4414384961128235\n",
      "Epoch 36, Batch 41, Loss: 0.4073808193206787\n",
      "Epoch 36, Batch 42, Loss: 0.482826292514801\n",
      "Epoch 36, Batch 43, Loss: 0.5080549716949463\n",
      "Epoch 36, Batch 44, Loss: 0.47145041823387146\n",
      "Epoch 36, Batch 45, Loss: 0.43726056814193726\n",
      "Epoch 36, Batch 46, Loss: 0.4891773760318756\n",
      "Epoch 36, Batch 47, Loss: 0.4620746076107025\n",
      "Epoch 36, Batch 48, Loss: 0.47990116477012634\n",
      "Epoch 36, Batch 49, Loss: 0.46955910325050354\n",
      "Epoch 36, Batch 50, Loss: 0.48931583762168884\n",
      "Epoch 36, Batch 51, Loss: 0.4589773714542389\n",
      "Epoch 36, Batch 52, Loss: 0.46178770065307617\n",
      "Epoch 36, Batch 53, Loss: 0.43817466497421265\n",
      "Epoch 36, Batch 54, Loss: 0.4633301794528961\n",
      "Epoch 36, Batch 55, Loss: 0.4592183232307434\n",
      "Epoch 36, Batch 56, Loss: 0.49261847138404846\n",
      "Epoch 36, Batch 57, Loss: 0.46141380071640015\n",
      "Epoch 36, Batch 58, Loss: 0.4728805422782898\n",
      "Epoch 36, Batch 59, Loss: 0.47010549902915955\n",
      "Epoch 36, Batch 60, Loss: 0.43317675590515137\n",
      "Epoch 36, Batch 61, Loss: 0.47168248891830444\n",
      "Epoch 36, Batch 62, Loss: 0.43076348304748535\n",
      "Epoch 36, Batch 63, Loss: 0.456794410943985\n",
      "Epoch 36, Batch 64, Loss: 0.4501500725746155\n",
      "Epoch 36, Batch 65, Loss: 0.448955774307251\n",
      "Epoch 36, Batch 66, Loss: 0.45382899045944214\n",
      "Epoch 36, Batch 67, Loss: 0.46652668714523315\n",
      "Epoch 36, Batch 68, Loss: 0.452678382396698\n",
      "Epoch 36, Batch 69, Loss: 0.5052272081375122\n",
      "Epoch 36, Batch 70, Loss: 0.45787352323532104\n",
      "Epoch 36, Batch 71, Loss: 0.4544130861759186\n",
      "Epoch 36, Batch 72, Loss: 0.4873430132865906\n",
      "Epoch 36, Batch 73, Loss: 0.4928516745567322\n",
      "Epoch 36, Batch 74, Loss: 0.4553360939025879\n",
      "Epoch 36, Batch 75, Loss: 0.46141108870506287\n",
      "Epoch 36, Batch 76, Loss: 0.4726300835609436\n",
      "Epoch 36, Batch 77, Loss: 0.5109902620315552\n",
      "Epoch 36, Batch 78, Loss: 0.4346238672733307\n",
      "Epoch 36, Batch 79, Loss: 0.46521997451782227\n",
      "Epoch 36, Batch 80, Loss: 0.46255940198898315\n",
      "Epoch 36, Batch 81, Loss: 0.4571874737739563\n",
      "Epoch 36, Batch 82, Loss: 0.4738140106201172\n",
      "Epoch 36, Batch 83, Loss: 0.46094417572021484\n",
      "Epoch 36, Batch 84, Loss: 0.454885333776474\n",
      "Epoch 36, Batch 85, Loss: 0.502800703048706\n",
      "Epoch 36, Batch 86, Loss: 0.49997788667678833\n",
      "Epoch 36, Batch 87, Loss: 0.4614258408546448\n",
      "Epoch 36, Batch 88, Loss: 0.42396244406700134\n",
      "Epoch 36, Batch 89, Loss: 0.4693588316440582\n",
      "Epoch 36, Batch 90, Loss: 0.5053369402885437\n",
      "Epoch 36, Batch 91, Loss: 0.46500468254089355\n",
      "Epoch 36, Batch 92, Loss: 0.4068155288696289\n",
      "Epoch 36, Batch 93, Loss: 0.4675460457801819\n",
      "Epoch 37, Batch 0, Loss: 0.5106567740440369\n",
      "Epoch 37, Batch 1, Loss: 0.4467935562133789\n",
      "Epoch 37, Batch 2, Loss: 0.49727973341941833\n",
      "Epoch 37, Batch 3, Loss: 0.44686684012413025\n",
      "Epoch 37, Batch 4, Loss: 0.45006194710731506\n",
      "Epoch 37, Batch 5, Loss: 0.502352774143219\n",
      "Epoch 37, Batch 6, Loss: 0.4629739224910736\n",
      "Epoch 37, Batch 7, Loss: 0.5012291669845581\n",
      "Epoch 37, Batch 8, Loss: 0.4546321928501129\n",
      "Epoch 37, Batch 9, Loss: 0.4863261580467224\n",
      "Epoch 37, Batch 10, Loss: 0.4556434750556946\n",
      "Epoch 37, Batch 11, Loss: 0.44152751564979553\n",
      "Epoch 37, Batch 12, Loss: 0.4412178099155426\n",
      "Epoch 37, Batch 13, Loss: 0.44046226143836975\n",
      "Epoch 37, Batch 14, Loss: 0.43953242897987366\n",
      "Epoch 37, Batch 15, Loss: 0.4954288601875305\n",
      "Epoch 37, Batch 16, Loss: 0.5256298780441284\n",
      "Epoch 37, Batch 17, Loss: 0.49252113699913025\n",
      "Epoch 37, Batch 18, Loss: 0.47214585542678833\n",
      "Epoch 37, Batch 19, Loss: 0.47468990087509155\n",
      "Epoch 37, Batch 20, Loss: 0.4255290925502777\n",
      "Epoch 37, Batch 21, Loss: 0.4703189432621002\n",
      "Epoch 37, Batch 22, Loss: 0.4570443034172058\n",
      "Epoch 37, Batch 23, Loss: 0.44464215636253357\n",
      "Epoch 37, Batch 24, Loss: 0.43871861696243286\n",
      "Epoch 37, Batch 25, Loss: 0.44166675209999084\n",
      "Epoch 37, Batch 26, Loss: 0.45413535833358765\n",
      "Epoch 37, Batch 27, Loss: 0.4235934317111969\n",
      "Epoch 37, Batch 28, Loss: 0.4222707748413086\n",
      "Epoch 37, Batch 29, Loss: 0.46891728043556213\n",
      "Epoch 37, Batch 30, Loss: 0.43540841341018677\n",
      "Epoch 37, Batch 31, Loss: 0.5276457071304321\n",
      "Epoch 37, Batch 32, Loss: 0.46081462502479553\n",
      "Epoch 37, Batch 33, Loss: 0.4200447201728821\n",
      "Epoch 37, Batch 34, Loss: 0.47817152738571167\n",
      "Epoch 37, Batch 35, Loss: 0.43733128905296326\n",
      "Epoch 37, Batch 36, Loss: 0.516593337059021\n",
      "Epoch 37, Batch 37, Loss: 0.46931391954421997\n",
      "Epoch 37, Batch 38, Loss: 0.43837490677833557\n",
      "Epoch 37, Batch 39, Loss: 0.4897529184818268\n",
      "Epoch 37, Batch 40, Loss: 0.44875025749206543\n",
      "Epoch 37, Batch 41, Loss: 0.39598363637924194\n",
      "Epoch 37, Batch 42, Loss: 0.5013157725334167\n",
      "Epoch 37, Batch 43, Loss: 0.502282440662384\n",
      "Epoch 37, Batch 44, Loss: 0.4626579284667969\n",
      "Epoch 37, Batch 45, Loss: 0.4946900010108948\n",
      "Epoch 37, Batch 46, Loss: 0.4733444154262543\n",
      "Epoch 37, Batch 47, Loss: 0.5218164920806885\n",
      "Epoch 37, Batch 48, Loss: 0.42522940039634705\n",
      "Epoch 37, Batch 49, Loss: 0.44610124826431274\n",
      "Epoch 37, Batch 50, Loss: 0.44259676337242126\n",
      "Epoch 37, Batch 51, Loss: 0.3926563858985901\n",
      "Epoch 37, Batch 52, Loss: 0.4591028690338135\n",
      "Epoch 37, Batch 53, Loss: 0.47212499380111694\n",
      "Epoch 37, Batch 54, Loss: 0.5131665468215942\n",
      "Epoch 37, Batch 55, Loss: 0.5071033835411072\n",
      "Epoch 37, Batch 56, Loss: 0.4060220718383789\n",
      "Epoch 37, Batch 57, Loss: 0.4655439257621765\n",
      "Epoch 37, Batch 58, Loss: 0.4567268490791321\n",
      "Epoch 37, Batch 59, Loss: 0.43465858697891235\n",
      "Epoch 37, Batch 60, Loss: 0.49925217032432556\n",
      "Epoch 37, Batch 61, Loss: 0.4354971945285797\n",
      "Epoch 37, Batch 62, Loss: 0.4654286801815033\n",
      "Epoch 37, Batch 63, Loss: 0.4317196309566498\n",
      "Epoch 37, Batch 64, Loss: 0.44108399748802185\n",
      "Epoch 37, Batch 65, Loss: 0.4534531235694885\n",
      "Epoch 37, Batch 66, Loss: 0.477308452129364\n",
      "Epoch 37, Batch 67, Loss: 0.4853610396385193\n",
      "Epoch 37, Batch 68, Loss: 0.4712715744972229\n",
      "Epoch 37, Batch 69, Loss: 0.5015199780464172\n",
      "Epoch 37, Batch 70, Loss: 0.43936529755592346\n",
      "Epoch 37, Batch 71, Loss: 0.5351378917694092\n",
      "Epoch 37, Batch 72, Loss: 0.4547043740749359\n",
      "Epoch 37, Batch 73, Loss: 0.43837612867355347\n",
      "Epoch 37, Batch 74, Loss: 0.42078033089637756\n",
      "Epoch 37, Batch 75, Loss: 0.46478232741355896\n",
      "Epoch 37, Batch 76, Loss: 0.47507014870643616\n",
      "Epoch 37, Batch 77, Loss: 0.4611654281616211\n",
      "Epoch 37, Batch 78, Loss: 0.5005818009376526\n",
      "Epoch 37, Batch 79, Loss: 0.5177117586135864\n",
      "Epoch 37, Batch 80, Loss: 0.4650871157646179\n",
      "Epoch 37, Batch 81, Loss: 0.45332813262939453\n",
      "Epoch 37, Batch 82, Loss: 0.45119136571884155\n",
      "Epoch 37, Batch 83, Loss: 0.44168171286582947\n",
      "Epoch 37, Batch 84, Loss: 0.5275485515594482\n",
      "Epoch 37, Batch 85, Loss: 0.4843445420265198\n",
      "Epoch 37, Batch 86, Loss: 0.4443802833557129\n",
      "Epoch 37, Batch 87, Loss: 0.45810467004776\n",
      "Epoch 37, Batch 88, Loss: 0.44351059198379517\n",
      "Epoch 37, Batch 89, Loss: 0.43670424818992615\n",
      "Epoch 37, Batch 90, Loss: 0.4608120322227478\n",
      "Epoch 37, Batch 91, Loss: 0.45465102791786194\n",
      "Epoch 37, Batch 92, Loss: 0.4936180114746094\n",
      "Epoch 37, Batch 93, Loss: 0.4594748914241791\n",
      "Epoch 38, Batch 0, Loss: 0.43061208724975586\n",
      "Epoch 38, Batch 1, Loss: 0.46837130188941956\n",
      "Epoch 38, Batch 2, Loss: 0.4155386984348297\n",
      "Epoch 38, Batch 3, Loss: 0.4438578486442566\n",
      "Epoch 38, Batch 4, Loss: 0.45278453826904297\n",
      "Epoch 38, Batch 5, Loss: 0.4668103754520416\n",
      "Epoch 38, Batch 6, Loss: 0.43541470170021057\n",
      "Epoch 38, Batch 7, Loss: 0.42242327332496643\n",
      "Epoch 38, Batch 8, Loss: 0.491326242685318\n",
      "Epoch 38, Batch 9, Loss: 0.44524988532066345\n",
      "Epoch 38, Batch 10, Loss: 0.46946316957473755\n",
      "Epoch 38, Batch 11, Loss: 0.4687291979789734\n",
      "Epoch 38, Batch 12, Loss: 0.41507062315940857\n",
      "Epoch 38, Batch 13, Loss: 0.4664863049983978\n",
      "Epoch 38, Batch 14, Loss: 0.5178993344306946\n",
      "Epoch 38, Batch 15, Loss: 0.44673722982406616\n",
      "Epoch 38, Batch 16, Loss: 0.45433980226516724\n",
      "Epoch 38, Batch 17, Loss: 0.45882683992385864\n",
      "Epoch 38, Batch 18, Loss: 0.4249647557735443\n",
      "Epoch 38, Batch 19, Loss: 0.46374526619911194\n",
      "Epoch 38, Batch 20, Loss: 0.48942098021507263\n",
      "Epoch 38, Batch 21, Loss: 0.40993791818618774\n",
      "Epoch 38, Batch 22, Loss: 0.4378277659416199\n",
      "Epoch 38, Batch 23, Loss: 0.4473861753940582\n",
      "Epoch 38, Batch 24, Loss: 0.4701665937900543\n",
      "Epoch 38, Batch 25, Loss: 0.4842326045036316\n",
      "Epoch 38, Batch 26, Loss: 0.4763888716697693\n",
      "Epoch 38, Batch 27, Loss: 0.47367820143699646\n",
      "Epoch 38, Batch 28, Loss: 0.46804577112197876\n",
      "Epoch 38, Batch 29, Loss: 0.5232770442962646\n",
      "Epoch 38, Batch 30, Loss: 0.4528215825557709\n",
      "Epoch 38, Batch 31, Loss: 0.500346839427948\n",
      "Epoch 38, Batch 32, Loss: 0.4307098984718323\n",
      "Epoch 38, Batch 33, Loss: 0.43087610602378845\n",
      "Epoch 38, Batch 34, Loss: 0.49756425619125366\n",
      "Epoch 38, Batch 35, Loss: 0.42717307806015015\n",
      "Epoch 38, Batch 36, Loss: 0.4301636815071106\n",
      "Epoch 38, Batch 37, Loss: 0.4557856619358063\n",
      "Epoch 38, Batch 38, Loss: 0.4416353702545166\n",
      "Epoch 38, Batch 39, Loss: 0.4292590618133545\n",
      "Epoch 38, Batch 40, Loss: 0.45868152379989624\n",
      "Epoch 38, Batch 41, Loss: 0.478609174489975\n",
      "Epoch 38, Batch 42, Loss: 0.46221405267715454\n",
      "Epoch 38, Batch 43, Loss: 0.4658854603767395\n",
      "Epoch 38, Batch 44, Loss: 0.5201316475868225\n",
      "Epoch 38, Batch 45, Loss: 0.4482242166996002\n",
      "Epoch 38, Batch 46, Loss: 0.4672237038612366\n",
      "Epoch 38, Batch 47, Loss: 0.4693834185600281\n",
      "Epoch 38, Batch 48, Loss: 0.4387930929660797\n",
      "Epoch 38, Batch 49, Loss: 0.45079928636550903\n",
      "Epoch 38, Batch 50, Loss: 0.46534091234207153\n",
      "Epoch 38, Batch 51, Loss: 0.4840628504753113\n",
      "Epoch 38, Batch 52, Loss: 0.4521431028842926\n",
      "Epoch 38, Batch 53, Loss: 0.44724422693252563\n",
      "Epoch 38, Batch 54, Loss: 0.4281286299228668\n",
      "Epoch 38, Batch 55, Loss: 0.4839293360710144\n",
      "Epoch 38, Batch 56, Loss: 0.45178547501564026\n",
      "Epoch 38, Batch 57, Loss: 0.4576847553253174\n",
      "Epoch 38, Batch 58, Loss: 0.4694744646549225\n",
      "Epoch 38, Batch 59, Loss: 0.4667115807533264\n",
      "Epoch 38, Batch 60, Loss: 0.4641382098197937\n",
      "Epoch 38, Batch 61, Loss: 0.49485936760902405\n",
      "Epoch 38, Batch 62, Loss: 0.455941766500473\n",
      "Epoch 38, Batch 63, Loss: 0.42058292031288147\n",
      "Epoch 38, Batch 64, Loss: 0.4535016119480133\n",
      "Epoch 38, Batch 65, Loss: 0.39348331093788147\n",
      "Epoch 38, Batch 66, Loss: 0.45473557710647583\n",
      "Epoch 38, Batch 67, Loss: 0.4592830538749695\n",
      "Epoch 38, Batch 68, Loss: 0.4708045423030853\n",
      "Epoch 38, Batch 69, Loss: 0.46372199058532715\n",
      "Epoch 38, Batch 70, Loss: 0.4602625370025635\n",
      "Epoch 38, Batch 71, Loss: 0.4849691390991211\n",
      "Epoch 38, Batch 72, Loss: 0.4327339231967926\n",
      "Epoch 38, Batch 73, Loss: 0.4943672716617584\n",
      "Epoch 38, Batch 74, Loss: 0.4571216106414795\n",
      "Epoch 38, Batch 75, Loss: 0.49742117524147034\n",
      "Epoch 38, Batch 76, Loss: 0.4565375745296478\n",
      "Epoch 38, Batch 77, Loss: 0.46605443954467773\n",
      "Epoch 38, Batch 78, Loss: 0.43641266226768494\n",
      "Epoch 38, Batch 79, Loss: 0.43387579917907715\n",
      "Epoch 38, Batch 80, Loss: 0.4270571172237396\n",
      "Epoch 38, Batch 81, Loss: 0.4869239330291748\n",
      "Epoch 38, Batch 82, Loss: 0.46391743421554565\n",
      "Epoch 38, Batch 83, Loss: 0.47896185517311096\n",
      "Epoch 38, Batch 84, Loss: 0.4758753776550293\n",
      "Epoch 38, Batch 85, Loss: 0.4543217122554779\n",
      "Epoch 38, Batch 86, Loss: 0.4117746949195862\n",
      "Epoch 38, Batch 87, Loss: 0.4893064498901367\n",
      "Epoch 38, Batch 88, Loss: 0.5065757632255554\n",
      "Epoch 38, Batch 89, Loss: 0.47293609380722046\n",
      "Epoch 38, Batch 90, Loss: 0.46139392256736755\n",
      "Epoch 38, Batch 91, Loss: 0.40470805764198303\n",
      "Epoch 38, Batch 92, Loss: 0.46587008237838745\n",
      "Epoch 38, Batch 93, Loss: 0.4759758710861206\n",
      "Epoch 39, Batch 0, Loss: 0.4496525824069977\n",
      "Epoch 39, Batch 1, Loss: 0.45853692293167114\n",
      "Epoch 39, Batch 2, Loss: 0.45168766379356384\n",
      "Epoch 39, Batch 3, Loss: 0.4087952673435211\n",
      "Epoch 39, Batch 4, Loss: 0.4148409962654114\n",
      "Epoch 39, Batch 5, Loss: 0.44976991415023804\n",
      "Epoch 39, Batch 6, Loss: 0.4355807304382324\n",
      "Epoch 39, Batch 7, Loss: 0.4765518307685852\n",
      "Epoch 39, Batch 8, Loss: 0.47832125425338745\n",
      "Epoch 39, Batch 9, Loss: 0.4649409353733063\n",
      "Epoch 39, Batch 10, Loss: 0.48567938804626465\n",
      "Epoch 39, Batch 11, Loss: 0.4390435218811035\n",
      "Epoch 39, Batch 12, Loss: 0.48720812797546387\n",
      "Epoch 39, Batch 13, Loss: 0.4505241811275482\n",
      "Epoch 39, Batch 14, Loss: 0.456672340631485\n",
      "Epoch 39, Batch 15, Loss: 0.43705353140830994\n",
      "Epoch 39, Batch 16, Loss: 0.4294964671134949\n",
      "Epoch 39, Batch 17, Loss: 0.4228151738643646\n",
      "Epoch 39, Batch 18, Loss: 0.5085688829421997\n",
      "Epoch 39, Batch 19, Loss: 0.4224158823490143\n",
      "Epoch 39, Batch 20, Loss: 0.4144175946712494\n",
      "Epoch 39, Batch 21, Loss: 0.4450433850288391\n",
      "Epoch 39, Batch 22, Loss: 0.43775510787963867\n",
      "Epoch 39, Batch 23, Loss: 0.49923649430274963\n",
      "Epoch 39, Batch 24, Loss: 0.4738697111606598\n",
      "Epoch 39, Batch 25, Loss: 0.5079358816146851\n",
      "Epoch 39, Batch 26, Loss: 0.4686160683631897\n",
      "Epoch 39, Batch 27, Loss: 0.4823252558708191\n",
      "Epoch 39, Batch 28, Loss: 0.4116707444190979\n",
      "Epoch 39, Batch 29, Loss: 0.46820488572120667\n",
      "Epoch 39, Batch 30, Loss: 0.47727465629577637\n",
      "Epoch 39, Batch 31, Loss: 0.4457758367061615\n",
      "Epoch 39, Batch 32, Loss: 0.4307705760002136\n",
      "Epoch 39, Batch 33, Loss: 0.5032587051391602\n",
      "Epoch 39, Batch 34, Loss: 0.4985863268375397\n",
      "Epoch 39, Batch 35, Loss: 0.40171489119529724\n",
      "Epoch 39, Batch 36, Loss: 0.4510788917541504\n",
      "Epoch 39, Batch 37, Loss: 0.4762422442436218\n",
      "Epoch 39, Batch 38, Loss: 0.49485379457473755\n",
      "Epoch 39, Batch 39, Loss: 0.46993058919906616\n",
      "Epoch 39, Batch 40, Loss: 0.45554786920547485\n",
      "Epoch 39, Batch 41, Loss: 0.4256812036037445\n",
      "Epoch 39, Batch 42, Loss: 0.4750109612941742\n",
      "Epoch 39, Batch 43, Loss: 0.4100327491760254\n",
      "Epoch 39, Batch 44, Loss: 0.39716821908950806\n",
      "Epoch 39, Batch 45, Loss: 0.42519742250442505\n",
      "Epoch 39, Batch 46, Loss: 0.4534882605075836\n",
      "Epoch 39, Batch 47, Loss: 0.41765886545181274\n",
      "Epoch 39, Batch 48, Loss: 0.48113545775413513\n",
      "Epoch 39, Batch 49, Loss: 0.46181267499923706\n",
      "Epoch 39, Batch 50, Loss: 0.48613953590393066\n",
      "Epoch 39, Batch 51, Loss: 0.42458248138427734\n",
      "Epoch 39, Batch 52, Loss: 0.4456849694252014\n",
      "Epoch 39, Batch 53, Loss: 0.49632328748703003\n",
      "Epoch 39, Batch 54, Loss: 0.4218122363090515\n",
      "Epoch 39, Batch 55, Loss: 0.45740142464637756\n",
      "Epoch 39, Batch 56, Loss: 0.4625808596611023\n",
      "Epoch 39, Batch 57, Loss: 0.4466741681098938\n",
      "Epoch 39, Batch 58, Loss: 0.48091620206832886\n",
      "Epoch 39, Batch 59, Loss: 0.4234692454338074\n",
      "Epoch 39, Batch 60, Loss: 0.4836527407169342\n",
      "Epoch 39, Batch 61, Loss: 0.4859870374202728\n",
      "Epoch 39, Batch 62, Loss: 0.43607550859451294\n",
      "Epoch 39, Batch 63, Loss: 0.45504865050315857\n",
      "Epoch 39, Batch 64, Loss: 0.47442084550857544\n",
      "Epoch 39, Batch 65, Loss: 0.44028720259666443\n",
      "Epoch 39, Batch 66, Loss: 0.5303832292556763\n",
      "Epoch 39, Batch 67, Loss: 0.4267760217189789\n",
      "Epoch 39, Batch 68, Loss: 0.4384177327156067\n",
      "Epoch 39, Batch 69, Loss: 0.4693051874637604\n",
      "Epoch 39, Batch 70, Loss: 0.43494781851768494\n",
      "Epoch 39, Batch 71, Loss: 0.43320322036743164\n",
      "Epoch 39, Batch 72, Loss: 0.4539998471736908\n",
      "Epoch 39, Batch 73, Loss: 0.43090733885765076\n",
      "Epoch 39, Batch 74, Loss: 0.4678071141242981\n",
      "Epoch 39, Batch 75, Loss: 0.40558385848999023\n",
      "Epoch 39, Batch 76, Loss: 0.5064901113510132\n",
      "Epoch 39, Batch 77, Loss: 0.4808225631713867\n",
      "Epoch 39, Batch 78, Loss: 0.3810417056083679\n",
      "Epoch 39, Batch 79, Loss: 0.4226521849632263\n",
      "Epoch 39, Batch 80, Loss: 0.40262371301651\n",
      "Epoch 39, Batch 81, Loss: 0.46570318937301636\n",
      "Epoch 39, Batch 82, Loss: 0.4150159955024719\n",
      "Epoch 39, Batch 83, Loss: 0.43138590455055237\n",
      "Epoch 39, Batch 84, Loss: 0.48030886054039\n",
      "Epoch 39, Batch 85, Loss: 0.4397701323032379\n",
      "Epoch 39, Batch 86, Loss: 0.4861176609992981\n",
      "Epoch 39, Batch 87, Loss: 0.44125860929489136\n",
      "Epoch 39, Batch 88, Loss: 0.44923216104507446\n",
      "Epoch 39, Batch 89, Loss: 0.4490918517112732\n",
      "Epoch 39, Batch 90, Loss: 0.4517118036746979\n",
      "Epoch 39, Batch 91, Loss: 0.4958097040653229\n",
      "Epoch 39, Batch 92, Loss: 0.506392240524292\n",
      "Epoch 39, Batch 93, Loss: 0.4365275204181671\n",
      "Epoch 40, Batch 0, Loss: 0.4409819543361664\n",
      "Epoch 40, Batch 1, Loss: 0.507496178150177\n",
      "Epoch 40, Batch 2, Loss: 0.48137807846069336\n",
      "Epoch 40, Batch 3, Loss: 0.47181862592697144\n",
      "Epoch 40, Batch 4, Loss: 0.44277095794677734\n",
      "Epoch 40, Batch 5, Loss: 0.42190486192703247\n",
      "Epoch 40, Batch 6, Loss: 0.4434622824192047\n",
      "Epoch 40, Batch 7, Loss: 0.4151952862739563\n",
      "Epoch 40, Batch 8, Loss: 0.473649263381958\n",
      "Epoch 40, Batch 9, Loss: 0.43339475989341736\n",
      "Epoch 40, Batch 10, Loss: 0.49671682715415955\n",
      "Epoch 40, Batch 11, Loss: 0.4682851731777191\n",
      "Epoch 40, Batch 12, Loss: 0.4868384301662445\n",
      "Epoch 40, Batch 13, Loss: 0.46722501516342163\n",
      "Epoch 40, Batch 14, Loss: 0.47795653343200684\n",
      "Epoch 40, Batch 15, Loss: 0.43653473258018494\n",
      "Epoch 40, Batch 16, Loss: 0.46726465225219727\n",
      "Epoch 40, Batch 17, Loss: 0.40946587920188904\n",
      "Epoch 40, Batch 18, Loss: 0.41565585136413574\n",
      "Epoch 40, Batch 19, Loss: 0.41347652673721313\n",
      "Epoch 40, Batch 20, Loss: 0.48677951097488403\n",
      "Epoch 40, Batch 21, Loss: 0.42274919152259827\n",
      "Epoch 40, Batch 22, Loss: 0.412126362323761\n",
      "Epoch 40, Batch 23, Loss: 0.46301212906837463\n",
      "Epoch 40, Batch 24, Loss: 0.43130117654800415\n",
      "Epoch 40, Batch 25, Loss: 0.46824511885643005\n",
      "Epoch 40, Batch 26, Loss: 0.44233208894729614\n",
      "Epoch 40, Batch 27, Loss: 0.43937721848487854\n",
      "Epoch 40, Batch 28, Loss: 0.45172348618507385\n",
      "Epoch 40, Batch 29, Loss: 0.4069361090660095\n",
      "Epoch 40, Batch 30, Loss: 0.41755956411361694\n",
      "Epoch 40, Batch 31, Loss: 0.40173497796058655\n",
      "Epoch 40, Batch 32, Loss: 0.4254671037197113\n",
      "Epoch 40, Batch 33, Loss: 0.4296521246433258\n",
      "Epoch 40, Batch 34, Loss: 0.47369369864463806\n",
      "Epoch 40, Batch 35, Loss: 0.43842071294784546\n",
      "Epoch 40, Batch 36, Loss: 0.45517605543136597\n",
      "Epoch 40, Batch 37, Loss: 0.463794469833374\n",
      "Epoch 40, Batch 38, Loss: 0.3845607042312622\n",
      "Epoch 40, Batch 39, Loss: 0.4612334370613098\n",
      "Epoch 40, Batch 40, Loss: 0.4536595940589905\n",
      "Epoch 40, Batch 41, Loss: 0.4395364820957184\n",
      "Epoch 40, Batch 42, Loss: 0.44927191734313965\n",
      "Epoch 40, Batch 43, Loss: 0.4814043939113617\n",
      "Epoch 40, Batch 44, Loss: 0.5143707394599915\n",
      "Epoch 40, Batch 45, Loss: 0.4436193108558655\n",
      "Epoch 40, Batch 46, Loss: 0.46967124938964844\n",
      "Epoch 40, Batch 47, Loss: 0.4519011378288269\n",
      "Epoch 40, Batch 48, Loss: 0.42046627402305603\n",
      "Epoch 40, Batch 49, Loss: 0.48612919449806213\n",
      "Epoch 40, Batch 50, Loss: 0.45792636275291443\n",
      "Epoch 40, Batch 51, Loss: 0.40921488404273987\n",
      "Epoch 40, Batch 52, Loss: 0.4724207818508148\n",
      "Epoch 40, Batch 53, Loss: 0.45397695899009705\n",
      "Epoch 40, Batch 54, Loss: 0.4260152280330658\n",
      "Epoch 40, Batch 55, Loss: 0.4317929744720459\n",
      "Epoch 40, Batch 56, Loss: 0.41936612129211426\n",
      "Epoch 40, Batch 57, Loss: 0.42811083793640137\n",
      "Epoch 40, Batch 58, Loss: 0.4663875102996826\n",
      "Epoch 40, Batch 59, Loss: 0.4626522958278656\n",
      "Epoch 40, Batch 60, Loss: 0.47586551308631897\n",
      "Epoch 40, Batch 61, Loss: 0.5016883611679077\n",
      "Epoch 40, Batch 62, Loss: 0.44428783655166626\n",
      "Epoch 40, Batch 63, Loss: 0.4619961678981781\n",
      "Epoch 40, Batch 64, Loss: 0.44611701369285583\n",
      "Epoch 40, Batch 65, Loss: 0.43521514534950256\n",
      "Epoch 40, Batch 66, Loss: 0.43691006302833557\n",
      "Epoch 40, Batch 67, Loss: 0.43172067403793335\n",
      "Epoch 40, Batch 68, Loss: 0.422558069229126\n",
      "Epoch 40, Batch 69, Loss: 0.4005642533302307\n",
      "Epoch 40, Batch 70, Loss: 0.46843498945236206\n",
      "Epoch 40, Batch 71, Loss: 0.4390265941619873\n",
      "Epoch 40, Batch 72, Loss: 0.44722867012023926\n",
      "Epoch 40, Batch 73, Loss: 0.4881284832954407\n",
      "Epoch 40, Batch 74, Loss: 0.5012995600700378\n",
      "Epoch 40, Batch 75, Loss: 0.4249269962310791\n",
      "Epoch 40, Batch 76, Loss: 0.5106549859046936\n",
      "Epoch 40, Batch 77, Loss: 0.43799859285354614\n",
      "Epoch 40, Batch 78, Loss: 0.42075324058532715\n",
      "Epoch 40, Batch 79, Loss: 0.4377667307853699\n",
      "Epoch 40, Batch 80, Loss: 0.43665018677711487\n",
      "Epoch 40, Batch 81, Loss: 0.4435831904411316\n",
      "Epoch 40, Batch 82, Loss: 0.4514797627925873\n",
      "Epoch 40, Batch 83, Loss: 0.4380679726600647\n",
      "Epoch 40, Batch 84, Loss: 0.4534386694431305\n",
      "Epoch 40, Batch 85, Loss: 0.43191462755203247\n",
      "Epoch 40, Batch 86, Loss: 0.47154489159584045\n",
      "Epoch 40, Batch 87, Loss: 0.5092821717262268\n",
      "Epoch 40, Batch 88, Loss: 0.45146313309669495\n",
      "Epoch 40, Batch 89, Loss: 0.4823947846889496\n",
      "Epoch 40, Batch 90, Loss: 0.43999385833740234\n",
      "Epoch 40, Batch 91, Loss: 0.42852744460105896\n",
      "Epoch 40, Batch 92, Loss: 0.41193312406539917\n",
      "Epoch 40, Batch 93, Loss: 0.42032864689826965\n",
      "Epoch 41, Batch 0, Loss: 0.43087226152420044\n",
      "Epoch 41, Batch 1, Loss: 0.4756126403808594\n",
      "Epoch 41, Batch 2, Loss: 0.4654894769191742\n",
      "Epoch 41, Batch 3, Loss: 0.4436010420322418\n",
      "Epoch 41, Batch 4, Loss: 0.4813331961631775\n",
      "Epoch 41, Batch 5, Loss: 0.43509602546691895\n",
      "Epoch 41, Batch 6, Loss: 0.4318715035915375\n",
      "Epoch 41, Batch 7, Loss: 0.4296194612979889\n",
      "Epoch 41, Batch 8, Loss: 0.3906748294830322\n",
      "Epoch 41, Batch 9, Loss: 0.45159897208213806\n",
      "Epoch 41, Batch 10, Loss: 0.4336637854576111\n",
      "Epoch 41, Batch 11, Loss: 0.4179799556732178\n",
      "Epoch 41, Batch 12, Loss: 0.43957677483558655\n",
      "Epoch 41, Batch 13, Loss: 0.4175138473510742\n",
      "Epoch 41, Batch 14, Loss: 0.419933557510376\n",
      "Epoch 41, Batch 15, Loss: 0.4439144730567932\n",
      "Epoch 41, Batch 16, Loss: 0.4104697108268738\n",
      "Epoch 41, Batch 17, Loss: 0.4103609025478363\n",
      "Epoch 41, Batch 18, Loss: 0.45784059166908264\n",
      "Epoch 41, Batch 19, Loss: 0.47258901596069336\n",
      "Epoch 41, Batch 20, Loss: 0.47165435552597046\n",
      "Epoch 41, Batch 21, Loss: 0.4058108925819397\n",
      "Epoch 41, Batch 22, Loss: 0.4545738101005554\n",
      "Epoch 41, Batch 23, Loss: 0.46462804079055786\n",
      "Epoch 41, Batch 24, Loss: 0.4810856878757477\n",
      "Epoch 41, Batch 25, Loss: 0.45840415358543396\n",
      "Epoch 41, Batch 26, Loss: 0.4563191533088684\n",
      "Epoch 41, Batch 27, Loss: 0.45016103982925415\n",
      "Epoch 41, Batch 28, Loss: 0.441653311252594\n",
      "Epoch 41, Batch 29, Loss: 0.4221912920475006\n",
      "Epoch 41, Batch 30, Loss: 0.4289182126522064\n",
      "Epoch 41, Batch 31, Loss: 0.4461154043674469\n",
      "Epoch 41, Batch 32, Loss: 0.43598055839538574\n",
      "Epoch 41, Batch 33, Loss: 0.45305126905441284\n",
      "Epoch 41, Batch 34, Loss: 0.48840588331222534\n",
      "Epoch 41, Batch 35, Loss: 0.4496828019618988\n",
      "Epoch 41, Batch 36, Loss: 0.463248074054718\n",
      "Epoch 41, Batch 37, Loss: 0.4087580144405365\n",
      "Epoch 41, Batch 38, Loss: 0.438319593667984\n",
      "Epoch 41, Batch 39, Loss: 0.468965619802475\n",
      "Epoch 41, Batch 40, Loss: 0.45866894721984863\n",
      "Epoch 41, Batch 41, Loss: 0.4356761574745178\n",
      "Epoch 41, Batch 42, Loss: 0.4207518696784973\n",
      "Epoch 41, Batch 43, Loss: 0.4455075263977051\n",
      "Epoch 41, Batch 44, Loss: 0.4530862271785736\n",
      "Epoch 41, Batch 45, Loss: 0.415618896484375\n",
      "Epoch 41, Batch 46, Loss: 0.479268878698349\n",
      "Epoch 41, Batch 47, Loss: 0.446007639169693\n",
      "Epoch 41, Batch 48, Loss: 0.4535391330718994\n",
      "Epoch 41, Batch 49, Loss: 0.44357210397720337\n",
      "Epoch 41, Batch 50, Loss: 0.44701939821243286\n",
      "Epoch 41, Batch 51, Loss: 0.44121256470680237\n",
      "Epoch 41, Batch 52, Loss: 0.4737991690635681\n",
      "Epoch 41, Batch 53, Loss: 0.4845806062221527\n",
      "Epoch 41, Batch 54, Loss: 0.4011405408382416\n",
      "Epoch 41, Batch 55, Loss: 0.4624769687652588\n",
      "Epoch 41, Batch 56, Loss: 0.4688102602958679\n",
      "Epoch 41, Batch 57, Loss: 0.4138077199459076\n",
      "Epoch 41, Batch 58, Loss: 0.45471906661987305\n",
      "Epoch 41, Batch 59, Loss: 0.4380405843257904\n",
      "Epoch 41, Batch 60, Loss: 0.45054373145103455\n",
      "Epoch 41, Batch 61, Loss: 0.4665309488773346\n",
      "Epoch 41, Batch 62, Loss: 0.48792457580566406\n",
      "Epoch 41, Batch 63, Loss: 0.4580085873603821\n",
      "Epoch 41, Batch 64, Loss: 0.43582266569137573\n",
      "Epoch 41, Batch 65, Loss: 0.4533284604549408\n",
      "Epoch 41, Batch 66, Loss: 0.4578064978122711\n",
      "Epoch 41, Batch 67, Loss: 0.43805742263793945\n",
      "Epoch 41, Batch 68, Loss: 0.4566006064414978\n",
      "Epoch 41, Batch 69, Loss: 0.4805360436439514\n",
      "Epoch 41, Batch 70, Loss: 0.4616922438144684\n",
      "Epoch 41, Batch 71, Loss: 0.4099588990211487\n",
      "Epoch 41, Batch 72, Loss: 0.4006717801094055\n",
      "Epoch 41, Batch 73, Loss: 0.38810405135154724\n",
      "Epoch 41, Batch 74, Loss: 0.43873172998428345\n",
      "Epoch 41, Batch 75, Loss: 0.43495383858680725\n",
      "Epoch 41, Batch 76, Loss: 0.4737195372581482\n",
      "Epoch 41, Batch 77, Loss: 0.4180850386619568\n",
      "Epoch 41, Batch 78, Loss: 0.4773203432559967\n",
      "Epoch 41, Batch 79, Loss: 0.4633611738681793\n",
      "Epoch 41, Batch 80, Loss: 0.4854390025138855\n",
      "Epoch 41, Batch 81, Loss: 0.4184812605381012\n",
      "Epoch 41, Batch 82, Loss: 0.39192527532577515\n",
      "Epoch 41, Batch 83, Loss: 0.4672310948371887\n",
      "Epoch 41, Batch 84, Loss: 0.5012294054031372\n",
      "Epoch 41, Batch 85, Loss: 0.4395558834075928\n",
      "Epoch 41, Batch 86, Loss: 0.3988201320171356\n",
      "Epoch 41, Batch 87, Loss: 0.4560319483280182\n",
      "Epoch 41, Batch 88, Loss: 0.41983503103256226\n",
      "Epoch 41, Batch 89, Loss: 0.44752106070518494\n",
      "Epoch 41, Batch 90, Loss: 0.4432784914970398\n",
      "Epoch 41, Batch 91, Loss: 0.4461442828178406\n",
      "Epoch 41, Batch 92, Loss: 0.40979069471359253\n",
      "Epoch 41, Batch 93, Loss: 0.3898182809352875\n",
      "Epoch 42, Batch 0, Loss: 0.3952934741973877\n",
      "Epoch 42, Batch 1, Loss: 0.4381020963191986\n",
      "Epoch 42, Batch 2, Loss: 0.41535717248916626\n",
      "Epoch 42, Batch 3, Loss: 0.46327805519104004\n",
      "Epoch 42, Batch 4, Loss: 0.42731475830078125\n",
      "Epoch 42, Batch 5, Loss: 0.43924084305763245\n",
      "Epoch 42, Batch 6, Loss: 0.41995805501937866\n",
      "Epoch 42, Batch 7, Loss: 0.44393783807754517\n",
      "Epoch 42, Batch 8, Loss: 0.4714905619621277\n",
      "Epoch 42, Batch 9, Loss: 0.44187426567077637\n",
      "Epoch 42, Batch 10, Loss: 0.4385550916194916\n",
      "Epoch 42, Batch 11, Loss: 0.42016687989234924\n",
      "Epoch 42, Batch 12, Loss: 0.4182250499725342\n",
      "Epoch 42, Batch 13, Loss: 0.42928701639175415\n",
      "Epoch 42, Batch 14, Loss: 0.4173111319541931\n",
      "Epoch 42, Batch 15, Loss: 0.47471943497657776\n",
      "Epoch 42, Batch 16, Loss: 0.44345131516456604\n",
      "Epoch 42, Batch 17, Loss: 0.43982893228530884\n",
      "Epoch 42, Batch 18, Loss: 0.45400482416152954\n",
      "Epoch 42, Batch 19, Loss: 0.49391302466392517\n",
      "Epoch 42, Batch 20, Loss: 0.4118809103965759\n",
      "Epoch 42, Batch 21, Loss: 0.4162059426307678\n",
      "Epoch 42, Batch 22, Loss: 0.4308207929134369\n",
      "Epoch 42, Batch 23, Loss: 0.45779451727867126\n",
      "Epoch 42, Batch 24, Loss: 0.42022642493247986\n",
      "Epoch 42, Batch 25, Loss: 0.49295902252197266\n",
      "Epoch 42, Batch 26, Loss: 0.4816111922264099\n",
      "Epoch 42, Batch 27, Loss: 0.49157342314720154\n",
      "Epoch 42, Batch 28, Loss: 0.48773717880249023\n",
      "Epoch 42, Batch 29, Loss: 0.4492422044277191\n",
      "Epoch 42, Batch 30, Loss: 0.4123618006706238\n",
      "Epoch 42, Batch 31, Loss: 0.4825513958930969\n",
      "Epoch 42, Batch 32, Loss: 0.47082382440567017\n",
      "Epoch 42, Batch 33, Loss: 0.4861162602901459\n",
      "Epoch 42, Batch 34, Loss: 0.4474378526210785\n",
      "Epoch 42, Batch 35, Loss: 0.44655752182006836\n",
      "Epoch 42, Batch 36, Loss: 0.4488905370235443\n",
      "Epoch 42, Batch 37, Loss: 0.4636969566345215\n",
      "Epoch 42, Batch 38, Loss: 0.453334242105484\n",
      "Epoch 42, Batch 39, Loss: 0.388569712638855\n",
      "Epoch 42, Batch 40, Loss: 0.46134012937545776\n",
      "Epoch 42, Batch 41, Loss: 0.4596516191959381\n",
      "Epoch 42, Batch 42, Loss: 0.43800869584083557\n",
      "Epoch 42, Batch 43, Loss: 0.42738309502601624\n",
      "Epoch 42, Batch 44, Loss: 0.4474756717681885\n",
      "Epoch 42, Batch 45, Loss: 0.4578081965446472\n",
      "Epoch 42, Batch 46, Loss: 0.40920209884643555\n",
      "Epoch 42, Batch 47, Loss: 0.4125540852546692\n",
      "Epoch 42, Batch 48, Loss: 0.43423566222190857\n",
      "Epoch 42, Batch 49, Loss: 0.3963041603565216\n",
      "Epoch 42, Batch 50, Loss: 0.4576502740383148\n",
      "Epoch 42, Batch 51, Loss: 0.3818986713886261\n",
      "Epoch 42, Batch 52, Loss: 0.4743550717830658\n",
      "Epoch 42, Batch 53, Loss: 0.426675945520401\n",
      "Epoch 42, Batch 54, Loss: 0.4123237133026123\n",
      "Epoch 42, Batch 55, Loss: 0.41057199239730835\n",
      "Epoch 42, Batch 56, Loss: 0.44105586409568787\n",
      "Epoch 42, Batch 57, Loss: 0.4225311279296875\n",
      "Epoch 42, Batch 58, Loss: 0.41398677229881287\n",
      "Epoch 42, Batch 59, Loss: 0.4286614954471588\n",
      "Epoch 42, Batch 60, Loss: 0.4179827570915222\n",
      "Epoch 42, Batch 61, Loss: 0.44927096366882324\n",
      "Epoch 42, Batch 62, Loss: 0.42268767952919006\n",
      "Epoch 42, Batch 63, Loss: 0.4155207574367523\n",
      "Epoch 42, Batch 64, Loss: 0.41217607259750366\n",
      "Epoch 42, Batch 65, Loss: 0.47773557901382446\n",
      "Epoch 42, Batch 66, Loss: 0.4617564082145691\n",
      "Epoch 42, Batch 67, Loss: 0.38330399990081787\n",
      "Epoch 42, Batch 68, Loss: 0.41911277174949646\n",
      "Epoch 42, Batch 69, Loss: 0.46987614035606384\n",
      "Epoch 42, Batch 70, Loss: 0.4562622606754303\n",
      "Epoch 42, Batch 71, Loss: 0.4119807779788971\n",
      "Epoch 42, Batch 72, Loss: 0.4672125279903412\n",
      "Epoch 42, Batch 73, Loss: 0.4528109133243561\n",
      "Epoch 42, Batch 74, Loss: 0.42769280076026917\n",
      "Epoch 42, Batch 75, Loss: 0.4278712272644043\n",
      "Epoch 42, Batch 76, Loss: 0.4238666594028473\n",
      "Epoch 42, Batch 77, Loss: 0.42070379853248596\n",
      "Epoch 42, Batch 78, Loss: 0.3929729461669922\n",
      "Epoch 42, Batch 79, Loss: 0.4466869831085205\n",
      "Epoch 42, Batch 80, Loss: 0.4595157206058502\n",
      "Epoch 42, Batch 81, Loss: 0.4482640326023102\n",
      "Epoch 42, Batch 82, Loss: 0.5246914625167847\n",
      "Epoch 42, Batch 83, Loss: 0.3834255337715149\n",
      "Epoch 42, Batch 84, Loss: 0.5151839256286621\n",
      "Epoch 42, Batch 85, Loss: 0.4730710983276367\n",
      "Epoch 42, Batch 86, Loss: 0.43220457434654236\n",
      "Epoch 42, Batch 87, Loss: 0.4550175070762634\n",
      "Epoch 42, Batch 88, Loss: 0.43316230177879333\n",
      "Epoch 42, Batch 89, Loss: 0.4172818064689636\n",
      "Epoch 42, Batch 90, Loss: 0.41151708364486694\n",
      "Epoch 42, Batch 91, Loss: 0.4283376634120941\n",
      "Epoch 42, Batch 92, Loss: 0.4513240456581116\n",
      "Epoch 42, Batch 93, Loss: 0.41995441913604736\n",
      "Epoch 43, Batch 0, Loss: 0.46446603536605835\n",
      "Epoch 43, Batch 1, Loss: 0.4556505084037781\n",
      "Epoch 43, Batch 2, Loss: 0.449160635471344\n",
      "Epoch 43, Batch 3, Loss: 0.42536282539367676\n",
      "Epoch 43, Batch 4, Loss: 0.43601375818252563\n",
      "Epoch 43, Batch 5, Loss: 0.46141767501831055\n",
      "Epoch 43, Batch 6, Loss: 0.4445440173149109\n",
      "Epoch 43, Batch 7, Loss: 0.41005387902259827\n",
      "Epoch 43, Batch 8, Loss: 0.4359824061393738\n",
      "Epoch 43, Batch 9, Loss: 0.4469505846500397\n",
      "Epoch 43, Batch 10, Loss: 0.431546688079834\n",
      "Epoch 43, Batch 11, Loss: 0.47729888558387756\n",
      "Epoch 43, Batch 12, Loss: 0.44933000206947327\n",
      "Epoch 43, Batch 13, Loss: 0.40863561630249023\n",
      "Epoch 43, Batch 14, Loss: 0.42654138803482056\n",
      "Epoch 43, Batch 15, Loss: 0.4301307797431946\n",
      "Epoch 43, Batch 16, Loss: 0.41982561349868774\n",
      "Epoch 43, Batch 17, Loss: 0.4333932399749756\n",
      "Epoch 43, Batch 18, Loss: 0.45560798048973083\n",
      "Epoch 43, Batch 19, Loss: 0.40912479162216187\n",
      "Epoch 43, Batch 20, Loss: 0.3854953348636627\n",
      "Epoch 43, Batch 21, Loss: 0.43919411301612854\n",
      "Epoch 43, Batch 22, Loss: 0.41818732023239136\n",
      "Epoch 43, Batch 23, Loss: 0.46017998456954956\n",
      "Epoch 43, Batch 24, Loss: 0.4643875062465668\n",
      "Epoch 43, Batch 25, Loss: 0.40737780928611755\n",
      "Epoch 43, Batch 26, Loss: 0.3699420094490051\n",
      "Epoch 43, Batch 27, Loss: 0.44058364629745483\n",
      "Epoch 43, Batch 28, Loss: 0.4057932496070862\n",
      "Epoch 43, Batch 29, Loss: 0.5027153491973877\n",
      "Epoch 43, Batch 30, Loss: 0.41668516397476196\n",
      "Epoch 43, Batch 31, Loss: 0.47976455092430115\n",
      "Epoch 43, Batch 32, Loss: 0.45335620641708374\n",
      "Epoch 43, Batch 33, Loss: 0.4679774343967438\n",
      "Epoch 43, Batch 34, Loss: 0.45842456817626953\n",
      "Epoch 43, Batch 35, Loss: 0.4855708181858063\n",
      "Epoch 43, Batch 36, Loss: 0.42782798409461975\n",
      "Epoch 43, Batch 37, Loss: 0.43022242188453674\n",
      "Epoch 43, Batch 38, Loss: 0.4680160582065582\n",
      "Epoch 43, Batch 39, Loss: 0.46350932121276855\n",
      "Epoch 43, Batch 40, Loss: 0.39745110273361206\n",
      "Epoch 43, Batch 41, Loss: 0.427316814661026\n",
      "Epoch 43, Batch 42, Loss: 0.44407862424850464\n",
      "Epoch 43, Batch 43, Loss: 0.43676334619522095\n",
      "Epoch 43, Batch 44, Loss: 0.4654797911643982\n",
      "Epoch 43, Batch 45, Loss: 0.45688071846961975\n",
      "Epoch 43, Batch 46, Loss: 0.4541921615600586\n",
      "Epoch 43, Batch 47, Loss: 0.42259377241134644\n",
      "Epoch 43, Batch 48, Loss: 0.4104278087615967\n",
      "Epoch 43, Batch 49, Loss: 0.4344829022884369\n",
      "Epoch 43, Batch 50, Loss: 0.3955506682395935\n",
      "Epoch 43, Batch 51, Loss: 0.410065233707428\n",
      "Epoch 43, Batch 52, Loss: 0.4028701186180115\n",
      "Epoch 43, Batch 53, Loss: 0.4363226890563965\n",
      "Epoch 43, Batch 54, Loss: 0.41481637954711914\n",
      "Epoch 43, Batch 55, Loss: 0.4347367286682129\n",
      "Epoch 43, Batch 56, Loss: 0.4628422260284424\n",
      "Epoch 43, Batch 57, Loss: 0.44829973578453064\n",
      "Epoch 43, Batch 58, Loss: 0.4050779342651367\n",
      "Epoch 43, Batch 59, Loss: 0.451587975025177\n",
      "Epoch 43, Batch 60, Loss: 0.47886696457862854\n",
      "Epoch 43, Batch 61, Loss: 0.444823682308197\n",
      "Epoch 43, Batch 62, Loss: 0.410667359828949\n",
      "Epoch 43, Batch 63, Loss: 0.466071754693985\n",
      "Epoch 43, Batch 64, Loss: 0.45197758078575134\n",
      "Epoch 43, Batch 65, Loss: 0.4205642342567444\n",
      "Epoch 43, Batch 66, Loss: 0.38936278223991394\n",
      "Epoch 43, Batch 67, Loss: 0.43065038323402405\n",
      "Epoch 43, Batch 68, Loss: 0.41702452301979065\n",
      "Epoch 43, Batch 69, Loss: 0.4728003144264221\n",
      "Epoch 43, Batch 70, Loss: 0.4169090688228607\n",
      "Epoch 43, Batch 71, Loss: 0.4683712422847748\n",
      "Epoch 43, Batch 72, Loss: 0.41206973791122437\n",
      "Epoch 43, Batch 73, Loss: 0.43799081444740295\n",
      "Epoch 43, Batch 74, Loss: 0.4359169602394104\n",
      "Epoch 43, Batch 75, Loss: 0.43046218156814575\n",
      "Epoch 43, Batch 76, Loss: 0.40792450308799744\n",
      "Epoch 43, Batch 77, Loss: 0.4150000512599945\n",
      "Epoch 43, Batch 78, Loss: 0.421806663274765\n",
      "Epoch 43, Batch 79, Loss: 0.4191820025444031\n",
      "Epoch 43, Batch 80, Loss: 0.4578682780265808\n",
      "Epoch 43, Batch 81, Loss: 0.42084017395973206\n",
      "Epoch 43, Batch 82, Loss: 0.4535068869590759\n",
      "Epoch 43, Batch 83, Loss: 0.37782591581344604\n",
      "Epoch 43, Batch 84, Loss: 0.42620959877967834\n",
      "Epoch 43, Batch 85, Loss: 0.43758517503738403\n",
      "Epoch 43, Batch 86, Loss: 0.4407723844051361\n",
      "Epoch 43, Batch 87, Loss: 0.41892656683921814\n",
      "Epoch 43, Batch 88, Loss: 0.49038639664649963\n",
      "Epoch 43, Batch 89, Loss: 0.4141760468482971\n",
      "Epoch 43, Batch 90, Loss: 0.4412173330783844\n",
      "Epoch 43, Batch 91, Loss: 0.48257261514663696\n",
      "Epoch 43, Batch 92, Loss: 0.42488622665405273\n",
      "Epoch 43, Batch 93, Loss: 0.3878510594367981\n",
      "Epoch 44, Batch 0, Loss: 0.43687644600868225\n",
      "Epoch 44, Batch 1, Loss: 0.45334261655807495\n",
      "Epoch 44, Batch 2, Loss: 0.40138131380081177\n",
      "Epoch 44, Batch 3, Loss: 0.44757503271102905\n",
      "Epoch 44, Batch 4, Loss: 0.4238835275173187\n",
      "Epoch 44, Batch 5, Loss: 0.5303380489349365\n",
      "Epoch 44, Batch 6, Loss: 0.4111906588077545\n",
      "Epoch 44, Batch 7, Loss: 0.42318612337112427\n",
      "Epoch 44, Batch 8, Loss: 0.43403711915016174\n",
      "Epoch 44, Batch 9, Loss: 0.4603823125362396\n",
      "Epoch 44, Batch 10, Loss: 0.4514375627040863\n",
      "Epoch 44, Batch 11, Loss: 0.3977779746055603\n",
      "Epoch 44, Batch 12, Loss: 0.4333322048187256\n",
      "Epoch 44, Batch 13, Loss: 0.4937990605831146\n",
      "Epoch 44, Batch 14, Loss: 0.4642994999885559\n",
      "Epoch 44, Batch 15, Loss: 0.3965035676956177\n",
      "Epoch 44, Batch 16, Loss: 0.4366505742073059\n",
      "Epoch 44, Batch 17, Loss: 0.4368760585784912\n",
      "Epoch 44, Batch 18, Loss: 0.49984079599380493\n",
      "Epoch 44, Batch 19, Loss: 0.4068555235862732\n",
      "Epoch 44, Batch 20, Loss: 0.4764331877231598\n",
      "Epoch 44, Batch 21, Loss: 0.4220797121524811\n",
      "Epoch 44, Batch 22, Loss: 0.38909024000167847\n",
      "Epoch 44, Batch 23, Loss: 0.3879213035106659\n",
      "Epoch 44, Batch 24, Loss: 0.441288560628891\n",
      "Epoch 44, Batch 25, Loss: 0.4553455412387848\n",
      "Epoch 44, Batch 26, Loss: 0.40826311707496643\n",
      "Epoch 44, Batch 27, Loss: 0.4334673285484314\n",
      "Epoch 44, Batch 28, Loss: 0.460648775100708\n",
      "Epoch 44, Batch 29, Loss: 0.429403692483902\n",
      "Epoch 44, Batch 30, Loss: 0.43992534279823303\n",
      "Epoch 44, Batch 31, Loss: 0.45060181617736816\n",
      "Epoch 44, Batch 32, Loss: 0.3907310366630554\n",
      "Epoch 44, Batch 33, Loss: 0.3847559690475464\n",
      "Epoch 44, Batch 34, Loss: 0.4278081953525543\n",
      "Epoch 44, Batch 35, Loss: 0.39715108275413513\n",
      "Epoch 44, Batch 36, Loss: 0.40980514883995056\n",
      "Epoch 44, Batch 37, Loss: 0.4057645797729492\n",
      "Epoch 44, Batch 38, Loss: 0.42373257875442505\n",
      "Epoch 44, Batch 39, Loss: 0.4062808156013489\n",
      "Epoch 44, Batch 40, Loss: 0.3926009237766266\n",
      "Epoch 44, Batch 41, Loss: 0.42097312211990356\n",
      "Epoch 44, Batch 42, Loss: 0.3956902027130127\n",
      "Epoch 44, Batch 43, Loss: 0.42462968826293945\n",
      "Epoch 44, Batch 44, Loss: 0.4494265019893646\n",
      "Epoch 44, Batch 45, Loss: 0.43919044733047485\n",
      "Epoch 44, Batch 46, Loss: 0.4025716781616211\n",
      "Epoch 44, Batch 47, Loss: 0.4683910310268402\n",
      "Epoch 44, Batch 48, Loss: 0.42655855417251587\n",
      "Epoch 44, Batch 49, Loss: 0.44351381063461304\n",
      "Epoch 44, Batch 50, Loss: 0.4360605776309967\n",
      "Epoch 44, Batch 51, Loss: 0.42967361211776733\n",
      "Epoch 44, Batch 52, Loss: 0.4013996720314026\n",
      "Epoch 44, Batch 53, Loss: 0.44697946310043335\n",
      "Epoch 44, Batch 54, Loss: 0.4467821717262268\n",
      "Epoch 44, Batch 55, Loss: 0.4562721848487854\n",
      "Epoch 44, Batch 56, Loss: 0.4337938725948334\n",
      "Epoch 44, Batch 57, Loss: 0.43732190132141113\n",
      "Epoch 44, Batch 58, Loss: 0.4762211740016937\n",
      "Epoch 44, Batch 59, Loss: 0.46717017889022827\n",
      "Epoch 44, Batch 60, Loss: 0.39619237184524536\n",
      "Epoch 44, Batch 61, Loss: 0.43851223587989807\n",
      "Epoch 44, Batch 62, Loss: 0.43081793189048767\n",
      "Epoch 44, Batch 63, Loss: 0.43398037552833557\n",
      "Epoch 44, Batch 64, Loss: 0.4088214337825775\n",
      "Epoch 44, Batch 65, Loss: 0.3943035304546356\n",
      "Epoch 44, Batch 66, Loss: 0.4459168314933777\n",
      "Epoch 44, Batch 67, Loss: 0.41094428300857544\n",
      "Epoch 44, Batch 68, Loss: 0.4704461097717285\n",
      "Epoch 44, Batch 69, Loss: 0.469912588596344\n",
      "Epoch 44, Batch 70, Loss: 0.4495798945426941\n",
      "Epoch 44, Batch 71, Loss: 0.4290933609008789\n",
      "Epoch 44, Batch 72, Loss: 0.458674818277359\n",
      "Epoch 44, Batch 73, Loss: 0.4797942042350769\n",
      "Epoch 44, Batch 74, Loss: 0.42074888944625854\n",
      "Epoch 44, Batch 75, Loss: 0.4282183051109314\n",
      "Epoch 44, Batch 76, Loss: 0.4233943521976471\n",
      "Epoch 44, Batch 77, Loss: 0.40899381041526794\n",
      "Epoch 44, Batch 78, Loss: 0.43176475167274475\n",
      "Epoch 44, Batch 79, Loss: 0.39273661375045776\n",
      "Epoch 44, Batch 80, Loss: 0.42611607909202576\n",
      "Epoch 44, Batch 81, Loss: 0.400656133890152\n",
      "Epoch 44, Batch 82, Loss: 0.41201597452163696\n",
      "Epoch 44, Batch 83, Loss: 0.45380744338035583\n",
      "Epoch 44, Batch 84, Loss: 0.4383533000946045\n",
      "Epoch 44, Batch 85, Loss: 0.4550052285194397\n",
      "Epoch 44, Batch 86, Loss: 0.4056200087070465\n",
      "Epoch 44, Batch 87, Loss: 0.4110240936279297\n",
      "Epoch 44, Batch 88, Loss: 0.40892115235328674\n",
      "Epoch 44, Batch 89, Loss: 0.4411957859992981\n",
      "Epoch 44, Batch 90, Loss: 0.49477189779281616\n",
      "Epoch 44, Batch 91, Loss: 0.43111318349838257\n",
      "Epoch 44, Batch 92, Loss: 0.3929157853126526\n",
      "Epoch 44, Batch 93, Loss: 0.41424983739852905\n",
      "Epoch 45, Batch 0, Loss: 0.41861504316329956\n",
      "Epoch 45, Batch 1, Loss: 0.40639472007751465\n",
      "Epoch 45, Batch 2, Loss: 0.4229225218296051\n",
      "Epoch 45, Batch 3, Loss: 0.44604891538619995\n",
      "Epoch 45, Batch 4, Loss: 0.4111751616001129\n",
      "Epoch 45, Batch 5, Loss: 0.4207867980003357\n",
      "Epoch 45, Batch 6, Loss: 0.4493563771247864\n",
      "Epoch 45, Batch 7, Loss: 0.37406328320503235\n",
      "Epoch 45, Batch 8, Loss: 0.3869784474372864\n",
      "Epoch 45, Batch 9, Loss: 0.3890737295150757\n",
      "Epoch 45, Batch 10, Loss: 0.37112441658973694\n",
      "Epoch 45, Batch 11, Loss: 0.3722829818725586\n",
      "Epoch 45, Batch 12, Loss: 0.45343726873397827\n",
      "Epoch 45, Batch 13, Loss: 0.3823625445365906\n",
      "Epoch 45, Batch 14, Loss: 0.4126247465610504\n",
      "Epoch 45, Batch 15, Loss: 0.409113347530365\n",
      "Epoch 45, Batch 16, Loss: 0.39563530683517456\n",
      "Epoch 45, Batch 17, Loss: 0.5151221752166748\n",
      "Epoch 45, Batch 18, Loss: 0.3997456431388855\n",
      "Epoch 45, Batch 19, Loss: 0.44526243209838867\n",
      "Epoch 45, Batch 20, Loss: 0.44952598214149475\n",
      "Epoch 45, Batch 21, Loss: 0.4653836786746979\n",
      "Epoch 45, Batch 22, Loss: 0.4018637537956238\n",
      "Epoch 45, Batch 23, Loss: 0.4607127606868744\n",
      "Epoch 45, Batch 24, Loss: 0.4767211973667145\n",
      "Epoch 45, Batch 25, Loss: 0.42883944511413574\n",
      "Epoch 45, Batch 26, Loss: 0.38903507590293884\n",
      "Epoch 45, Batch 27, Loss: 0.3990860879421234\n",
      "Epoch 45, Batch 28, Loss: 0.42952293157577515\n",
      "Epoch 45, Batch 29, Loss: 0.4288223683834076\n",
      "Epoch 45, Batch 30, Loss: 0.47050970792770386\n",
      "Epoch 45, Batch 31, Loss: 0.42183512449264526\n",
      "Epoch 45, Batch 32, Loss: 0.44560784101486206\n",
      "Epoch 45, Batch 33, Loss: 0.4350571036338806\n",
      "Epoch 45, Batch 34, Loss: 0.4461234509944916\n",
      "Epoch 45, Batch 35, Loss: 0.41347306966781616\n",
      "Epoch 45, Batch 36, Loss: 0.48591089248657227\n",
      "Epoch 45, Batch 37, Loss: 0.36221233010292053\n",
      "Epoch 45, Batch 38, Loss: 0.48365044593811035\n",
      "Epoch 45, Batch 39, Loss: 0.4317091405391693\n",
      "Epoch 45, Batch 40, Loss: 0.5052348375320435\n",
      "Epoch 45, Batch 41, Loss: 0.44250673055648804\n",
      "Epoch 45, Batch 42, Loss: 0.37784260511398315\n",
      "Epoch 45, Batch 43, Loss: 0.398219496011734\n",
      "Epoch 45, Batch 44, Loss: 0.48499512672424316\n",
      "Epoch 45, Batch 45, Loss: 0.47734713554382324\n",
      "Epoch 45, Batch 46, Loss: 0.4108806252479553\n",
      "Epoch 45, Batch 47, Loss: 0.42855677008628845\n",
      "Epoch 45, Batch 48, Loss: 0.38228678703308105\n",
      "Epoch 45, Batch 49, Loss: 0.3931499123573303\n",
      "Epoch 45, Batch 50, Loss: 0.42012375593185425\n",
      "Epoch 45, Batch 51, Loss: 0.37732425332069397\n",
      "Epoch 45, Batch 52, Loss: 0.4510592818260193\n",
      "Epoch 45, Batch 53, Loss: 0.4108627736568451\n",
      "Epoch 45, Batch 54, Loss: 0.43718552589416504\n",
      "Epoch 45, Batch 55, Loss: 0.43387970328330994\n",
      "Epoch 45, Batch 56, Loss: 0.4101881980895996\n",
      "Epoch 45, Batch 57, Loss: 0.4623338282108307\n",
      "Epoch 45, Batch 58, Loss: 0.42592254281044006\n",
      "Epoch 45, Batch 59, Loss: 0.4493778347969055\n",
      "Epoch 45, Batch 60, Loss: 0.3943532109260559\n",
      "Epoch 45, Batch 61, Loss: 0.4298042356967926\n",
      "Epoch 45, Batch 62, Loss: 0.42012619972229004\n",
      "Epoch 45, Batch 63, Loss: 0.44430771470069885\n",
      "Epoch 45, Batch 64, Loss: 0.4051273465156555\n",
      "Epoch 45, Batch 65, Loss: 0.4134632647037506\n",
      "Epoch 45, Batch 66, Loss: 0.44482555985450745\n",
      "Epoch 45, Batch 67, Loss: 0.44041723012924194\n",
      "Epoch 45, Batch 68, Loss: 0.4188528060913086\n",
      "Epoch 45, Batch 69, Loss: 0.44004321098327637\n",
      "Epoch 45, Batch 70, Loss: 0.4443811774253845\n",
      "Epoch 45, Batch 71, Loss: 0.42299145460128784\n",
      "Epoch 45, Batch 72, Loss: 0.4613313674926758\n",
      "Epoch 45, Batch 73, Loss: 0.3927757740020752\n",
      "Epoch 45, Batch 74, Loss: 0.4493345618247986\n",
      "Epoch 45, Batch 75, Loss: 0.436434268951416\n",
      "Epoch 45, Batch 76, Loss: 0.4623151421546936\n",
      "Epoch 45, Batch 77, Loss: 0.41149377822875977\n",
      "Epoch 45, Batch 78, Loss: 0.46919840574264526\n",
      "Epoch 45, Batch 79, Loss: 0.4203799366950989\n",
      "Epoch 45, Batch 80, Loss: 0.4101404547691345\n",
      "Epoch 45, Batch 81, Loss: 0.4119550585746765\n",
      "Epoch 45, Batch 82, Loss: 0.4327906668186188\n",
      "Epoch 45, Batch 83, Loss: 0.4403800964355469\n",
      "Epoch 45, Batch 84, Loss: 0.4078610837459564\n",
      "Epoch 45, Batch 85, Loss: 0.38384783267974854\n",
      "Epoch 45, Batch 86, Loss: 0.4382503032684326\n",
      "Epoch 45, Batch 87, Loss: 0.42824944853782654\n",
      "Epoch 45, Batch 88, Loss: 0.4622432291507721\n",
      "Epoch 45, Batch 89, Loss: 0.4279955327510834\n",
      "Epoch 45, Batch 90, Loss: 0.4191877841949463\n",
      "Epoch 45, Batch 91, Loss: 0.5110860466957092\n",
      "Epoch 45, Batch 92, Loss: 0.4454246461391449\n",
      "Epoch 45, Batch 93, Loss: 0.3937639892101288\n",
      "Epoch 46, Batch 0, Loss: 0.4084113538265228\n",
      "Epoch 46, Batch 1, Loss: 0.3988867402076721\n",
      "Epoch 46, Batch 2, Loss: 0.4709419310092926\n",
      "Epoch 46, Batch 3, Loss: 0.3941201865673065\n",
      "Epoch 46, Batch 4, Loss: 0.4319300651550293\n",
      "Epoch 46, Batch 5, Loss: 0.4366673529148102\n",
      "Epoch 46, Batch 6, Loss: 0.43564629554748535\n",
      "Epoch 46, Batch 7, Loss: 0.430593341588974\n",
      "Epoch 46, Batch 8, Loss: 0.3768109381198883\n",
      "Epoch 46, Batch 9, Loss: 0.41840362548828125\n",
      "Epoch 46, Batch 10, Loss: 0.4509292244911194\n",
      "Epoch 46, Batch 11, Loss: 0.3922775387763977\n",
      "Epoch 46, Batch 12, Loss: 0.38929569721221924\n",
      "Epoch 46, Batch 13, Loss: 0.46728014945983887\n",
      "Epoch 46, Batch 14, Loss: 0.4181141257286072\n",
      "Epoch 46, Batch 15, Loss: 0.41966453194618225\n",
      "Epoch 46, Batch 16, Loss: 0.43932467699050903\n",
      "Epoch 46, Batch 17, Loss: 0.4019014239311218\n",
      "Epoch 46, Batch 18, Loss: 0.37275588512420654\n",
      "Epoch 46, Batch 19, Loss: 0.4294505715370178\n",
      "Epoch 46, Batch 20, Loss: 0.42323702573776245\n",
      "Epoch 46, Batch 21, Loss: 0.4699265956878662\n",
      "Epoch 46, Batch 22, Loss: 0.3955453336238861\n",
      "Epoch 46, Batch 23, Loss: 0.4043675363063812\n",
      "Epoch 46, Batch 24, Loss: 0.39814627170562744\n",
      "Epoch 46, Batch 25, Loss: 0.4630686640739441\n",
      "Epoch 46, Batch 26, Loss: 0.40640443563461304\n",
      "Epoch 46, Batch 27, Loss: 0.4397597312927246\n",
      "Epoch 46, Batch 28, Loss: 0.44169139862060547\n",
      "Epoch 46, Batch 29, Loss: 0.42821890115737915\n",
      "Epoch 46, Batch 30, Loss: 0.41964834928512573\n",
      "Epoch 46, Batch 31, Loss: 0.4479965269565582\n",
      "Epoch 46, Batch 32, Loss: 0.38816097378730774\n",
      "Epoch 46, Batch 33, Loss: 0.4425433278083801\n",
      "Epoch 46, Batch 34, Loss: 0.3958521783351898\n",
      "Epoch 46, Batch 35, Loss: 0.4544115662574768\n",
      "Epoch 46, Batch 36, Loss: 0.44333022832870483\n",
      "Epoch 46, Batch 37, Loss: 0.3806907832622528\n",
      "Epoch 46, Batch 38, Loss: 0.40125662088394165\n",
      "Epoch 46, Batch 39, Loss: 0.4278072714805603\n",
      "Epoch 46, Batch 40, Loss: 0.44832974672317505\n",
      "Epoch 46, Batch 41, Loss: 0.40576276183128357\n",
      "Epoch 46, Batch 42, Loss: 0.41287535429000854\n",
      "Epoch 46, Batch 43, Loss: 0.3977780342102051\n",
      "Epoch 46, Batch 44, Loss: 0.4105960428714752\n",
      "Epoch 46, Batch 45, Loss: 0.4122508466243744\n",
      "Epoch 46, Batch 46, Loss: 0.38074442744255066\n",
      "Epoch 46, Batch 47, Loss: 0.47689175605773926\n",
      "Epoch 46, Batch 48, Loss: 0.456346333026886\n",
      "Epoch 46, Batch 49, Loss: 0.3913373649120331\n",
      "Epoch 46, Batch 50, Loss: 0.43773937225341797\n",
      "Epoch 46, Batch 51, Loss: 0.4179856777191162\n",
      "Epoch 46, Batch 52, Loss: 0.4109441637992859\n",
      "Epoch 46, Batch 53, Loss: 0.4712160527706146\n",
      "Epoch 46, Batch 54, Loss: 0.42936253547668457\n",
      "Epoch 46, Batch 55, Loss: 0.41146963834762573\n",
      "Epoch 46, Batch 56, Loss: 0.4507344365119934\n",
      "Epoch 46, Batch 57, Loss: 0.4406617283821106\n",
      "Epoch 46, Batch 58, Loss: 0.48332810401916504\n",
      "Epoch 46, Batch 59, Loss: 0.45090657472610474\n",
      "Epoch 46, Batch 60, Loss: 0.429985374212265\n",
      "Epoch 46, Batch 61, Loss: 0.44780296087265015\n",
      "Epoch 46, Batch 62, Loss: 0.44802823662757874\n",
      "Epoch 46, Batch 63, Loss: 0.42463773488998413\n",
      "Epoch 46, Batch 64, Loss: 0.4356697201728821\n",
      "Epoch 46, Batch 65, Loss: 0.37719839811325073\n",
      "Epoch 46, Batch 66, Loss: 0.41174831986427307\n",
      "Epoch 46, Batch 67, Loss: 0.43591445684432983\n",
      "Epoch 46, Batch 68, Loss: 0.4068484902381897\n",
      "Epoch 46, Batch 69, Loss: 0.4234822392463684\n",
      "Epoch 46, Batch 70, Loss: 0.4071129858493805\n",
      "Epoch 46, Batch 71, Loss: 0.4233308434486389\n",
      "Epoch 46, Batch 72, Loss: 0.46304425597190857\n",
      "Epoch 46, Batch 73, Loss: 0.4168911576271057\n",
      "Epoch 46, Batch 74, Loss: 0.42040902376174927\n",
      "Epoch 46, Batch 75, Loss: 0.44803619384765625\n",
      "Epoch 46, Batch 76, Loss: 0.42973941564559937\n",
      "Epoch 46, Batch 77, Loss: 0.41002362966537476\n",
      "Epoch 46, Batch 78, Loss: 0.4520799219608307\n",
      "Epoch 46, Batch 79, Loss: 0.4238317012786865\n",
      "Epoch 46, Batch 80, Loss: 0.3915826082229614\n",
      "Epoch 46, Batch 81, Loss: 0.43199867010116577\n",
      "Epoch 46, Batch 82, Loss: 0.42457836866378784\n",
      "Epoch 46, Batch 83, Loss: 0.4397384226322174\n",
      "Epoch 46, Batch 84, Loss: 0.42307549715042114\n",
      "Epoch 46, Batch 85, Loss: 0.40621018409729004\n",
      "Epoch 46, Batch 86, Loss: 0.4183756411075592\n",
      "Epoch 46, Batch 87, Loss: 0.42850223183631897\n",
      "Epoch 46, Batch 88, Loss: 0.43510183691978455\n",
      "Epoch 46, Batch 89, Loss: 0.39617639780044556\n",
      "Epoch 46, Batch 90, Loss: 0.42359432578086853\n",
      "Epoch 46, Batch 91, Loss: 0.41682106256484985\n",
      "Epoch 46, Batch 92, Loss: 0.4327453076839447\n",
      "Epoch 46, Batch 93, Loss: 0.43240195512771606\n",
      "Epoch 47, Batch 0, Loss: 0.4418836236000061\n",
      "Epoch 47, Batch 1, Loss: 0.4110759198665619\n",
      "Epoch 47, Batch 2, Loss: 0.4434238374233246\n",
      "Epoch 47, Batch 3, Loss: 0.44813814759254456\n",
      "Epoch 47, Batch 4, Loss: 0.42898836731910706\n",
      "Epoch 47, Batch 5, Loss: 0.42243605852127075\n",
      "Epoch 47, Batch 6, Loss: 0.3842218816280365\n",
      "Epoch 47, Batch 7, Loss: 0.4291977286338806\n",
      "Epoch 47, Batch 8, Loss: 0.4184266924858093\n",
      "Epoch 47, Batch 9, Loss: 0.4419960379600525\n",
      "Epoch 47, Batch 10, Loss: 0.45867210626602173\n",
      "Epoch 47, Batch 11, Loss: 0.41220253705978394\n",
      "Epoch 47, Batch 12, Loss: 0.4215311110019684\n",
      "Epoch 47, Batch 13, Loss: 0.4453308582305908\n",
      "Epoch 47, Batch 14, Loss: 0.390320360660553\n",
      "Epoch 47, Batch 15, Loss: 0.42025184631347656\n",
      "Epoch 47, Batch 16, Loss: 0.4105662405490875\n",
      "Epoch 47, Batch 17, Loss: 0.4204622805118561\n",
      "Epoch 47, Batch 18, Loss: 0.41223374009132385\n",
      "Epoch 47, Batch 19, Loss: 0.37799975275993347\n",
      "Epoch 47, Batch 20, Loss: 0.45455026626586914\n",
      "Epoch 47, Batch 21, Loss: 0.43672341108322144\n",
      "Epoch 47, Batch 22, Loss: 0.4566133916378021\n",
      "Epoch 47, Batch 23, Loss: 0.4004666805267334\n",
      "Epoch 47, Batch 24, Loss: 0.4357239603996277\n",
      "Epoch 47, Batch 25, Loss: 0.39004984498023987\n",
      "Epoch 47, Batch 26, Loss: 0.46559005975723267\n",
      "Epoch 47, Batch 27, Loss: 0.4550101161003113\n",
      "Epoch 47, Batch 28, Loss: 0.4481584131717682\n",
      "Epoch 47, Batch 29, Loss: 0.43203505873680115\n",
      "Epoch 47, Batch 30, Loss: 0.4337173402309418\n",
      "Epoch 47, Batch 31, Loss: 0.39014559984207153\n",
      "Epoch 47, Batch 32, Loss: 0.4106995463371277\n",
      "Epoch 47, Batch 33, Loss: 0.39891666173934937\n",
      "Epoch 47, Batch 34, Loss: 0.3855345845222473\n",
      "Epoch 47, Batch 35, Loss: 0.42936939001083374\n",
      "Epoch 47, Batch 36, Loss: 0.4243175983428955\n",
      "Epoch 47, Batch 37, Loss: 0.38609084486961365\n",
      "Epoch 47, Batch 38, Loss: 0.3785262703895569\n",
      "Epoch 47, Batch 39, Loss: 0.40278419852256775\n",
      "Epoch 47, Batch 40, Loss: 0.4258899688720703\n",
      "Epoch 47, Batch 41, Loss: 0.4035886824131012\n",
      "Epoch 47, Batch 42, Loss: 0.45565932989120483\n",
      "Epoch 47, Batch 43, Loss: 0.39645442366600037\n",
      "Epoch 47, Batch 44, Loss: 0.42373523116111755\n",
      "Epoch 47, Batch 45, Loss: 0.430431604385376\n",
      "Epoch 47, Batch 46, Loss: 0.4390620291233063\n",
      "Epoch 47, Batch 47, Loss: 0.4167723059654236\n",
      "Epoch 47, Batch 48, Loss: 0.4250783324241638\n",
      "Epoch 47, Batch 49, Loss: 0.39391225576400757\n",
      "Epoch 47, Batch 50, Loss: 0.4139372408390045\n",
      "Epoch 47, Batch 51, Loss: 0.4032064974308014\n",
      "Epoch 47, Batch 52, Loss: 0.4224907457828522\n",
      "Epoch 47, Batch 53, Loss: 0.459400475025177\n",
      "Epoch 47, Batch 54, Loss: 0.4577866494655609\n",
      "Epoch 47, Batch 55, Loss: 0.4913218021392822\n",
      "Epoch 47, Batch 56, Loss: 0.41019439697265625\n",
      "Epoch 47, Batch 57, Loss: 0.4286057949066162\n",
      "Epoch 47, Batch 58, Loss: 0.4685734808444977\n",
      "Epoch 47, Batch 59, Loss: 0.3812895119190216\n",
      "Epoch 47, Batch 60, Loss: 0.4053010940551758\n",
      "Epoch 47, Batch 61, Loss: 0.3760797381401062\n",
      "Epoch 47, Batch 62, Loss: 0.42138832807540894\n",
      "Epoch 47, Batch 63, Loss: 0.4093759059906006\n",
      "Epoch 47, Batch 64, Loss: 0.44754964113235474\n",
      "Epoch 47, Batch 65, Loss: 0.40054425597190857\n",
      "Epoch 47, Batch 66, Loss: 0.4101862907409668\n",
      "Epoch 47, Batch 67, Loss: 0.4153706133365631\n",
      "Epoch 47, Batch 68, Loss: 0.4214242398738861\n",
      "Epoch 47, Batch 69, Loss: 0.3649391531944275\n",
      "Epoch 47, Batch 70, Loss: 0.44784656167030334\n",
      "Epoch 47, Batch 71, Loss: 0.3810957372188568\n",
      "Epoch 47, Batch 72, Loss: 0.407744824886322\n",
      "Epoch 47, Batch 73, Loss: 0.40538978576660156\n",
      "Epoch 47, Batch 74, Loss: 0.45488110184669495\n",
      "Epoch 47, Batch 75, Loss: 0.4449058175086975\n",
      "Epoch 47, Batch 76, Loss: 0.47191762924194336\n",
      "Epoch 47, Batch 77, Loss: 0.4098241925239563\n",
      "Epoch 47, Batch 78, Loss: 0.4184374213218689\n",
      "Epoch 47, Batch 79, Loss: 0.41702860593795776\n",
      "Epoch 47, Batch 80, Loss: 0.3908613324165344\n",
      "Epoch 47, Batch 81, Loss: 0.35169699788093567\n",
      "Epoch 47, Batch 82, Loss: 0.45167016983032227\n",
      "Epoch 47, Batch 83, Loss: 0.4323231279850006\n",
      "Epoch 47, Batch 84, Loss: 0.3791877031326294\n",
      "Epoch 47, Batch 85, Loss: 0.4477897584438324\n",
      "Epoch 47, Batch 86, Loss: 0.4407818913459778\n",
      "Epoch 47, Batch 87, Loss: 0.44756847620010376\n",
      "Epoch 47, Batch 88, Loss: 0.4088258743286133\n",
      "Epoch 47, Batch 89, Loss: 0.39048564434051514\n",
      "Epoch 47, Batch 90, Loss: 0.4302009642124176\n",
      "Epoch 47, Batch 91, Loss: 0.431549608707428\n",
      "Epoch 47, Batch 92, Loss: 0.3817301094532013\n",
      "Epoch 47, Batch 93, Loss: 0.4477435052394867\n",
      "Epoch 48, Batch 0, Loss: 0.3606882095336914\n",
      "Epoch 48, Batch 1, Loss: 0.3766863942146301\n",
      "Epoch 48, Batch 2, Loss: 0.4944082796573639\n",
      "Epoch 48, Batch 3, Loss: 0.43076127767562866\n",
      "Epoch 48, Batch 4, Loss: 0.44859424233436584\n",
      "Epoch 48, Batch 5, Loss: 0.39993852376937866\n",
      "Epoch 48, Batch 6, Loss: 0.4234078526496887\n",
      "Epoch 48, Batch 7, Loss: 0.42103129625320435\n",
      "Epoch 48, Batch 8, Loss: 0.3945865035057068\n",
      "Epoch 48, Batch 9, Loss: 0.40123453736305237\n",
      "Epoch 48, Batch 10, Loss: 0.4094253480434418\n",
      "Epoch 48, Batch 11, Loss: 0.40916919708251953\n",
      "Epoch 48, Batch 12, Loss: 0.4324134290218353\n",
      "Epoch 48, Batch 13, Loss: 0.3974444270133972\n",
      "Epoch 48, Batch 14, Loss: 0.39171677827835083\n",
      "Epoch 48, Batch 15, Loss: 0.38475778698921204\n",
      "Epoch 48, Batch 16, Loss: 0.3999149203300476\n",
      "Epoch 48, Batch 17, Loss: 0.4003831744194031\n",
      "Epoch 48, Batch 18, Loss: 0.4263193607330322\n",
      "Epoch 48, Batch 19, Loss: 0.43550801277160645\n",
      "Epoch 48, Batch 20, Loss: 0.43613147735595703\n",
      "Epoch 48, Batch 21, Loss: 0.4542560577392578\n",
      "Epoch 48, Batch 22, Loss: 0.45588675141334534\n",
      "Epoch 48, Batch 23, Loss: 0.4259568154811859\n",
      "Epoch 48, Batch 24, Loss: 0.38811588287353516\n",
      "Epoch 48, Batch 25, Loss: 0.4016154408454895\n",
      "Epoch 48, Batch 26, Loss: 0.39155495166778564\n",
      "Epoch 48, Batch 27, Loss: 0.41709470748901367\n",
      "Epoch 48, Batch 28, Loss: 0.44012951850891113\n",
      "Epoch 48, Batch 29, Loss: 0.4443918764591217\n",
      "Epoch 48, Batch 30, Loss: 0.40490832924842834\n",
      "Epoch 48, Batch 31, Loss: 0.46027547121047974\n",
      "Epoch 48, Batch 32, Loss: 0.4061151146888733\n",
      "Epoch 48, Batch 33, Loss: 0.4283425807952881\n",
      "Epoch 48, Batch 34, Loss: 0.45153316855430603\n",
      "Epoch 48, Batch 35, Loss: 0.3993934690952301\n",
      "Epoch 48, Batch 36, Loss: 0.4186781942844391\n",
      "Epoch 48, Batch 37, Loss: 0.394584983587265\n",
      "Epoch 48, Batch 38, Loss: 0.4449590742588043\n",
      "Epoch 48, Batch 39, Loss: 0.41748300194740295\n",
      "Epoch 48, Batch 40, Loss: 0.4262339472770691\n",
      "Epoch 48, Batch 41, Loss: 0.38893699645996094\n",
      "Epoch 48, Batch 42, Loss: 0.43693703413009644\n",
      "Epoch 48, Batch 43, Loss: 0.3650309443473816\n",
      "Epoch 48, Batch 44, Loss: 0.42429789900779724\n",
      "Epoch 48, Batch 45, Loss: 0.4430173337459564\n",
      "Epoch 48, Batch 46, Loss: 0.435312420129776\n",
      "Epoch 48, Batch 47, Loss: 0.37072232365608215\n",
      "Epoch 48, Batch 48, Loss: 0.4422689378261566\n",
      "Epoch 48, Batch 49, Loss: 0.43614253401756287\n",
      "Epoch 48, Batch 50, Loss: 0.4753471910953522\n",
      "Epoch 48, Batch 51, Loss: 0.44854217767715454\n",
      "Epoch 48, Batch 52, Loss: 0.4639190137386322\n",
      "Epoch 48, Batch 53, Loss: 0.4276188313961029\n",
      "Epoch 48, Batch 54, Loss: 0.4185141921043396\n",
      "Epoch 48, Batch 55, Loss: 0.4310912489891052\n",
      "Epoch 48, Batch 56, Loss: 0.42914527654647827\n",
      "Epoch 48, Batch 57, Loss: 0.39644938707351685\n",
      "Epoch 48, Batch 58, Loss: 0.41375842690467834\n",
      "Epoch 48, Batch 59, Loss: 0.3906816840171814\n",
      "Epoch 48, Batch 60, Loss: 0.40459775924682617\n",
      "Epoch 48, Batch 61, Loss: 0.4294455051422119\n",
      "Epoch 48, Batch 62, Loss: 0.3832027316093445\n",
      "Epoch 48, Batch 63, Loss: 0.4017784595489502\n",
      "Epoch 48, Batch 64, Loss: 0.4021020531654358\n",
      "Epoch 48, Batch 65, Loss: 0.4138353765010834\n",
      "Epoch 48, Batch 66, Loss: 0.40082621574401855\n",
      "Epoch 48, Batch 67, Loss: 0.3581089377403259\n",
      "Epoch 48, Batch 68, Loss: 0.4359259009361267\n",
      "Epoch 48, Batch 69, Loss: 0.4174235463142395\n",
      "Epoch 48, Batch 70, Loss: 0.48582449555397034\n",
      "Epoch 48, Batch 71, Loss: 0.3915471136569977\n",
      "Epoch 48, Batch 72, Loss: 0.45529240369796753\n",
      "Epoch 48, Batch 73, Loss: 0.4499741196632385\n",
      "Epoch 48, Batch 74, Loss: 0.4363548755645752\n",
      "Epoch 48, Batch 75, Loss: 0.4321342408657074\n",
      "Epoch 48, Batch 76, Loss: 0.3925810754299164\n",
      "Epoch 48, Batch 77, Loss: 0.3723967969417572\n",
      "Epoch 48, Batch 78, Loss: 0.4130595326423645\n",
      "Epoch 48, Batch 79, Loss: 0.41531506180763245\n",
      "Epoch 48, Batch 80, Loss: 0.4158746302127838\n",
      "Epoch 48, Batch 81, Loss: 0.4149347245693207\n",
      "Epoch 48, Batch 82, Loss: 0.4046791195869446\n",
      "Epoch 48, Batch 83, Loss: 0.42508116364479065\n",
      "Epoch 48, Batch 84, Loss: 0.4213330149650574\n",
      "Epoch 48, Batch 85, Loss: 0.4259938597679138\n",
      "Epoch 48, Batch 86, Loss: 0.39636313915252686\n",
      "Epoch 48, Batch 87, Loss: 0.3927364945411682\n",
      "Epoch 48, Batch 88, Loss: 0.4672977328300476\n",
      "Epoch 48, Batch 89, Loss: 0.365098774433136\n",
      "Epoch 48, Batch 90, Loss: 0.3928352892398834\n",
      "Epoch 48, Batch 91, Loss: 0.44789329171180725\n",
      "Epoch 48, Batch 92, Loss: 0.393114298582077\n",
      "Epoch 48, Batch 93, Loss: 0.40041667222976685\n",
      "Epoch 49, Batch 0, Loss: 0.4464488625526428\n",
      "Epoch 49, Batch 1, Loss: 0.41450318694114685\n",
      "Epoch 49, Batch 2, Loss: 0.37555912137031555\n",
      "Epoch 49, Batch 3, Loss: 0.39391177892684937\n",
      "Epoch 49, Batch 4, Loss: 0.3981470465660095\n",
      "Epoch 49, Batch 5, Loss: 0.41262882947921753\n",
      "Epoch 49, Batch 6, Loss: 0.37545138597488403\n",
      "Epoch 49, Batch 7, Loss: 0.3984030783176422\n",
      "Epoch 49, Batch 8, Loss: 0.35987046360969543\n",
      "Epoch 49, Batch 9, Loss: 0.4548124372959137\n",
      "Epoch 49, Batch 10, Loss: 0.3970164656639099\n",
      "Epoch 49, Batch 11, Loss: 0.3840978443622589\n",
      "Epoch 49, Batch 12, Loss: 0.4015883505344391\n",
      "Epoch 49, Batch 13, Loss: 0.398188054561615\n",
      "Epoch 49, Batch 14, Loss: 0.41555270552635193\n",
      "Epoch 49, Batch 15, Loss: 0.44587182998657227\n",
      "Epoch 49, Batch 16, Loss: 0.4205452799797058\n",
      "Epoch 49, Batch 17, Loss: 0.4141036570072174\n",
      "Epoch 49, Batch 18, Loss: 0.38241511583328247\n",
      "Epoch 49, Batch 19, Loss: 0.3728766143321991\n",
      "Epoch 49, Batch 20, Loss: 0.42212527990341187\n",
      "Epoch 49, Batch 21, Loss: 0.48565417528152466\n",
      "Epoch 49, Batch 22, Loss: 0.42558175325393677\n",
      "Epoch 49, Batch 23, Loss: 0.414965957403183\n",
      "Epoch 49, Batch 24, Loss: 0.3550490736961365\n",
      "Epoch 49, Batch 25, Loss: 0.46086302399635315\n",
      "Epoch 49, Batch 26, Loss: 0.4065190255641937\n",
      "Epoch 49, Batch 27, Loss: 0.40963441133499146\n",
      "Epoch 49, Batch 28, Loss: 0.47613874077796936\n",
      "Epoch 49, Batch 29, Loss: 0.38973039388656616\n",
      "Epoch 49, Batch 30, Loss: 0.39172783493995667\n",
      "Epoch 49, Batch 31, Loss: 0.4425126016139984\n",
      "Epoch 49, Batch 32, Loss: 0.42803439497947693\n",
      "Epoch 49, Batch 33, Loss: 0.4241566061973572\n",
      "Epoch 49, Batch 34, Loss: 0.4349497854709625\n",
      "Epoch 49, Batch 35, Loss: 0.42653903365135193\n",
      "Epoch 49, Batch 36, Loss: 0.3864994943141937\n",
      "Epoch 49, Batch 37, Loss: 0.43122321367263794\n",
      "Epoch 49, Batch 38, Loss: 0.4083077311515808\n",
      "Epoch 49, Batch 39, Loss: 0.4759928584098816\n",
      "Epoch 49, Batch 40, Loss: 0.3820343613624573\n",
      "Epoch 49, Batch 41, Loss: 0.3937709629535675\n",
      "Epoch 49, Batch 42, Loss: 0.3935360908508301\n",
      "Epoch 49, Batch 43, Loss: 0.41469860076904297\n",
      "Epoch 49, Batch 44, Loss: 0.4630827009677887\n",
      "Epoch 49, Batch 45, Loss: 0.4494269788265228\n",
      "Epoch 49, Batch 46, Loss: 0.4287954270839691\n",
      "Epoch 49, Batch 47, Loss: 0.40172338485717773\n",
      "Epoch 49, Batch 48, Loss: 0.4325319826602936\n",
      "Epoch 49, Batch 49, Loss: 0.40707889199256897\n",
      "Epoch 49, Batch 50, Loss: 0.40422797203063965\n",
      "Epoch 49, Batch 51, Loss: 0.4125641882419586\n",
      "Epoch 49, Batch 52, Loss: 0.43321529030799866\n",
      "Epoch 49, Batch 53, Loss: 0.418191522359848\n",
      "Epoch 49, Batch 54, Loss: 0.3887747526168823\n",
      "Epoch 49, Batch 55, Loss: 0.4451979696750641\n",
      "Epoch 49, Batch 56, Loss: 0.384392112493515\n",
      "Epoch 49, Batch 57, Loss: 0.4085243344306946\n",
      "Epoch 49, Batch 58, Loss: 0.4096032679080963\n",
      "Epoch 49, Batch 59, Loss: 0.3901897072792053\n",
      "Epoch 49, Batch 60, Loss: 0.3978533148765564\n",
      "Epoch 49, Batch 61, Loss: 0.4141807556152344\n",
      "Epoch 49, Batch 62, Loss: 0.40498262643814087\n",
      "Epoch 49, Batch 63, Loss: 0.38396474719047546\n",
      "Epoch 49, Batch 64, Loss: 0.4014893174171448\n",
      "Epoch 49, Batch 65, Loss: 0.4692501127719879\n",
      "Epoch 49, Batch 66, Loss: 0.4038407802581787\n",
      "Epoch 49, Batch 67, Loss: 0.3888636827468872\n",
      "Epoch 49, Batch 68, Loss: 0.39605972170829773\n",
      "Epoch 49, Batch 69, Loss: 0.47453364729881287\n",
      "Epoch 49, Batch 70, Loss: 0.3969476819038391\n",
      "Epoch 49, Batch 71, Loss: 0.4055916666984558\n",
      "Epoch 49, Batch 72, Loss: 0.371784508228302\n",
      "Epoch 49, Batch 73, Loss: 0.45721083879470825\n",
      "Epoch 49, Batch 74, Loss: 0.40040454268455505\n",
      "Epoch 49, Batch 75, Loss: 0.44365936517715454\n",
      "Epoch 49, Batch 76, Loss: 0.4485376477241516\n",
      "Epoch 49, Batch 77, Loss: 0.37977316975593567\n",
      "Epoch 49, Batch 78, Loss: 0.4336802065372467\n",
      "Epoch 49, Batch 79, Loss: 0.3627532124519348\n",
      "Epoch 49, Batch 80, Loss: 0.43387651443481445\n",
      "Epoch 49, Batch 81, Loss: 0.44997653365135193\n",
      "Epoch 49, Batch 82, Loss: 0.42844724655151367\n",
      "Epoch 49, Batch 83, Loss: 0.4626144766807556\n",
      "Epoch 49, Batch 84, Loss: 0.41925138235092163\n",
      "Epoch 49, Batch 85, Loss: 0.40469369292259216\n",
      "Epoch 49, Batch 86, Loss: 0.43763574957847595\n",
      "Epoch 49, Batch 87, Loss: 0.45838528871536255\n",
      "Epoch 49, Batch 88, Loss: 0.39888015389442444\n",
      "Epoch 49, Batch 89, Loss: 0.36762145161628723\n",
      "Epoch 49, Batch 90, Loss: 0.38044363260269165\n",
      "Epoch 49, Batch 91, Loss: 0.40622735023498535\n",
      "Epoch 49, Batch 92, Loss: 0.38598090410232544\n",
      "Epoch 49, Batch 93, Loss: 0.46220114827156067\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.functional.nll_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "for current_epoch in range(n_epoch):\n",
    "  for batch_idx, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(batch_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, batch_labels)\n",
    "    train_losses.append(loss.item())\n",
    "    print(f\"Epoch {current_epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a4168150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_losses, color='blue')\n",
    "plt.legend(['Train Loss'], loc='upper right')\n",
    "plt.xlabel('batch count')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3b7e21bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.3966, Accuracy: 8968/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.cpu()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "  for data, target in test_loader:\n",
    "    output = model(data)\n",
    "    test_loss += torch.nn.functional.nll_loss(output, target, size_average=False).item()\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

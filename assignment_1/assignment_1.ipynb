{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1663cf1",
   "metadata": {},
   "source": [
    "## Assignment 1 - Mnist with a simple (one hidden layer) Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d388c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cpu\") #Use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #If nvidea GPU is available use GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de9d27",
   "metadata": {},
   "source": [
    "Creating the nn.module for a neural network with two layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d56704ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "    \"\"\"\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    self.D_out = D_out\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    In the forward function we accept a Tensor of input data and we must return\n",
    "    a Tensor of output data. We can use Modules defined in the constructor as\n",
    "    well as arbitrary (differentiable) operations on Tensors.\n",
    "    Uses log_softmax\n",
    "    \"\"\"\n",
    "    # flatten the tensor\n",
    "    x = x.view(x.size(0), -1)\n",
    "    h_relu = self.linear1(x).clamp(min=0)\n",
    "    y_pred = self.linear2(h_relu)\n",
    "    return torch.nn.functional.log_softmax(y_pred, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323d337",
   "metadata": {},
   "source": [
    "Setup vars that impact the time and effectivness of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aed69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64*10\n",
    "batch_size_test = 1000\n",
    "learning_rate = 1e-3\n",
    "n_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d0f37",
   "metadata": {},
   "source": [
    "Loading the mnist-dataset, pictures of handwritten numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aefdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "  transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "      (0.1307,), (0.3081,)),\n",
    "  ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e09b7c",
   "metadata": {},
   "source": [
    "Precomputing the *torchvision.transforms* and loading the **complete** dataset on the GPU. This is not nessesary and in some cases a stupid idea, but in this case it causes a huge speedup. This is why we decided to keep it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02867b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "for img, label in train_mnist:\n",
    "    train_data.append(img)\n",
    "    train_labels.append(label)\n",
    "\n",
    "train_data = torch.stack(train_data)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torch.utils.data.TensorDataset(train_data, train_labels),\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73a6ed",
   "metadata": {},
   "source": [
    "Loading the test-dataset. Here we don't precompute and don't transfer the data to the GPU because we will only use this data a few times. The train data will be used in multiple epochs so we get much more use out of the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "289afc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "      torchvision.transforms.ToTensor(),\n",
    "      torchvision.transforms.Normalize(\n",
    "        (0.1307,), (0.3081,))\n",
    "    ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eaf22",
   "metadata": {},
   "source": [
    "Setting up the vars for the network. \n",
    "\n",
    "The hidden layer is 100 per assignment. \n",
    "\n",
    "The  output needs to be 10 for numbers 0-9.\n",
    "\n",
    "And finally the input is 784=28*28 for the gray-value each pixel of a 28x28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "536661d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, H, D_out = 784, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d2f8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model = model.to(device)\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82286e",
   "metadata": {},
   "source": [
    "Training the model using nn.functional.nll_loss as loss and optim.SGD as optimizer \n",
    "\n",
    "Time \n",
    "cpu(i5-9400F): 53.1s\n",
    "\n",
    "gpu(4060 Ti): 39.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "460a996f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 2.3584201335906982\n",
      "Epoch 0, Batch 1, Loss: 2.348037004470825\n",
      "Epoch 0, Batch 2, Loss: 2.3575196266174316\n",
      "Epoch 0, Batch 3, Loss: 2.3530869483947754\n",
      "Epoch 0, Batch 4, Loss: 2.34489369392395\n",
      "Epoch 0, Batch 5, Loss: 2.349109411239624\n",
      "Epoch 0, Batch 6, Loss: 2.344327449798584\n",
      "Epoch 0, Batch 7, Loss: 2.342656135559082\n",
      "Epoch 0, Batch 8, Loss: 2.34576416015625\n",
      "Epoch 0, Batch 9, Loss: 2.3485403060913086\n",
      "Epoch 0, Batch 10, Loss: 2.3382277488708496\n",
      "Epoch 0, Batch 11, Loss: 2.3534862995147705\n",
      "Epoch 0, Batch 12, Loss: 2.3262929916381836\n",
      "Epoch 0, Batch 13, Loss: 2.3463969230651855\n",
      "Epoch 0, Batch 14, Loss: 2.331749439239502\n",
      "Epoch 0, Batch 15, Loss: 2.3243298530578613\n",
      "Epoch 0, Batch 16, Loss: 2.322272777557373\n",
      "Epoch 0, Batch 17, Loss: 2.3208365440368652\n",
      "Epoch 0, Batch 18, Loss: 2.321154832839966\n",
      "Epoch 0, Batch 19, Loss: 2.332862138748169\n",
      "Epoch 0, Batch 20, Loss: 2.3274924755096436\n",
      "Epoch 0, Batch 21, Loss: 2.3237709999084473\n",
      "Epoch 0, Batch 22, Loss: 2.316664218902588\n",
      "Epoch 0, Batch 23, Loss: 2.329336643218994\n",
      "Epoch 0, Batch 24, Loss: 2.332667827606201\n",
      "Epoch 0, Batch 25, Loss: 2.325004816055298\n",
      "Epoch 0, Batch 26, Loss: 2.311689853668213\n",
      "Epoch 0, Batch 27, Loss: 2.300266981124878\n",
      "Epoch 0, Batch 28, Loss: 2.291560411453247\n",
      "Epoch 0, Batch 29, Loss: 2.3128676414489746\n",
      "Epoch 0, Batch 30, Loss: 2.31459641456604\n",
      "Epoch 0, Batch 31, Loss: 2.3080530166625977\n",
      "Epoch 0, Batch 32, Loss: 2.3130581378936768\n",
      "Epoch 0, Batch 33, Loss: 2.3095645904541016\n",
      "Epoch 0, Batch 34, Loss: 2.3079726696014404\n",
      "Epoch 0, Batch 35, Loss: 2.319784641265869\n",
      "Epoch 0, Batch 36, Loss: 2.29127836227417\n",
      "Epoch 0, Batch 37, Loss: 2.2779407501220703\n",
      "Epoch 0, Batch 38, Loss: 2.3057374954223633\n",
      "Epoch 0, Batch 39, Loss: 2.2997190952301025\n",
      "Epoch 0, Batch 40, Loss: 2.2905802726745605\n",
      "Epoch 0, Batch 41, Loss: 2.290199041366577\n",
      "Epoch 0, Batch 42, Loss: 2.278937578201294\n",
      "Epoch 0, Batch 43, Loss: 2.2881197929382324\n",
      "Epoch 0, Batch 44, Loss: 2.2697715759277344\n",
      "Epoch 0, Batch 45, Loss: 2.267141819000244\n",
      "Epoch 0, Batch 46, Loss: 2.276001453399658\n",
      "Epoch 0, Batch 47, Loss: 2.282860279083252\n",
      "Epoch 0, Batch 48, Loss: 2.2893002033233643\n",
      "Epoch 0, Batch 49, Loss: 2.2809805870056152\n",
      "Epoch 0, Batch 50, Loss: 2.264237403869629\n",
      "Epoch 0, Batch 51, Loss: 2.2719249725341797\n",
      "Epoch 0, Batch 52, Loss: 2.271998167037964\n",
      "Epoch 0, Batch 53, Loss: 2.273024320602417\n",
      "Epoch 0, Batch 54, Loss: 2.279810667037964\n",
      "Epoch 0, Batch 55, Loss: 2.2558205127716064\n",
      "Epoch 0, Batch 56, Loss: 2.2642886638641357\n",
      "Epoch 0, Batch 57, Loss: 2.2749483585357666\n",
      "Epoch 0, Batch 58, Loss: 2.2735352516174316\n",
      "Epoch 0, Batch 59, Loss: 2.2578957080841064\n",
      "Epoch 0, Batch 60, Loss: 2.2516777515411377\n",
      "Epoch 0, Batch 61, Loss: 2.25607967376709\n",
      "Epoch 0, Batch 62, Loss: 2.250805377960205\n",
      "Epoch 0, Batch 63, Loss: 2.2540502548217773\n",
      "Epoch 0, Batch 64, Loss: 2.2698235511779785\n",
      "Epoch 0, Batch 65, Loss: 2.247406482696533\n",
      "Epoch 0, Batch 66, Loss: 2.237511157989502\n",
      "Epoch 0, Batch 67, Loss: 2.246039867401123\n",
      "Epoch 0, Batch 68, Loss: 2.2487642765045166\n",
      "Epoch 0, Batch 69, Loss: 2.265526294708252\n",
      "Epoch 0, Batch 70, Loss: 2.2692716121673584\n",
      "Epoch 0, Batch 71, Loss: 2.2449300289154053\n",
      "Epoch 0, Batch 72, Loss: 2.241630792617798\n",
      "Epoch 0, Batch 73, Loss: 2.224090337753296\n",
      "Epoch 0, Batch 74, Loss: 2.2561609745025635\n",
      "Epoch 0, Batch 75, Loss: 2.2431862354278564\n",
      "Epoch 0, Batch 76, Loss: 2.245059013366699\n",
      "Epoch 0, Batch 77, Loss: 2.2345633506774902\n",
      "Epoch 0, Batch 78, Loss: 2.2277441024780273\n",
      "Epoch 0, Batch 79, Loss: 2.233241319656372\n",
      "Epoch 0, Batch 80, Loss: 2.227106809616089\n",
      "Epoch 0, Batch 81, Loss: 2.2263617515563965\n",
      "Epoch 0, Batch 82, Loss: 2.246330738067627\n",
      "Epoch 0, Batch 83, Loss: 2.2408053874969482\n",
      "Epoch 0, Batch 84, Loss: 2.230090618133545\n",
      "Epoch 0, Batch 85, Loss: 2.2185473442077637\n",
      "Epoch 0, Batch 86, Loss: 2.229954719543457\n",
      "Epoch 0, Batch 87, Loss: 2.2079005241394043\n",
      "Epoch 0, Batch 88, Loss: 2.227186679840088\n",
      "Epoch 0, Batch 89, Loss: 2.2198331356048584\n",
      "Epoch 0, Batch 90, Loss: 2.2207272052764893\n",
      "Epoch 0, Batch 91, Loss: 2.2122130393981934\n",
      "Epoch 0, Batch 92, Loss: 2.21356201171875\n",
      "Epoch 0, Batch 93, Loss: 2.20756196975708\n",
      "Epoch 1, Batch 0, Loss: 2.20998215675354\n",
      "Epoch 1, Batch 1, Loss: 2.2047536373138428\n",
      "Epoch 1, Batch 2, Loss: 2.2167179584503174\n",
      "Epoch 1, Batch 3, Loss: 2.219658374786377\n",
      "Epoch 1, Batch 4, Loss: 2.2057838439941406\n",
      "Epoch 1, Batch 5, Loss: 2.2125229835510254\n",
      "Epoch 1, Batch 6, Loss: 2.2017836570739746\n",
      "Epoch 1, Batch 7, Loss: 2.20451283454895\n",
      "Epoch 1, Batch 8, Loss: 2.190438747406006\n",
      "Epoch 1, Batch 9, Loss: 2.1898581981658936\n",
      "Epoch 1, Batch 10, Loss: 2.196791887283325\n",
      "Epoch 1, Batch 11, Loss: 2.2032322883605957\n",
      "Epoch 1, Batch 12, Loss: 2.1932971477508545\n",
      "Epoch 1, Batch 13, Loss: 2.176666736602783\n",
      "Epoch 1, Batch 14, Loss: 2.1896486282348633\n",
      "Epoch 1, Batch 15, Loss: 2.181422472000122\n",
      "Epoch 1, Batch 16, Loss: 2.19456148147583\n",
      "Epoch 1, Batch 17, Loss: 2.1954712867736816\n",
      "Epoch 1, Batch 18, Loss: 2.1914138793945312\n",
      "Epoch 1, Batch 19, Loss: 2.180476665496826\n",
      "Epoch 1, Batch 20, Loss: 2.1863629817962646\n",
      "Epoch 1, Batch 21, Loss: 2.18925142288208\n",
      "Epoch 1, Batch 22, Loss: 2.17750883102417\n",
      "Epoch 1, Batch 23, Loss: 2.187448024749756\n",
      "Epoch 1, Batch 24, Loss: 2.1735286712646484\n",
      "Epoch 1, Batch 25, Loss: 2.176114082336426\n",
      "Epoch 1, Batch 26, Loss: 2.1894760131835938\n",
      "Epoch 1, Batch 27, Loss: 2.1790804862976074\n",
      "Epoch 1, Batch 28, Loss: 2.1813905239105225\n",
      "Epoch 1, Batch 29, Loss: 2.176297664642334\n",
      "Epoch 1, Batch 30, Loss: 2.1703202724456787\n",
      "Epoch 1, Batch 31, Loss: 2.1665539741516113\n",
      "Epoch 1, Batch 32, Loss: 2.160658359527588\n",
      "Epoch 1, Batch 33, Loss: 2.1596615314483643\n",
      "Epoch 1, Batch 34, Loss: 2.161851644515991\n",
      "Epoch 1, Batch 35, Loss: 2.1613006591796875\n",
      "Epoch 1, Batch 36, Loss: 2.162855625152588\n",
      "Epoch 1, Batch 37, Loss: 2.1538567543029785\n",
      "Epoch 1, Batch 38, Loss: 2.1724982261657715\n",
      "Epoch 1, Batch 39, Loss: 2.153947353363037\n",
      "Epoch 1, Batch 40, Loss: 2.1508212089538574\n",
      "Epoch 1, Batch 41, Loss: 2.153137683868408\n",
      "Epoch 1, Batch 42, Loss: 2.151287078857422\n",
      "Epoch 1, Batch 43, Loss: 2.142657995223999\n",
      "Epoch 1, Batch 44, Loss: 2.1590170860290527\n",
      "Epoch 1, Batch 45, Loss: 2.1402764320373535\n",
      "Epoch 1, Batch 46, Loss: 2.1510672569274902\n",
      "Epoch 1, Batch 47, Loss: 2.134819746017456\n",
      "Epoch 1, Batch 48, Loss: 2.140484571456909\n",
      "Epoch 1, Batch 49, Loss: 2.1504533290863037\n",
      "Epoch 1, Batch 50, Loss: 2.138185977935791\n",
      "Epoch 1, Batch 51, Loss: 2.128612518310547\n",
      "Epoch 1, Batch 52, Loss: 2.1370227336883545\n",
      "Epoch 1, Batch 53, Loss: 2.1239101886749268\n",
      "Epoch 1, Batch 54, Loss: 2.141603946685791\n",
      "Epoch 1, Batch 55, Loss: 2.130293130874634\n",
      "Epoch 1, Batch 56, Loss: 2.1261415481567383\n",
      "Epoch 1, Batch 57, Loss: 2.118760347366333\n",
      "Epoch 1, Batch 58, Loss: 2.1322810649871826\n",
      "Epoch 1, Batch 59, Loss: 2.1160831451416016\n",
      "Epoch 1, Batch 60, Loss: 2.143913745880127\n",
      "Epoch 1, Batch 61, Loss: 2.1300463676452637\n",
      "Epoch 1, Batch 62, Loss: 2.1334636211395264\n",
      "Epoch 1, Batch 63, Loss: 2.11991548538208\n",
      "Epoch 1, Batch 64, Loss: 2.1169047355651855\n",
      "Epoch 1, Batch 65, Loss: 2.115898609161377\n",
      "Epoch 1, Batch 66, Loss: 2.1266703605651855\n",
      "Epoch 1, Batch 67, Loss: 2.105368137359619\n",
      "Epoch 1, Batch 68, Loss: 2.117673635482788\n",
      "Epoch 1, Batch 69, Loss: 2.1170716285705566\n",
      "Epoch 1, Batch 70, Loss: 2.111309051513672\n",
      "Epoch 1, Batch 71, Loss: 2.108598232269287\n",
      "Epoch 1, Batch 72, Loss: 2.114614963531494\n",
      "Epoch 1, Batch 73, Loss: 2.1194634437561035\n",
      "Epoch 1, Batch 74, Loss: 2.118018388748169\n",
      "Epoch 1, Batch 75, Loss: 2.0880448818206787\n",
      "Epoch 1, Batch 76, Loss: 2.084353446960449\n",
      "Epoch 1, Batch 77, Loss: 2.096897602081299\n",
      "Epoch 1, Batch 78, Loss: 2.0922458171844482\n",
      "Epoch 1, Batch 79, Loss: 2.0928807258605957\n",
      "Epoch 1, Batch 80, Loss: 2.0921175479888916\n",
      "Epoch 1, Batch 81, Loss: 2.1058037281036377\n",
      "Epoch 1, Batch 82, Loss: 2.0929033756256104\n",
      "Epoch 1, Batch 83, Loss: 2.0951619148254395\n",
      "Epoch 1, Batch 84, Loss: 2.094853401184082\n",
      "Epoch 1, Batch 85, Loss: 2.091813325881958\n",
      "Epoch 1, Batch 86, Loss: 2.0849554538726807\n",
      "Epoch 1, Batch 87, Loss: 2.0986227989196777\n",
      "Epoch 1, Batch 88, Loss: 2.089280843734741\n",
      "Epoch 1, Batch 89, Loss: 2.1011130809783936\n",
      "Epoch 1, Batch 90, Loss: 2.0854544639587402\n",
      "Epoch 1, Batch 91, Loss: 2.0800187587738037\n",
      "Epoch 1, Batch 92, Loss: 2.0714175701141357\n",
      "Epoch 1, Batch 93, Loss: 2.098747491836548\n",
      "Epoch 2, Batch 0, Loss: 2.072033405303955\n",
      "Epoch 2, Batch 1, Loss: 2.100024461746216\n",
      "Epoch 2, Batch 2, Loss: 2.093996047973633\n",
      "Epoch 2, Batch 3, Loss: 2.076943874359131\n",
      "Epoch 2, Batch 4, Loss: 2.0727946758270264\n",
      "Epoch 2, Batch 5, Loss: 2.064723014831543\n",
      "Epoch 2, Batch 6, Loss: 2.0706629753112793\n",
      "Epoch 2, Batch 7, Loss: 2.059697151184082\n",
      "Epoch 2, Batch 8, Loss: 2.067762851715088\n",
      "Epoch 2, Batch 9, Loss: 2.067859172821045\n",
      "Epoch 2, Batch 10, Loss: 2.0698726177215576\n",
      "Epoch 2, Batch 11, Loss: 2.0577757358551025\n",
      "Epoch 2, Batch 12, Loss: 2.0573041439056396\n",
      "Epoch 2, Batch 13, Loss: 2.05258846282959\n",
      "Epoch 2, Batch 14, Loss: 2.0427193641662598\n",
      "Epoch 2, Batch 15, Loss: 2.049307107925415\n",
      "Epoch 2, Batch 16, Loss: 2.0528481006622314\n",
      "Epoch 2, Batch 17, Loss: 2.0594489574432373\n",
      "Epoch 2, Batch 18, Loss: 2.0442419052124023\n",
      "Epoch 2, Batch 19, Loss: 2.062952995300293\n",
      "Epoch 2, Batch 20, Loss: 2.0441553592681885\n",
      "Epoch 2, Batch 21, Loss: 2.074052095413208\n",
      "Epoch 2, Batch 22, Loss: 2.054478645324707\n",
      "Epoch 2, Batch 23, Loss: 2.0633034706115723\n",
      "Epoch 2, Batch 24, Loss: 2.057314395904541\n",
      "Epoch 2, Batch 25, Loss: 2.0516014099121094\n",
      "Epoch 2, Batch 26, Loss: 2.0298876762390137\n",
      "Epoch 2, Batch 27, Loss: 2.045246124267578\n",
      "Epoch 2, Batch 28, Loss: 2.029949426651001\n",
      "Epoch 2, Batch 29, Loss: 2.0338549613952637\n",
      "Epoch 2, Batch 30, Loss: 2.032423496246338\n",
      "Epoch 2, Batch 31, Loss: 2.038170576095581\n",
      "Epoch 2, Batch 32, Loss: 2.036166191101074\n",
      "Epoch 2, Batch 33, Loss: 2.038323402404785\n",
      "Epoch 2, Batch 34, Loss: 2.042372941970825\n",
      "Epoch 2, Batch 35, Loss: 2.020306348800659\n",
      "Epoch 2, Batch 36, Loss: 2.036736249923706\n",
      "Epoch 2, Batch 37, Loss: 2.0309672355651855\n",
      "Epoch 2, Batch 38, Loss: 2.0258283615112305\n",
      "Epoch 2, Batch 39, Loss: 2.0397210121154785\n",
      "Epoch 2, Batch 40, Loss: 2.0085859298706055\n",
      "Epoch 2, Batch 41, Loss: 2.0242695808410645\n",
      "Epoch 2, Batch 42, Loss: 2.01206374168396\n",
      "Epoch 2, Batch 43, Loss: 2.013442039489746\n",
      "Epoch 2, Batch 44, Loss: 2.0083212852478027\n",
      "Epoch 2, Batch 45, Loss: 2.020963430404663\n",
      "Epoch 2, Batch 46, Loss: 2.011164903640747\n",
      "Epoch 2, Batch 47, Loss: 2.0189707279205322\n",
      "Epoch 2, Batch 48, Loss: 2.0086464881896973\n",
      "Epoch 2, Batch 49, Loss: 2.0054047107696533\n",
      "Epoch 2, Batch 50, Loss: 2.006554126739502\n",
      "Epoch 2, Batch 51, Loss: 2.001472234725952\n",
      "Epoch 2, Batch 52, Loss: 1.996474266052246\n",
      "Epoch 2, Batch 53, Loss: 1.9925708770751953\n",
      "Epoch 2, Batch 54, Loss: 2.0006566047668457\n",
      "Epoch 2, Batch 55, Loss: 2.002702236175537\n",
      "Epoch 2, Batch 56, Loss: 2.002501964569092\n",
      "Epoch 2, Batch 57, Loss: 1.9895620346069336\n",
      "Epoch 2, Batch 58, Loss: 1.9850575923919678\n",
      "Epoch 2, Batch 59, Loss: 2.021061420440674\n",
      "Epoch 2, Batch 60, Loss: 2.0118300914764404\n",
      "Epoch 2, Batch 61, Loss: 1.9873590469360352\n",
      "Epoch 2, Batch 62, Loss: 1.9694305658340454\n",
      "Epoch 2, Batch 63, Loss: 1.9913469552993774\n",
      "Epoch 2, Batch 64, Loss: 1.9806878566741943\n",
      "Epoch 2, Batch 65, Loss: 1.98934006690979\n",
      "Epoch 2, Batch 66, Loss: 1.9725723266601562\n",
      "Epoch 2, Batch 67, Loss: 1.9693152904510498\n",
      "Epoch 2, Batch 68, Loss: 1.9813871383666992\n",
      "Epoch 2, Batch 69, Loss: 2.0005202293395996\n",
      "Epoch 2, Batch 70, Loss: 1.9811666011810303\n",
      "Epoch 2, Batch 71, Loss: 1.9798431396484375\n",
      "Epoch 2, Batch 72, Loss: 1.9816240072250366\n",
      "Epoch 2, Batch 73, Loss: 1.9845889806747437\n",
      "Epoch 2, Batch 74, Loss: 1.9732334613800049\n",
      "Epoch 2, Batch 75, Loss: 1.9910526275634766\n",
      "Epoch 2, Batch 76, Loss: 1.95327889919281\n",
      "Epoch 2, Batch 77, Loss: 1.9513778686523438\n",
      "Epoch 2, Batch 78, Loss: 1.9531800746917725\n",
      "Epoch 2, Batch 79, Loss: 1.9651553630828857\n",
      "Epoch 2, Batch 80, Loss: 1.9728167057037354\n",
      "Epoch 2, Batch 81, Loss: 1.9425954818725586\n",
      "Epoch 2, Batch 82, Loss: 1.9546682834625244\n",
      "Epoch 2, Batch 83, Loss: 1.968934416770935\n",
      "Epoch 2, Batch 84, Loss: 1.9754348993301392\n",
      "Epoch 2, Batch 85, Loss: 1.9716989994049072\n",
      "Epoch 2, Batch 86, Loss: 1.9591413736343384\n",
      "Epoch 2, Batch 87, Loss: 1.9555206298828125\n",
      "Epoch 2, Batch 88, Loss: 1.9418779611587524\n",
      "Epoch 2, Batch 89, Loss: 1.94106125831604\n",
      "Epoch 2, Batch 90, Loss: 1.9511833190917969\n",
      "Epoch 2, Batch 91, Loss: 1.9453102350234985\n",
      "Epoch 2, Batch 92, Loss: 1.926997184753418\n",
      "Epoch 2, Batch 93, Loss: 1.9360651969909668\n",
      "Epoch 3, Batch 0, Loss: 1.9476053714752197\n",
      "Epoch 3, Batch 1, Loss: 1.946860909461975\n",
      "Epoch 3, Batch 2, Loss: 1.94828200340271\n",
      "Epoch 3, Batch 3, Loss: 1.9461429119110107\n",
      "Epoch 3, Batch 4, Loss: 1.9502149820327759\n",
      "Epoch 3, Batch 5, Loss: 1.9518108367919922\n",
      "Epoch 3, Batch 6, Loss: 1.9216091632843018\n",
      "Epoch 3, Batch 7, Loss: 1.9396417140960693\n",
      "Epoch 3, Batch 8, Loss: 1.9221023321151733\n",
      "Epoch 3, Batch 9, Loss: 1.9566490650177002\n",
      "Epoch 3, Batch 10, Loss: 1.950178861618042\n",
      "Epoch 3, Batch 11, Loss: 1.9284546375274658\n",
      "Epoch 3, Batch 12, Loss: 1.922279953956604\n",
      "Epoch 3, Batch 13, Loss: 1.9397281408309937\n",
      "Epoch 3, Batch 14, Loss: 1.915191650390625\n",
      "Epoch 3, Batch 15, Loss: 1.948216199874878\n",
      "Epoch 3, Batch 16, Loss: 1.9106407165527344\n",
      "Epoch 3, Batch 17, Loss: 1.9180047512054443\n",
      "Epoch 3, Batch 18, Loss: 1.9051752090454102\n",
      "Epoch 3, Batch 19, Loss: 1.91830313205719\n",
      "Epoch 3, Batch 20, Loss: 1.9009771347045898\n",
      "Epoch 3, Batch 21, Loss: 1.9076095819473267\n",
      "Epoch 3, Batch 22, Loss: 1.9101362228393555\n",
      "Epoch 3, Batch 23, Loss: 1.9181667566299438\n",
      "Epoch 3, Batch 24, Loss: 1.911628007888794\n",
      "Epoch 3, Batch 25, Loss: 1.9113292694091797\n",
      "Epoch 3, Batch 26, Loss: 1.9153112173080444\n",
      "Epoch 3, Batch 27, Loss: 1.8879492282867432\n",
      "Epoch 3, Batch 28, Loss: 1.921807050704956\n",
      "Epoch 3, Batch 29, Loss: 1.904013991355896\n",
      "Epoch 3, Batch 30, Loss: 1.8919293880462646\n",
      "Epoch 3, Batch 31, Loss: 1.8892396688461304\n",
      "Epoch 3, Batch 32, Loss: 1.906945824623108\n",
      "Epoch 3, Batch 33, Loss: 1.9080148935317993\n",
      "Epoch 3, Batch 34, Loss: 1.8925873041152954\n",
      "Epoch 3, Batch 35, Loss: 1.9018112421035767\n",
      "Epoch 3, Batch 36, Loss: 1.8975565433502197\n",
      "Epoch 3, Batch 37, Loss: 1.884049415588379\n",
      "Epoch 3, Batch 38, Loss: 1.8899062871932983\n",
      "Epoch 3, Batch 39, Loss: 1.8963121175765991\n",
      "Epoch 3, Batch 40, Loss: 1.8798086643218994\n",
      "Epoch 3, Batch 41, Loss: 1.886466383934021\n",
      "Epoch 3, Batch 42, Loss: 1.889061689376831\n",
      "Epoch 3, Batch 43, Loss: 1.8580862283706665\n",
      "Epoch 3, Batch 44, Loss: 1.8934437036514282\n",
      "Epoch 3, Batch 45, Loss: 1.8701250553131104\n",
      "Epoch 3, Batch 46, Loss: 1.866865873336792\n",
      "Epoch 3, Batch 47, Loss: 1.8571035861968994\n",
      "Epoch 3, Batch 48, Loss: 1.892183542251587\n",
      "Epoch 3, Batch 49, Loss: 1.8865026235580444\n",
      "Epoch 3, Batch 50, Loss: 1.8674898147583008\n",
      "Epoch 3, Batch 51, Loss: 1.859561562538147\n",
      "Epoch 3, Batch 52, Loss: 1.8608782291412354\n",
      "Epoch 3, Batch 53, Loss: 1.8657214641571045\n",
      "Epoch 3, Batch 54, Loss: 1.8426923751831055\n",
      "Epoch 3, Batch 55, Loss: 1.857351541519165\n",
      "Epoch 3, Batch 56, Loss: 1.8607540130615234\n",
      "Epoch 3, Batch 57, Loss: 1.8644921779632568\n",
      "Epoch 3, Batch 58, Loss: 1.875758409500122\n",
      "Epoch 3, Batch 59, Loss: 1.8537304401397705\n",
      "Epoch 3, Batch 60, Loss: 1.879765510559082\n",
      "Epoch 3, Batch 61, Loss: 1.8409522771835327\n",
      "Epoch 3, Batch 62, Loss: 1.8699398040771484\n",
      "Epoch 3, Batch 63, Loss: 1.839980125427246\n",
      "Epoch 3, Batch 64, Loss: 1.86212956905365\n",
      "Epoch 3, Batch 65, Loss: 1.8493133783340454\n",
      "Epoch 3, Batch 66, Loss: 1.8351465463638306\n",
      "Epoch 3, Batch 67, Loss: 1.8344675302505493\n",
      "Epoch 3, Batch 68, Loss: 1.8449691534042358\n",
      "Epoch 3, Batch 69, Loss: 1.8242683410644531\n",
      "Epoch 3, Batch 70, Loss: 1.8493328094482422\n",
      "Epoch 3, Batch 71, Loss: 1.8519384860992432\n",
      "Epoch 3, Batch 72, Loss: 1.8413499593734741\n",
      "Epoch 3, Batch 73, Loss: 1.8504879474639893\n",
      "Epoch 3, Batch 74, Loss: 1.843463659286499\n",
      "Epoch 3, Batch 75, Loss: 1.8323713541030884\n",
      "Epoch 3, Batch 76, Loss: 1.8329055309295654\n",
      "Epoch 3, Batch 77, Loss: 1.8322566747665405\n",
      "Epoch 3, Batch 78, Loss: 1.8403568267822266\n",
      "Epoch 3, Batch 79, Loss: 1.8110716342926025\n",
      "Epoch 3, Batch 80, Loss: 1.8335001468658447\n",
      "Epoch 3, Batch 81, Loss: 1.845740556716919\n",
      "Epoch 3, Batch 82, Loss: 1.8194849491119385\n",
      "Epoch 3, Batch 83, Loss: 1.8133608102798462\n",
      "Epoch 3, Batch 84, Loss: 1.8351348638534546\n",
      "Epoch 3, Batch 85, Loss: 1.832360029220581\n",
      "Epoch 3, Batch 86, Loss: 1.8019163608551025\n",
      "Epoch 3, Batch 87, Loss: 1.798423409461975\n",
      "Epoch 3, Batch 88, Loss: 1.802870512008667\n",
      "Epoch 3, Batch 89, Loss: 1.7903766632080078\n",
      "Epoch 3, Batch 90, Loss: 1.7996413707733154\n",
      "Epoch 3, Batch 91, Loss: 1.8170515298843384\n",
      "Epoch 3, Batch 92, Loss: 1.80404531955719\n",
      "Epoch 3, Batch 93, Loss: 1.8066117763519287\n",
      "Epoch 4, Batch 0, Loss: 1.8016436100006104\n",
      "Epoch 4, Batch 1, Loss: 1.8185609579086304\n",
      "Epoch 4, Batch 2, Loss: 1.8336318731307983\n",
      "Epoch 4, Batch 3, Loss: 1.7997896671295166\n",
      "Epoch 4, Batch 4, Loss: 1.8001070022583008\n",
      "Epoch 4, Batch 5, Loss: 1.8247663974761963\n",
      "Epoch 4, Batch 6, Loss: 1.785646677017212\n",
      "Epoch 4, Batch 7, Loss: 1.8100860118865967\n",
      "Epoch 4, Batch 8, Loss: 1.7842258214950562\n",
      "Epoch 4, Batch 9, Loss: 1.7719833850860596\n",
      "Epoch 4, Batch 10, Loss: 1.787052869796753\n",
      "Epoch 4, Batch 11, Loss: 1.7682626247406006\n",
      "Epoch 4, Batch 12, Loss: 1.7794363498687744\n",
      "Epoch 4, Batch 13, Loss: 1.7990633249282837\n",
      "Epoch 4, Batch 14, Loss: 1.7823059558868408\n",
      "Epoch 4, Batch 15, Loss: 1.7645705938339233\n",
      "Epoch 4, Batch 16, Loss: 1.7953481674194336\n",
      "Epoch 4, Batch 17, Loss: 1.7994401454925537\n",
      "Epoch 4, Batch 18, Loss: 1.7904279232025146\n",
      "Epoch 4, Batch 19, Loss: 1.8009084463119507\n",
      "Epoch 4, Batch 20, Loss: 1.774195671081543\n",
      "Epoch 4, Batch 21, Loss: 1.7733838558197021\n",
      "Epoch 4, Batch 22, Loss: 1.787353515625\n",
      "Epoch 4, Batch 23, Loss: 1.785082459449768\n",
      "Epoch 4, Batch 24, Loss: 1.783905029296875\n",
      "Epoch 4, Batch 25, Loss: 1.7678382396697998\n",
      "Epoch 4, Batch 26, Loss: 1.7594465017318726\n",
      "Epoch 4, Batch 27, Loss: 1.7450501918792725\n",
      "Epoch 4, Batch 28, Loss: 1.7790504693984985\n",
      "Epoch 4, Batch 29, Loss: 1.7832040786743164\n",
      "Epoch 4, Batch 30, Loss: 1.7732661962509155\n",
      "Epoch 4, Batch 31, Loss: 1.7412971258163452\n",
      "Epoch 4, Batch 32, Loss: 1.7445722818374634\n",
      "Epoch 4, Batch 33, Loss: 1.764819860458374\n",
      "Epoch 4, Batch 34, Loss: 1.7551429271697998\n",
      "Epoch 4, Batch 35, Loss: 1.766714334487915\n",
      "Epoch 4, Batch 36, Loss: 1.747758150100708\n",
      "Epoch 4, Batch 37, Loss: 1.7612279653549194\n",
      "Epoch 4, Batch 38, Loss: 1.714016318321228\n",
      "Epoch 4, Batch 39, Loss: 1.7618396282196045\n",
      "Epoch 4, Batch 40, Loss: 1.7361059188842773\n",
      "Epoch 4, Batch 41, Loss: 1.7571477890014648\n",
      "Epoch 4, Batch 42, Loss: 1.7408727407455444\n",
      "Epoch 4, Batch 43, Loss: 1.7450170516967773\n",
      "Epoch 4, Batch 44, Loss: 1.7587335109710693\n",
      "Epoch 4, Batch 45, Loss: 1.742370367050171\n",
      "Epoch 4, Batch 46, Loss: 1.7546212673187256\n",
      "Epoch 4, Batch 47, Loss: 1.7656618356704712\n",
      "Epoch 4, Batch 48, Loss: 1.7221496105194092\n",
      "Epoch 4, Batch 49, Loss: 1.7214181423187256\n",
      "Epoch 4, Batch 50, Loss: 1.7666501998901367\n",
      "Epoch 4, Batch 51, Loss: 1.740073800086975\n",
      "Epoch 4, Batch 52, Loss: 1.721989393234253\n",
      "Epoch 4, Batch 53, Loss: 1.7408664226531982\n",
      "Epoch 4, Batch 54, Loss: 1.7575111389160156\n",
      "Epoch 4, Batch 55, Loss: 1.7266356945037842\n",
      "Epoch 4, Batch 56, Loss: 1.7084232568740845\n",
      "Epoch 4, Batch 57, Loss: 1.7319446802139282\n",
      "Epoch 4, Batch 58, Loss: 1.733475685119629\n",
      "Epoch 4, Batch 59, Loss: 1.7128562927246094\n",
      "Epoch 4, Batch 60, Loss: 1.742071509361267\n",
      "Epoch 4, Batch 61, Loss: 1.7014656066894531\n",
      "Epoch 4, Batch 62, Loss: 1.7510993480682373\n",
      "Epoch 4, Batch 63, Loss: 1.730255126953125\n",
      "Epoch 4, Batch 64, Loss: 1.7149183750152588\n",
      "Epoch 4, Batch 65, Loss: 1.7033021450042725\n",
      "Epoch 4, Batch 66, Loss: 1.6948572397232056\n",
      "Epoch 4, Batch 67, Loss: 1.7029716968536377\n",
      "Epoch 4, Batch 68, Loss: 1.7117856740951538\n",
      "Epoch 4, Batch 69, Loss: 1.7077674865722656\n",
      "Epoch 4, Batch 70, Loss: 1.7167565822601318\n",
      "Epoch 4, Batch 71, Loss: 1.6921403408050537\n",
      "Epoch 4, Batch 72, Loss: 1.720362901687622\n",
      "Epoch 4, Batch 73, Loss: 1.7025642395019531\n",
      "Epoch 4, Batch 74, Loss: 1.6934610605239868\n",
      "Epoch 4, Batch 75, Loss: 1.6834018230438232\n",
      "Epoch 4, Batch 76, Loss: 1.6923646926879883\n",
      "Epoch 4, Batch 77, Loss: 1.6821095943450928\n",
      "Epoch 4, Batch 78, Loss: 1.6870094537734985\n",
      "Epoch 4, Batch 79, Loss: 1.681980848312378\n",
      "Epoch 4, Batch 80, Loss: 1.6863441467285156\n",
      "Epoch 4, Batch 81, Loss: 1.6762803792953491\n",
      "Epoch 4, Batch 82, Loss: 1.6942058801651\n",
      "Epoch 4, Batch 83, Loss: 1.6897751092910767\n",
      "Epoch 4, Batch 84, Loss: 1.713529348373413\n",
      "Epoch 4, Batch 85, Loss: 1.666347861289978\n",
      "Epoch 4, Batch 86, Loss: 1.6734645366668701\n",
      "Epoch 4, Batch 87, Loss: 1.6848366260528564\n",
      "Epoch 4, Batch 88, Loss: 1.6680402755737305\n",
      "Epoch 4, Batch 89, Loss: 1.6639232635498047\n",
      "Epoch 4, Batch 90, Loss: 1.6944879293441772\n",
      "Epoch 4, Batch 91, Loss: 1.6747022867202759\n",
      "Epoch 4, Batch 92, Loss: 1.6683323383331299\n",
      "Epoch 4, Batch 93, Loss: 1.6556432247161865\n",
      "Epoch 5, Batch 0, Loss: 1.6741641759872437\n",
      "Epoch 5, Batch 1, Loss: 1.665770173072815\n",
      "Epoch 5, Batch 2, Loss: 1.6933367252349854\n",
      "Epoch 5, Batch 3, Loss: 1.6495082378387451\n",
      "Epoch 5, Batch 4, Loss: 1.6664612293243408\n",
      "Epoch 5, Batch 5, Loss: 1.6951831579208374\n",
      "Epoch 5, Batch 6, Loss: 1.641674280166626\n",
      "Epoch 5, Batch 7, Loss: 1.6670795679092407\n",
      "Epoch 5, Batch 8, Loss: 1.6624596118927002\n",
      "Epoch 5, Batch 9, Loss: 1.674370527267456\n",
      "Epoch 5, Batch 10, Loss: 1.6337772607803345\n",
      "Epoch 5, Batch 11, Loss: 1.6460869312286377\n",
      "Epoch 5, Batch 12, Loss: 1.6459344625473022\n",
      "Epoch 5, Batch 13, Loss: 1.6368328332901\n",
      "Epoch 5, Batch 14, Loss: 1.6726258993148804\n",
      "Epoch 5, Batch 15, Loss: 1.6613155603408813\n",
      "Epoch 5, Batch 16, Loss: 1.6582858562469482\n",
      "Epoch 5, Batch 17, Loss: 1.6553151607513428\n",
      "Epoch 5, Batch 18, Loss: 1.6541812419891357\n",
      "Epoch 5, Batch 19, Loss: 1.650205373764038\n",
      "Epoch 5, Batch 20, Loss: 1.6348216533660889\n",
      "Epoch 5, Batch 21, Loss: 1.6203248500823975\n",
      "Epoch 5, Batch 22, Loss: 1.628806710243225\n",
      "Epoch 5, Batch 23, Loss: 1.6654618978500366\n",
      "Epoch 5, Batch 24, Loss: 1.6340163946151733\n",
      "Epoch 5, Batch 25, Loss: 1.6426324844360352\n",
      "Epoch 5, Batch 26, Loss: 1.6457977294921875\n",
      "Epoch 5, Batch 27, Loss: 1.597205638885498\n",
      "Epoch 5, Batch 28, Loss: 1.6215541362762451\n",
      "Epoch 5, Batch 29, Loss: 1.655901551246643\n",
      "Epoch 5, Batch 30, Loss: 1.6268529891967773\n",
      "Epoch 5, Batch 31, Loss: 1.6156965494155884\n",
      "Epoch 5, Batch 32, Loss: 1.6388731002807617\n",
      "Epoch 5, Batch 33, Loss: 1.607893705368042\n",
      "Epoch 5, Batch 34, Loss: 1.6217727661132812\n",
      "Epoch 5, Batch 35, Loss: 1.6399004459381104\n",
      "Epoch 5, Batch 36, Loss: 1.62265944480896\n",
      "Epoch 5, Batch 37, Loss: 1.622051477432251\n",
      "Epoch 5, Batch 38, Loss: 1.6464742422103882\n",
      "Epoch 5, Batch 39, Loss: 1.589054822921753\n",
      "Epoch 5, Batch 40, Loss: 1.6397182941436768\n",
      "Epoch 5, Batch 41, Loss: 1.6114342212677002\n",
      "Epoch 5, Batch 42, Loss: 1.6224758625030518\n",
      "Epoch 5, Batch 43, Loss: 1.6254587173461914\n",
      "Epoch 5, Batch 44, Loss: 1.6093699932098389\n",
      "Epoch 5, Batch 45, Loss: 1.5947514772415161\n",
      "Epoch 5, Batch 46, Loss: 1.6208765506744385\n",
      "Epoch 5, Batch 47, Loss: 1.5840622186660767\n",
      "Epoch 5, Batch 48, Loss: 1.5668665170669556\n",
      "Epoch 5, Batch 49, Loss: 1.6241624355316162\n",
      "Epoch 5, Batch 50, Loss: 1.5820064544677734\n",
      "Epoch 5, Batch 51, Loss: 1.6079782247543335\n",
      "Epoch 5, Batch 52, Loss: 1.626712441444397\n",
      "Epoch 5, Batch 53, Loss: 1.5966978073120117\n",
      "Epoch 5, Batch 54, Loss: 1.5876376628875732\n",
      "Epoch 5, Batch 55, Loss: 1.6149476766586304\n",
      "Epoch 5, Batch 56, Loss: 1.587158441543579\n",
      "Epoch 5, Batch 57, Loss: 1.6033484935760498\n",
      "Epoch 5, Batch 58, Loss: 1.596319556236267\n",
      "Epoch 5, Batch 59, Loss: 1.56577467918396\n",
      "Epoch 5, Batch 60, Loss: 1.6137170791625977\n",
      "Epoch 5, Batch 61, Loss: 1.5872498750686646\n",
      "Epoch 5, Batch 62, Loss: 1.598178744316101\n",
      "Epoch 5, Batch 63, Loss: 1.5668323040008545\n",
      "Epoch 5, Batch 64, Loss: 1.5599770545959473\n",
      "Epoch 5, Batch 65, Loss: 1.584863543510437\n",
      "Epoch 5, Batch 66, Loss: 1.5868396759033203\n",
      "Epoch 5, Batch 67, Loss: 1.5908660888671875\n",
      "Epoch 5, Batch 68, Loss: 1.6041733026504517\n",
      "Epoch 5, Batch 69, Loss: 1.5801247358322144\n",
      "Epoch 5, Batch 70, Loss: 1.5968286991119385\n",
      "Epoch 5, Batch 71, Loss: 1.5635874271392822\n",
      "Epoch 5, Batch 72, Loss: 1.5736463069915771\n",
      "Epoch 5, Batch 73, Loss: 1.5658725500106812\n",
      "Epoch 5, Batch 74, Loss: 1.565532922744751\n",
      "Epoch 5, Batch 75, Loss: 1.5724503993988037\n",
      "Epoch 5, Batch 76, Loss: 1.5457379817962646\n",
      "Epoch 5, Batch 77, Loss: 1.544503927230835\n",
      "Epoch 5, Batch 78, Loss: 1.5809301137924194\n",
      "Epoch 5, Batch 79, Loss: 1.5604671239852905\n",
      "Epoch 5, Batch 80, Loss: 1.5681602954864502\n",
      "Epoch 5, Batch 81, Loss: 1.5603320598602295\n",
      "Epoch 5, Batch 82, Loss: 1.5425059795379639\n",
      "Epoch 5, Batch 83, Loss: 1.544715166091919\n",
      "Epoch 5, Batch 84, Loss: 1.547349452972412\n",
      "Epoch 5, Batch 85, Loss: 1.554482102394104\n",
      "Epoch 5, Batch 86, Loss: 1.5287306308746338\n",
      "Epoch 5, Batch 87, Loss: 1.5325586795806885\n",
      "Epoch 5, Batch 88, Loss: 1.5754339694976807\n",
      "Epoch 5, Batch 89, Loss: 1.5414907932281494\n",
      "Epoch 5, Batch 90, Loss: 1.5703970193862915\n",
      "Epoch 5, Batch 91, Loss: 1.54703688621521\n",
      "Epoch 5, Batch 92, Loss: 1.5861124992370605\n",
      "Epoch 5, Batch 93, Loss: 1.5319350957870483\n",
      "Epoch 6, Batch 0, Loss: 1.5988861322402954\n",
      "Epoch 6, Batch 1, Loss: 1.5550698041915894\n",
      "Epoch 6, Batch 2, Loss: 1.5436532497406006\n",
      "Epoch 6, Batch 3, Loss: 1.5493435859680176\n",
      "Epoch 6, Batch 4, Loss: 1.5070301294326782\n",
      "Epoch 6, Batch 5, Loss: 1.518329381942749\n",
      "Epoch 6, Batch 6, Loss: 1.5201165676116943\n",
      "Epoch 6, Batch 7, Loss: 1.5473136901855469\n",
      "Epoch 6, Batch 8, Loss: 1.5096937417984009\n",
      "Epoch 6, Batch 9, Loss: 1.5198954343795776\n",
      "Epoch 6, Batch 10, Loss: 1.5212435722351074\n",
      "Epoch 6, Batch 11, Loss: 1.528367042541504\n",
      "Epoch 6, Batch 12, Loss: 1.5277044773101807\n",
      "Epoch 6, Batch 13, Loss: 1.5073063373565674\n",
      "Epoch 6, Batch 14, Loss: 1.548396348953247\n",
      "Epoch 6, Batch 15, Loss: 1.505233645439148\n",
      "Epoch 6, Batch 16, Loss: 1.5070903301239014\n",
      "Epoch 6, Batch 17, Loss: 1.5331324338912964\n",
      "Epoch 6, Batch 18, Loss: 1.5044257640838623\n",
      "Epoch 6, Batch 19, Loss: 1.4938747882843018\n",
      "Epoch 6, Batch 20, Loss: 1.5583457946777344\n",
      "Epoch 6, Batch 21, Loss: 1.4763457775115967\n",
      "Epoch 6, Batch 22, Loss: 1.5085680484771729\n",
      "Epoch 6, Batch 23, Loss: 1.515074372291565\n",
      "Epoch 6, Batch 24, Loss: 1.4867584705352783\n",
      "Epoch 6, Batch 25, Loss: 1.5109097957611084\n",
      "Epoch 6, Batch 26, Loss: 1.4674766063690186\n",
      "Epoch 6, Batch 27, Loss: 1.504713535308838\n",
      "Epoch 6, Batch 28, Loss: 1.511430263519287\n",
      "Epoch 6, Batch 29, Loss: 1.5209037065505981\n",
      "Epoch 6, Batch 30, Loss: 1.5219520330429077\n",
      "Epoch 6, Batch 31, Loss: 1.460442066192627\n",
      "Epoch 6, Batch 32, Loss: 1.4802601337432861\n",
      "Epoch 6, Batch 33, Loss: 1.4965730905532837\n",
      "Epoch 6, Batch 34, Loss: 1.4958866834640503\n",
      "Epoch 6, Batch 35, Loss: 1.4922068119049072\n",
      "Epoch 6, Batch 36, Loss: 1.4802335500717163\n",
      "Epoch 6, Batch 37, Loss: 1.479799747467041\n",
      "Epoch 6, Batch 38, Loss: 1.5440635681152344\n",
      "Epoch 6, Batch 39, Loss: 1.5128710269927979\n",
      "Epoch 6, Batch 40, Loss: 1.4685914516448975\n",
      "Epoch 6, Batch 41, Loss: 1.469920039176941\n",
      "Epoch 6, Batch 42, Loss: 1.4976565837860107\n",
      "Epoch 6, Batch 43, Loss: 1.4980459213256836\n",
      "Epoch 6, Batch 44, Loss: 1.486816644668579\n",
      "Epoch 6, Batch 45, Loss: 1.4615312814712524\n",
      "Epoch 6, Batch 46, Loss: 1.489303469657898\n",
      "Epoch 6, Batch 47, Loss: 1.4996451139450073\n",
      "Epoch 6, Batch 48, Loss: 1.503607988357544\n",
      "Epoch 6, Batch 49, Loss: 1.4334112405776978\n",
      "Epoch 6, Batch 50, Loss: 1.4792463779449463\n",
      "Epoch 6, Batch 51, Loss: 1.4822391271591187\n",
      "Epoch 6, Batch 52, Loss: 1.4696290493011475\n",
      "Epoch 6, Batch 53, Loss: 1.4631991386413574\n",
      "Epoch 6, Batch 54, Loss: 1.480057954788208\n",
      "Epoch 6, Batch 55, Loss: 1.5029313564300537\n",
      "Epoch 6, Batch 56, Loss: 1.5043927431106567\n",
      "Epoch 6, Batch 57, Loss: 1.5196713209152222\n",
      "Epoch 6, Batch 58, Loss: 1.4719468355178833\n",
      "Epoch 6, Batch 59, Loss: 1.4527390003204346\n",
      "Epoch 6, Batch 60, Loss: 1.4685875177383423\n",
      "Epoch 6, Batch 61, Loss: 1.4707529544830322\n",
      "Epoch 6, Batch 62, Loss: 1.4673395156860352\n",
      "Epoch 6, Batch 63, Loss: 1.4629050493240356\n",
      "Epoch 6, Batch 64, Loss: 1.440032958984375\n",
      "Epoch 6, Batch 65, Loss: 1.4371424913406372\n",
      "Epoch 6, Batch 66, Loss: 1.4638749361038208\n",
      "Epoch 6, Batch 67, Loss: 1.4357410669326782\n",
      "Epoch 6, Batch 68, Loss: 1.453309416770935\n",
      "Epoch 6, Batch 69, Loss: 1.468740701675415\n",
      "Epoch 6, Batch 70, Loss: 1.4734361171722412\n",
      "Epoch 6, Batch 71, Loss: 1.437792420387268\n",
      "Epoch 6, Batch 72, Loss: 1.4513494968414307\n",
      "Epoch 6, Batch 73, Loss: 1.4470620155334473\n",
      "Epoch 6, Batch 74, Loss: 1.4559928178787231\n",
      "Epoch 6, Batch 75, Loss: 1.450935959815979\n",
      "Epoch 6, Batch 76, Loss: 1.408503770828247\n",
      "Epoch 6, Batch 77, Loss: 1.488506555557251\n",
      "Epoch 6, Batch 78, Loss: 1.4188339710235596\n",
      "Epoch 6, Batch 79, Loss: 1.4088060855865479\n",
      "Epoch 6, Batch 80, Loss: 1.4640649557113647\n",
      "Epoch 6, Batch 81, Loss: 1.4593007564544678\n",
      "Epoch 6, Batch 82, Loss: 1.4838470220565796\n",
      "Epoch 6, Batch 83, Loss: 1.4114114046096802\n",
      "Epoch 6, Batch 84, Loss: 1.440373420715332\n",
      "Epoch 6, Batch 85, Loss: 1.4113563299179077\n",
      "Epoch 6, Batch 86, Loss: 1.4185796976089478\n",
      "Epoch 6, Batch 87, Loss: 1.4372514486312866\n",
      "Epoch 6, Batch 88, Loss: 1.4579201936721802\n",
      "Epoch 6, Batch 89, Loss: 1.4286538362503052\n",
      "Epoch 6, Batch 90, Loss: 1.4549691677093506\n",
      "Epoch 6, Batch 91, Loss: 1.4412000179290771\n",
      "Epoch 6, Batch 92, Loss: 1.4250694513320923\n",
      "Epoch 6, Batch 93, Loss: 1.4354157447814941\n",
      "Epoch 7, Batch 0, Loss: 1.403438687324524\n",
      "Epoch 7, Batch 1, Loss: 1.4253854751586914\n",
      "Epoch 7, Batch 2, Loss: 1.4291660785675049\n",
      "Epoch 7, Batch 3, Loss: 1.3987635374069214\n",
      "Epoch 7, Batch 4, Loss: 1.418099045753479\n",
      "Epoch 7, Batch 5, Loss: 1.415254831314087\n",
      "Epoch 7, Batch 6, Loss: 1.3996944427490234\n",
      "Epoch 7, Batch 7, Loss: 1.3902997970581055\n",
      "Epoch 7, Batch 8, Loss: 1.3759567737579346\n",
      "Epoch 7, Batch 9, Loss: 1.429180383682251\n",
      "Epoch 7, Batch 10, Loss: 1.4556002616882324\n",
      "Epoch 7, Batch 11, Loss: 1.3772284984588623\n",
      "Epoch 7, Batch 12, Loss: 1.3996328115463257\n",
      "Epoch 7, Batch 13, Loss: 1.4054282903671265\n",
      "Epoch 7, Batch 14, Loss: 1.4130953550338745\n",
      "Epoch 7, Batch 15, Loss: 1.4007937908172607\n",
      "Epoch 7, Batch 16, Loss: 1.4137064218521118\n",
      "Epoch 7, Batch 17, Loss: 1.421616554260254\n",
      "Epoch 7, Batch 18, Loss: 1.4060845375061035\n",
      "Epoch 7, Batch 19, Loss: 1.3732343912124634\n",
      "Epoch 7, Batch 20, Loss: 1.3939186334609985\n",
      "Epoch 7, Batch 21, Loss: 1.4436910152435303\n",
      "Epoch 7, Batch 22, Loss: 1.417128324508667\n",
      "Epoch 7, Batch 23, Loss: 1.3763407468795776\n",
      "Epoch 7, Batch 24, Loss: 1.4263578653335571\n",
      "Epoch 7, Batch 25, Loss: 1.3805551528930664\n",
      "Epoch 7, Batch 26, Loss: 1.352860689163208\n",
      "Epoch 7, Batch 27, Loss: 1.3909727334976196\n",
      "Epoch 7, Batch 28, Loss: 1.3843919038772583\n",
      "Epoch 7, Batch 29, Loss: 1.4224944114685059\n",
      "Epoch 7, Batch 30, Loss: 1.3828684091567993\n",
      "Epoch 7, Batch 31, Loss: 1.392336368560791\n",
      "Epoch 7, Batch 32, Loss: 1.39699387550354\n",
      "Epoch 7, Batch 33, Loss: 1.3822365999221802\n",
      "Epoch 7, Batch 34, Loss: 1.3957293033599854\n",
      "Epoch 7, Batch 35, Loss: 1.3827486038208008\n",
      "Epoch 7, Batch 36, Loss: 1.3746545314788818\n",
      "Epoch 7, Batch 37, Loss: 1.3580650091171265\n",
      "Epoch 7, Batch 38, Loss: 1.39165198802948\n",
      "Epoch 7, Batch 39, Loss: 1.3639262914657593\n",
      "Epoch 7, Batch 40, Loss: 1.3718048334121704\n",
      "Epoch 7, Batch 41, Loss: 1.3654850721359253\n",
      "Epoch 7, Batch 42, Loss: 1.3720061779022217\n",
      "Epoch 7, Batch 43, Loss: 1.4255695343017578\n",
      "Epoch 7, Batch 44, Loss: 1.3674652576446533\n",
      "Epoch 7, Batch 45, Loss: 1.3733420372009277\n",
      "Epoch 7, Batch 46, Loss: 1.3623216152191162\n",
      "Epoch 7, Batch 47, Loss: 1.3912324905395508\n",
      "Epoch 7, Batch 48, Loss: 1.3383585214614868\n",
      "Epoch 7, Batch 49, Loss: 1.3612946271896362\n",
      "Epoch 7, Batch 50, Loss: 1.351744532585144\n",
      "Epoch 7, Batch 51, Loss: 1.3579485416412354\n",
      "Epoch 7, Batch 52, Loss: 1.3559154272079468\n",
      "Epoch 7, Batch 53, Loss: 1.3886088132858276\n",
      "Epoch 7, Batch 54, Loss: 1.3623223304748535\n",
      "Epoch 7, Batch 55, Loss: 1.3695720434188843\n",
      "Epoch 7, Batch 56, Loss: 1.309154987335205\n",
      "Epoch 7, Batch 57, Loss: 1.3629169464111328\n",
      "Epoch 7, Batch 58, Loss: 1.3442739248275757\n",
      "Epoch 7, Batch 59, Loss: 1.3552826642990112\n",
      "Epoch 7, Batch 60, Loss: 1.3298850059509277\n",
      "Epoch 7, Batch 61, Loss: 1.3386375904083252\n",
      "Epoch 7, Batch 62, Loss: 1.35499107837677\n",
      "Epoch 7, Batch 63, Loss: 1.3280029296875\n",
      "Epoch 7, Batch 64, Loss: 1.3452645540237427\n",
      "Epoch 7, Batch 65, Loss: 1.320361852645874\n",
      "Epoch 7, Batch 66, Loss: 1.3429310321807861\n",
      "Epoch 7, Batch 67, Loss: 1.3529164791107178\n",
      "Epoch 7, Batch 68, Loss: 1.3292955160140991\n",
      "Epoch 7, Batch 69, Loss: 1.3552485704421997\n",
      "Epoch 7, Batch 70, Loss: 1.3486676216125488\n",
      "Epoch 7, Batch 71, Loss: 1.3429830074310303\n",
      "Epoch 7, Batch 72, Loss: 1.3311254978179932\n",
      "Epoch 7, Batch 73, Loss: 1.3621318340301514\n",
      "Epoch 7, Batch 74, Loss: 1.3164184093475342\n",
      "Epoch 7, Batch 75, Loss: 1.3244297504425049\n",
      "Epoch 7, Batch 76, Loss: 1.4021862745285034\n",
      "Epoch 7, Batch 77, Loss: 1.3132848739624023\n",
      "Epoch 7, Batch 78, Loss: 1.3114888668060303\n",
      "Epoch 7, Batch 79, Loss: 1.3110630512237549\n",
      "Epoch 7, Batch 80, Loss: 1.3274153470993042\n",
      "Epoch 7, Batch 81, Loss: 1.3145312070846558\n",
      "Epoch 7, Batch 82, Loss: 1.3456244468688965\n",
      "Epoch 7, Batch 83, Loss: 1.3062981367111206\n",
      "Epoch 7, Batch 84, Loss: 1.3450069427490234\n",
      "Epoch 7, Batch 85, Loss: 1.312831163406372\n",
      "Epoch 7, Batch 86, Loss: 1.3290083408355713\n",
      "Epoch 7, Batch 87, Loss: 1.353285789489746\n",
      "Epoch 7, Batch 88, Loss: 1.3610763549804688\n",
      "Epoch 7, Batch 89, Loss: 1.3253506422042847\n",
      "Epoch 7, Batch 90, Loss: 1.30794358253479\n",
      "Epoch 7, Batch 91, Loss: 1.2754548788070679\n",
      "Epoch 7, Batch 92, Loss: 1.347084879875183\n",
      "Epoch 7, Batch 93, Loss: 1.3191108703613281\n",
      "Epoch 8, Batch 0, Loss: 1.3258553743362427\n",
      "Epoch 8, Batch 1, Loss: 1.313997507095337\n",
      "Epoch 8, Batch 2, Loss: 1.295885443687439\n",
      "Epoch 8, Batch 3, Loss: 1.3372552394866943\n",
      "Epoch 8, Batch 4, Loss: 1.3238859176635742\n",
      "Epoch 8, Batch 5, Loss: 1.3069154024124146\n",
      "Epoch 8, Batch 6, Loss: 1.3241394758224487\n",
      "Epoch 8, Batch 7, Loss: 1.3097586631774902\n",
      "Epoch 8, Batch 8, Loss: 1.2674977779388428\n",
      "Epoch 8, Batch 9, Loss: 1.2860006093978882\n",
      "Epoch 8, Batch 10, Loss: 1.2977595329284668\n",
      "Epoch 8, Batch 11, Loss: 1.2757830619812012\n",
      "Epoch 8, Batch 12, Loss: 1.3094708919525146\n",
      "Epoch 8, Batch 13, Loss: 1.2796803712844849\n",
      "Epoch 8, Batch 14, Loss: 1.2571758031845093\n",
      "Epoch 8, Batch 15, Loss: 1.3070613145828247\n",
      "Epoch 8, Batch 16, Loss: 1.3024072647094727\n",
      "Epoch 8, Batch 17, Loss: 1.290877342224121\n",
      "Epoch 8, Batch 18, Loss: 1.2911494970321655\n",
      "Epoch 8, Batch 19, Loss: 1.2717808485031128\n",
      "Epoch 8, Batch 20, Loss: 1.259534239768982\n",
      "Epoch 8, Batch 21, Loss: 1.29215407371521\n",
      "Epoch 8, Batch 22, Loss: 1.2932924032211304\n",
      "Epoch 8, Batch 23, Loss: 1.3030554056167603\n",
      "Epoch 8, Batch 24, Loss: 1.309561014175415\n",
      "Epoch 8, Batch 25, Loss: 1.3049241304397583\n",
      "Epoch 8, Batch 26, Loss: 1.287832260131836\n",
      "Epoch 8, Batch 27, Loss: 1.2666500806808472\n",
      "Epoch 8, Batch 28, Loss: 1.2845196723937988\n",
      "Epoch 8, Batch 29, Loss: 1.2571910619735718\n",
      "Epoch 8, Batch 30, Loss: 1.2461649179458618\n",
      "Epoch 8, Batch 31, Loss: 1.3029651641845703\n",
      "Epoch 8, Batch 32, Loss: 1.2901479005813599\n",
      "Epoch 8, Batch 33, Loss: 1.2851428985595703\n",
      "Epoch 8, Batch 34, Loss: 1.2765086889266968\n",
      "Epoch 8, Batch 35, Loss: 1.304365873336792\n",
      "Epoch 8, Batch 36, Loss: 1.2486414909362793\n",
      "Epoch 8, Batch 37, Loss: 1.2493526935577393\n",
      "Epoch 8, Batch 38, Loss: 1.2766025066375732\n",
      "Epoch 8, Batch 39, Loss: 1.2900505065917969\n",
      "Epoch 8, Batch 40, Loss: 1.290610671043396\n",
      "Epoch 8, Batch 41, Loss: 1.2975555658340454\n",
      "Epoch 8, Batch 42, Loss: 1.2623810768127441\n",
      "Epoch 8, Batch 43, Loss: 1.2804147005081177\n",
      "Epoch 8, Batch 44, Loss: 1.2601535320281982\n",
      "Epoch 8, Batch 45, Loss: 1.251494288444519\n",
      "Epoch 8, Batch 46, Loss: 1.2501347064971924\n",
      "Epoch 8, Batch 47, Loss: 1.2498265504837036\n",
      "Epoch 8, Batch 48, Loss: 1.2727696895599365\n",
      "Epoch 8, Batch 49, Loss: 1.2476445436477661\n",
      "Epoch 8, Batch 50, Loss: 1.2717864513397217\n",
      "Epoch 8, Batch 51, Loss: 1.2628438472747803\n",
      "Epoch 8, Batch 52, Loss: 1.2594325542449951\n",
      "Epoch 8, Batch 53, Loss: 1.2625105381011963\n",
      "Epoch 8, Batch 54, Loss: 1.247686743736267\n",
      "Epoch 8, Batch 55, Loss: 1.2641003131866455\n",
      "Epoch 8, Batch 56, Loss: 1.2522422075271606\n",
      "Epoch 8, Batch 57, Loss: 1.2350962162017822\n",
      "Epoch 8, Batch 58, Loss: 1.290783405303955\n",
      "Epoch 8, Batch 59, Loss: 1.238183856010437\n",
      "Epoch 8, Batch 60, Loss: 1.2470571994781494\n",
      "Epoch 8, Batch 61, Loss: 1.2602713108062744\n",
      "Epoch 8, Batch 62, Loss: 1.281022548675537\n",
      "Epoch 8, Batch 63, Loss: 1.231299877166748\n",
      "Epoch 8, Batch 64, Loss: 1.244762659072876\n",
      "Epoch 8, Batch 65, Loss: 1.1991283893585205\n",
      "Epoch 8, Batch 66, Loss: 1.2242522239685059\n",
      "Epoch 8, Batch 67, Loss: 1.23766028881073\n",
      "Epoch 8, Batch 68, Loss: 1.2309720516204834\n",
      "Epoch 8, Batch 69, Loss: 1.2424415349960327\n",
      "Epoch 8, Batch 70, Loss: 1.2033755779266357\n",
      "Epoch 8, Batch 71, Loss: 1.2672730684280396\n",
      "Epoch 8, Batch 72, Loss: 1.242440938949585\n",
      "Epoch 8, Batch 73, Loss: 1.2357962131500244\n",
      "Epoch 8, Batch 74, Loss: 1.2172236442565918\n",
      "Epoch 8, Batch 75, Loss: 1.2151845693588257\n",
      "Epoch 8, Batch 76, Loss: 1.2298288345336914\n",
      "Epoch 8, Batch 77, Loss: 1.252799391746521\n",
      "Epoch 8, Batch 78, Loss: 1.2371352910995483\n",
      "Epoch 8, Batch 79, Loss: 1.2331782579421997\n",
      "Epoch 8, Batch 80, Loss: 1.232743263244629\n",
      "Epoch 8, Batch 81, Loss: 1.2301676273345947\n",
      "Epoch 8, Batch 82, Loss: 1.2411104440689087\n",
      "Epoch 8, Batch 83, Loss: 1.2273585796356201\n",
      "Epoch 8, Batch 84, Loss: 1.2600979804992676\n",
      "Epoch 8, Batch 85, Loss: 1.2207266092300415\n",
      "Epoch 8, Batch 86, Loss: 1.2334492206573486\n",
      "Epoch 8, Batch 87, Loss: 1.1963781118392944\n",
      "Epoch 8, Batch 88, Loss: 1.1809643507003784\n",
      "Epoch 8, Batch 89, Loss: 1.2520644664764404\n",
      "Epoch 8, Batch 90, Loss: 1.239495038986206\n",
      "Epoch 8, Batch 91, Loss: 1.221112847328186\n",
      "Epoch 8, Batch 92, Loss: 1.2419593334197998\n",
      "Epoch 8, Batch 93, Loss: 1.3030142784118652\n",
      "Epoch 9, Batch 0, Loss: 1.2301578521728516\n",
      "Epoch 9, Batch 1, Loss: 1.1981103420257568\n",
      "Epoch 9, Batch 2, Loss: 1.2054728269577026\n",
      "Epoch 9, Batch 3, Loss: 1.2323555946350098\n",
      "Epoch 9, Batch 4, Loss: 1.187828779220581\n",
      "Epoch 9, Batch 5, Loss: 1.207003116607666\n",
      "Epoch 9, Batch 6, Loss: 1.205333948135376\n",
      "Epoch 9, Batch 7, Loss: 1.2109299898147583\n",
      "Epoch 9, Batch 8, Loss: 1.204849362373352\n",
      "Epoch 9, Batch 9, Loss: 1.2294727563858032\n",
      "Epoch 9, Batch 10, Loss: 1.18459153175354\n",
      "Epoch 9, Batch 11, Loss: 1.2120436429977417\n",
      "Epoch 9, Batch 12, Loss: 1.2197061777114868\n",
      "Epoch 9, Batch 13, Loss: 1.1934525966644287\n",
      "Epoch 9, Batch 14, Loss: 1.1715242862701416\n",
      "Epoch 9, Batch 15, Loss: 1.2164082527160645\n",
      "Epoch 9, Batch 16, Loss: 1.2019977569580078\n",
      "Epoch 9, Batch 17, Loss: 1.181397557258606\n",
      "Epoch 9, Batch 18, Loss: 1.1766678094863892\n",
      "Epoch 9, Batch 19, Loss: 1.2249341011047363\n",
      "Epoch 9, Batch 20, Loss: 1.1778490543365479\n",
      "Epoch 9, Batch 21, Loss: 1.2380236387252808\n",
      "Epoch 9, Batch 22, Loss: 1.2061530351638794\n",
      "Epoch 9, Batch 23, Loss: 1.2031991481781006\n",
      "Epoch 9, Batch 24, Loss: 1.210964560508728\n",
      "Epoch 9, Batch 25, Loss: 1.1789413690567017\n",
      "Epoch 9, Batch 26, Loss: 1.168689489364624\n",
      "Epoch 9, Batch 27, Loss: 1.159926414489746\n",
      "Epoch 9, Batch 28, Loss: 1.173119068145752\n",
      "Epoch 9, Batch 29, Loss: 1.2290300130844116\n",
      "Epoch 9, Batch 30, Loss: 1.1993873119354248\n",
      "Epoch 9, Batch 31, Loss: 1.1690654754638672\n",
      "Epoch 9, Batch 32, Loss: 1.246667504310608\n",
      "Epoch 9, Batch 33, Loss: 1.1593856811523438\n",
      "Epoch 9, Batch 34, Loss: 1.1974427700042725\n",
      "Epoch 9, Batch 35, Loss: 1.1735025644302368\n",
      "Epoch 9, Batch 36, Loss: 1.1932452917099\n",
      "Epoch 9, Batch 37, Loss: 1.1839585304260254\n",
      "Epoch 9, Batch 38, Loss: 1.1353703737258911\n",
      "Epoch 9, Batch 39, Loss: 1.1826177835464478\n",
      "Epoch 9, Batch 40, Loss: 1.1429862976074219\n",
      "Epoch 9, Batch 41, Loss: 1.159600853919983\n",
      "Epoch 9, Batch 42, Loss: 1.1633844375610352\n",
      "Epoch 9, Batch 43, Loss: 1.194342851638794\n",
      "Epoch 9, Batch 44, Loss: 1.159694790840149\n",
      "Epoch 9, Batch 45, Loss: 1.179439902305603\n",
      "Epoch 9, Batch 46, Loss: 1.194273591041565\n",
      "Epoch 9, Batch 47, Loss: 1.1855199337005615\n",
      "Epoch 9, Batch 48, Loss: 1.1926090717315674\n",
      "Epoch 9, Batch 49, Loss: 1.1827925443649292\n",
      "Epoch 9, Batch 50, Loss: 1.1949756145477295\n",
      "Epoch 9, Batch 51, Loss: 1.1320488452911377\n",
      "Epoch 9, Batch 52, Loss: 1.1535931825637817\n",
      "Epoch 9, Batch 53, Loss: 1.202970266342163\n",
      "Epoch 9, Batch 54, Loss: 1.1522622108459473\n",
      "Epoch 9, Batch 55, Loss: 1.1622581481933594\n",
      "Epoch 9, Batch 56, Loss: 1.1404792070388794\n",
      "Epoch 9, Batch 57, Loss: 1.1387825012207031\n",
      "Epoch 9, Batch 58, Loss: 1.1385411024093628\n",
      "Epoch 9, Batch 59, Loss: 1.1789531707763672\n",
      "Epoch 9, Batch 60, Loss: 1.1645147800445557\n",
      "Epoch 9, Batch 61, Loss: 1.1423490047454834\n",
      "Epoch 9, Batch 62, Loss: 1.1441104412078857\n",
      "Epoch 9, Batch 63, Loss: 1.1526000499725342\n",
      "Epoch 9, Batch 64, Loss: 1.0949697494506836\n",
      "Epoch 9, Batch 65, Loss: 1.1429630517959595\n",
      "Epoch 9, Batch 66, Loss: 1.1820935010910034\n",
      "Epoch 9, Batch 67, Loss: 1.123075008392334\n",
      "Epoch 9, Batch 68, Loss: 1.1745014190673828\n",
      "Epoch 9, Batch 69, Loss: 1.1600208282470703\n",
      "Epoch 9, Batch 70, Loss: 1.161219596862793\n",
      "Epoch 9, Batch 71, Loss: 1.1430045366287231\n",
      "Epoch 9, Batch 72, Loss: 1.1775602102279663\n",
      "Epoch 9, Batch 73, Loss: 1.1649049520492554\n",
      "Epoch 9, Batch 74, Loss: 1.147499918937683\n",
      "Epoch 9, Batch 75, Loss: 1.1669493913650513\n",
      "Epoch 9, Batch 76, Loss: 1.1533558368682861\n",
      "Epoch 9, Batch 77, Loss: 1.1598913669586182\n",
      "Epoch 9, Batch 78, Loss: 1.1469190120697021\n",
      "Epoch 9, Batch 79, Loss: 1.1792138814926147\n",
      "Epoch 9, Batch 80, Loss: 1.163615345954895\n",
      "Epoch 9, Batch 81, Loss: 1.1408069133758545\n",
      "Epoch 9, Batch 82, Loss: 1.147772192955017\n",
      "Epoch 9, Batch 83, Loss: 1.1766728162765503\n",
      "Epoch 9, Batch 84, Loss: 1.161426305770874\n",
      "Epoch 9, Batch 85, Loss: 1.1093400716781616\n",
      "Epoch 9, Batch 86, Loss: 1.13937509059906\n",
      "Epoch 9, Batch 87, Loss: 1.1187572479248047\n",
      "Epoch 9, Batch 88, Loss: 1.143369436264038\n",
      "Epoch 9, Batch 89, Loss: 1.109344244003296\n",
      "Epoch 9, Batch 90, Loss: 1.1279220581054688\n",
      "Epoch 9, Batch 91, Loss: 1.1171088218688965\n",
      "Epoch 9, Batch 92, Loss: 1.1189799308776855\n",
      "Epoch 9, Batch 93, Loss: 1.1054481267929077\n",
      "Epoch 10, Batch 0, Loss: 1.0919629335403442\n",
      "Epoch 10, Batch 1, Loss: 1.0820376873016357\n",
      "Epoch 10, Batch 2, Loss: 1.155790090560913\n",
      "Epoch 10, Batch 3, Loss: 1.1555402278900146\n",
      "Epoch 10, Batch 4, Loss: 1.1580774784088135\n",
      "Epoch 10, Batch 5, Loss: 1.1362175941467285\n",
      "Epoch 10, Batch 6, Loss: 1.126427412033081\n",
      "Epoch 10, Batch 7, Loss: 1.1166760921478271\n",
      "Epoch 10, Batch 8, Loss: 1.1545618772506714\n",
      "Epoch 10, Batch 9, Loss: 1.1606504917144775\n",
      "Epoch 10, Batch 10, Loss: 1.118534803390503\n",
      "Epoch 10, Batch 11, Loss: 1.1227514743804932\n",
      "Epoch 10, Batch 12, Loss: 1.106995701789856\n",
      "Epoch 10, Batch 13, Loss: 1.1443315744400024\n",
      "Epoch 10, Batch 14, Loss: 1.0739531517028809\n",
      "Epoch 10, Batch 15, Loss: 1.132581353187561\n",
      "Epoch 10, Batch 16, Loss: 1.1184470653533936\n",
      "Epoch 10, Batch 17, Loss: 1.0923432111740112\n",
      "Epoch 10, Batch 18, Loss: 1.1053862571716309\n",
      "Epoch 10, Batch 19, Loss: 1.1156197786331177\n",
      "Epoch 10, Batch 20, Loss: 1.1397931575775146\n",
      "Epoch 10, Batch 21, Loss: 1.0986278057098389\n",
      "Epoch 10, Batch 22, Loss: 1.1190928220748901\n",
      "Epoch 10, Batch 23, Loss: 1.0672883987426758\n",
      "Epoch 10, Batch 24, Loss: 1.1202975511550903\n",
      "Epoch 10, Batch 25, Loss: 1.1016592979431152\n",
      "Epoch 10, Batch 26, Loss: 1.1341254711151123\n",
      "Epoch 10, Batch 27, Loss: 1.1308386325836182\n",
      "Epoch 10, Batch 28, Loss: 1.117143154144287\n",
      "Epoch 10, Batch 29, Loss: 1.0669012069702148\n",
      "Epoch 10, Batch 30, Loss: 1.0862709283828735\n",
      "Epoch 10, Batch 31, Loss: 1.0953857898712158\n",
      "Epoch 10, Batch 32, Loss: 1.1545053720474243\n",
      "Epoch 10, Batch 33, Loss: 1.0828840732574463\n",
      "Epoch 10, Batch 34, Loss: 1.0800155401229858\n",
      "Epoch 10, Batch 35, Loss: 1.106170415878296\n",
      "Epoch 10, Batch 36, Loss: 1.1111934185028076\n",
      "Epoch 10, Batch 37, Loss: 1.0935964584350586\n",
      "Epoch 10, Batch 38, Loss: 1.0721491575241089\n",
      "Epoch 10, Batch 39, Loss: 1.0938935279846191\n",
      "Epoch 10, Batch 40, Loss: 1.1066534519195557\n",
      "Epoch 10, Batch 41, Loss: 1.1033622026443481\n",
      "Epoch 10, Batch 42, Loss: 1.1133650541305542\n",
      "Epoch 10, Batch 43, Loss: 1.045333743095398\n",
      "Epoch 10, Batch 44, Loss: 1.0811179876327515\n",
      "Epoch 10, Batch 45, Loss: 1.0963627099990845\n",
      "Epoch 10, Batch 46, Loss: 1.0787211656570435\n",
      "Epoch 10, Batch 47, Loss: 1.0800014734268188\n",
      "Epoch 10, Batch 48, Loss: 1.0787394046783447\n",
      "Epoch 10, Batch 49, Loss: 1.103517770767212\n",
      "Epoch 10, Batch 50, Loss: 1.0954535007476807\n",
      "Epoch 10, Batch 51, Loss: 1.074326992034912\n",
      "Epoch 10, Batch 52, Loss: 1.0919737815856934\n",
      "Epoch 10, Batch 53, Loss: 1.0954911708831787\n",
      "Epoch 10, Batch 54, Loss: 1.0776374340057373\n",
      "Epoch 10, Batch 55, Loss: 1.093778371810913\n",
      "Epoch 10, Batch 56, Loss: 1.0788843631744385\n",
      "Epoch 10, Batch 57, Loss: 1.0164097547531128\n",
      "Epoch 10, Batch 58, Loss: 1.084592580795288\n",
      "Epoch 10, Batch 59, Loss: 1.1147418022155762\n",
      "Epoch 10, Batch 60, Loss: 1.0434129238128662\n",
      "Epoch 10, Batch 61, Loss: 1.074465274810791\n",
      "Epoch 10, Batch 62, Loss: 1.0809389352798462\n",
      "Epoch 10, Batch 63, Loss: 1.0623815059661865\n",
      "Epoch 10, Batch 64, Loss: 1.081019401550293\n",
      "Epoch 10, Batch 65, Loss: 1.0168397426605225\n",
      "Epoch 10, Batch 66, Loss: 1.096846342086792\n",
      "Epoch 10, Batch 67, Loss: 1.0758023262023926\n",
      "Epoch 10, Batch 68, Loss: 1.0761572122573853\n",
      "Epoch 10, Batch 69, Loss: 1.0792291164398193\n",
      "Epoch 10, Batch 70, Loss: 1.0708160400390625\n",
      "Epoch 10, Batch 71, Loss: 1.0813567638397217\n",
      "Epoch 10, Batch 72, Loss: 1.0917305946350098\n",
      "Epoch 10, Batch 73, Loss: 1.0662165880203247\n",
      "Epoch 10, Batch 74, Loss: 1.0589842796325684\n",
      "Epoch 10, Batch 75, Loss: 1.0522083044052124\n",
      "Epoch 10, Batch 76, Loss: 1.0707659721374512\n",
      "Epoch 10, Batch 77, Loss: 1.0455265045166016\n",
      "Epoch 10, Batch 78, Loss: 1.085486650466919\n",
      "Epoch 10, Batch 79, Loss: 1.090670108795166\n",
      "Epoch 10, Batch 80, Loss: 1.1112245321273804\n",
      "Epoch 10, Batch 81, Loss: 1.067405343055725\n",
      "Epoch 10, Batch 82, Loss: 1.0733368396759033\n",
      "Epoch 10, Batch 83, Loss: 1.0623798370361328\n",
      "Epoch 10, Batch 84, Loss: 1.0645372867584229\n",
      "Epoch 10, Batch 85, Loss: 1.0536839962005615\n",
      "Epoch 10, Batch 86, Loss: 1.0583000183105469\n",
      "Epoch 10, Batch 87, Loss: 1.0023930072784424\n",
      "Epoch 10, Batch 88, Loss: 1.1145223379135132\n",
      "Epoch 10, Batch 89, Loss: 1.068689227104187\n",
      "Epoch 10, Batch 90, Loss: 1.0805752277374268\n",
      "Epoch 10, Batch 91, Loss: 1.021638035774231\n",
      "Epoch 10, Batch 92, Loss: 1.1072361469268799\n",
      "Epoch 10, Batch 93, Loss: 1.063541293144226\n",
      "Epoch 11, Batch 0, Loss: 1.0390757322311401\n",
      "Epoch 11, Batch 1, Loss: 1.0780465602874756\n",
      "Epoch 11, Batch 2, Loss: 1.0302135944366455\n",
      "Epoch 11, Batch 3, Loss: 1.0575255155563354\n",
      "Epoch 11, Batch 4, Loss: 0.9904314279556274\n",
      "Epoch 11, Batch 5, Loss: 1.0687142610549927\n",
      "Epoch 11, Batch 6, Loss: 1.0304508209228516\n",
      "Epoch 11, Batch 7, Loss: 1.0842257738113403\n",
      "Epoch 11, Batch 8, Loss: 1.0753690004348755\n",
      "Epoch 11, Batch 9, Loss: 1.0595184564590454\n",
      "Epoch 11, Batch 10, Loss: 1.0826565027236938\n",
      "Epoch 11, Batch 11, Loss: 1.0360440015792847\n",
      "Epoch 11, Batch 12, Loss: 1.067913293838501\n",
      "Epoch 11, Batch 13, Loss: 1.0512067079544067\n",
      "Epoch 11, Batch 14, Loss: 1.043123722076416\n",
      "Epoch 11, Batch 15, Loss: 1.0820574760437012\n",
      "Epoch 11, Batch 16, Loss: 1.0450464487075806\n",
      "Epoch 11, Batch 17, Loss: 1.0407301187515259\n",
      "Epoch 11, Batch 18, Loss: 1.0008505582809448\n",
      "Epoch 11, Batch 19, Loss: 1.0544233322143555\n",
      "Epoch 11, Batch 20, Loss: 1.0537621974945068\n",
      "Epoch 11, Batch 21, Loss: 1.0376005172729492\n",
      "Epoch 11, Batch 22, Loss: 0.9976210594177246\n",
      "Epoch 11, Batch 23, Loss: 1.0208420753479004\n",
      "Epoch 11, Batch 24, Loss: 1.0688890218734741\n",
      "Epoch 11, Batch 25, Loss: 1.0405622720718384\n",
      "Epoch 11, Batch 26, Loss: 1.009367823600769\n",
      "Epoch 11, Batch 27, Loss: 1.0882627964019775\n",
      "Epoch 11, Batch 28, Loss: 1.0203640460968018\n",
      "Epoch 11, Batch 29, Loss: 1.009915828704834\n",
      "Epoch 11, Batch 30, Loss: 1.0487319231033325\n",
      "Epoch 11, Batch 31, Loss: 1.0236704349517822\n",
      "Epoch 11, Batch 32, Loss: 1.054993987083435\n",
      "Epoch 11, Batch 33, Loss: 0.9891960024833679\n",
      "Epoch 11, Batch 34, Loss: 1.0778920650482178\n",
      "Epoch 11, Batch 35, Loss: 1.049142599105835\n",
      "Epoch 11, Batch 36, Loss: 1.0723484754562378\n",
      "Epoch 11, Batch 37, Loss: 1.0433067083358765\n",
      "Epoch 11, Batch 38, Loss: 1.0305352210998535\n",
      "Epoch 11, Batch 39, Loss: 1.0221208333969116\n",
      "Epoch 11, Batch 40, Loss: 1.0151067972183228\n",
      "Epoch 11, Batch 41, Loss: 1.0568764209747314\n",
      "Epoch 11, Batch 42, Loss: 1.0161106586456299\n",
      "Epoch 11, Batch 43, Loss: 1.0355474948883057\n",
      "Epoch 11, Batch 44, Loss: 1.0431740283966064\n",
      "Epoch 11, Batch 45, Loss: 1.0164496898651123\n",
      "Epoch 11, Batch 46, Loss: 0.9610099792480469\n",
      "Epoch 11, Batch 47, Loss: 1.0239160060882568\n",
      "Epoch 11, Batch 48, Loss: 1.0317275524139404\n",
      "Epoch 11, Batch 49, Loss: 0.9914520382881165\n",
      "Epoch 11, Batch 50, Loss: 1.0079070329666138\n",
      "Epoch 11, Batch 51, Loss: 1.0594557523727417\n",
      "Epoch 11, Batch 52, Loss: 1.0478839874267578\n",
      "Epoch 11, Batch 53, Loss: 0.9903702735900879\n",
      "Epoch 11, Batch 54, Loss: 0.9795319437980652\n",
      "Epoch 11, Batch 55, Loss: 1.035352349281311\n",
      "Epoch 11, Batch 56, Loss: 1.0342134237289429\n",
      "Epoch 11, Batch 57, Loss: 1.0407130718231201\n",
      "Epoch 11, Batch 58, Loss: 1.044079303741455\n",
      "Epoch 11, Batch 59, Loss: 0.9720765948295593\n",
      "Epoch 11, Batch 60, Loss: 1.011725902557373\n",
      "Epoch 11, Batch 61, Loss: 1.0349478721618652\n",
      "Epoch 11, Batch 62, Loss: 1.0026849508285522\n",
      "Epoch 11, Batch 63, Loss: 1.0488505363464355\n",
      "Epoch 11, Batch 64, Loss: 1.0351548194885254\n",
      "Epoch 11, Batch 65, Loss: 1.0300980806350708\n",
      "Epoch 11, Batch 66, Loss: 1.025315284729004\n",
      "Epoch 11, Batch 67, Loss: 0.9882806539535522\n",
      "Epoch 11, Batch 68, Loss: 1.0284769535064697\n",
      "Epoch 11, Batch 69, Loss: 0.9515358805656433\n",
      "Epoch 11, Batch 70, Loss: 1.0292885303497314\n",
      "Epoch 11, Batch 71, Loss: 0.9972571134567261\n",
      "Epoch 11, Batch 72, Loss: 1.015889286994934\n",
      "Epoch 11, Batch 73, Loss: 0.9995709657669067\n",
      "Epoch 11, Batch 74, Loss: 0.9996665716171265\n",
      "Epoch 11, Batch 75, Loss: 0.9510918855667114\n",
      "Epoch 11, Batch 76, Loss: 0.9941684603691101\n",
      "Epoch 11, Batch 77, Loss: 0.9774119257926941\n",
      "Epoch 11, Batch 78, Loss: 1.0165151357650757\n",
      "Epoch 11, Batch 79, Loss: 0.9739762544631958\n",
      "Epoch 11, Batch 80, Loss: 0.9474336504936218\n",
      "Epoch 11, Batch 81, Loss: 0.9906601905822754\n",
      "Epoch 11, Batch 82, Loss: 0.9961383938789368\n",
      "Epoch 11, Batch 83, Loss: 1.0103169679641724\n",
      "Epoch 11, Batch 84, Loss: 0.9728901982307434\n",
      "Epoch 11, Batch 85, Loss: 1.006133794784546\n",
      "Epoch 11, Batch 86, Loss: 0.9578697085380554\n",
      "Epoch 11, Batch 87, Loss: 0.958438515663147\n",
      "Epoch 11, Batch 88, Loss: 0.9419229626655579\n",
      "Epoch 11, Batch 89, Loss: 0.9744706153869629\n",
      "Epoch 11, Batch 90, Loss: 0.9965518116950989\n",
      "Epoch 11, Batch 91, Loss: 0.9535802602767944\n",
      "Epoch 11, Batch 92, Loss: 0.9645286798477173\n",
      "Epoch 11, Batch 93, Loss: 1.003630518913269\n",
      "Epoch 12, Batch 0, Loss: 1.0152312517166138\n",
      "Epoch 12, Batch 1, Loss: 1.0231921672821045\n",
      "Epoch 12, Batch 2, Loss: 1.0024139881134033\n",
      "Epoch 12, Batch 3, Loss: 0.9863898158073425\n",
      "Epoch 12, Batch 4, Loss: 1.0195491313934326\n",
      "Epoch 12, Batch 5, Loss: 0.9788745045661926\n",
      "Epoch 12, Batch 6, Loss: 1.000218152999878\n",
      "Epoch 12, Batch 7, Loss: 0.9575600624084473\n",
      "Epoch 12, Batch 8, Loss: 1.0215994119644165\n",
      "Epoch 12, Batch 9, Loss: 0.9496804475784302\n",
      "Epoch 12, Batch 10, Loss: 1.0002143383026123\n",
      "Epoch 12, Batch 11, Loss: 1.0191469192504883\n",
      "Epoch 12, Batch 12, Loss: 0.9582595825195312\n",
      "Epoch 12, Batch 13, Loss: 1.014394998550415\n",
      "Epoch 12, Batch 14, Loss: 0.9657561182975769\n",
      "Epoch 12, Batch 15, Loss: 0.9848834276199341\n",
      "Epoch 12, Batch 16, Loss: 0.9533065557479858\n",
      "Epoch 12, Batch 17, Loss: 0.9354402422904968\n",
      "Epoch 12, Batch 18, Loss: 0.9549311399459839\n",
      "Epoch 12, Batch 19, Loss: 0.9594329595565796\n",
      "Epoch 12, Batch 20, Loss: 0.963545024394989\n",
      "Epoch 12, Batch 21, Loss: 0.9531406164169312\n",
      "Epoch 12, Batch 22, Loss: 0.9999049305915833\n",
      "Epoch 12, Batch 23, Loss: 0.971982479095459\n",
      "Epoch 12, Batch 24, Loss: 0.9466641545295715\n",
      "Epoch 12, Batch 25, Loss: 0.9372766613960266\n",
      "Epoch 12, Batch 26, Loss: 0.9838024973869324\n",
      "Epoch 12, Batch 27, Loss: 0.9979426264762878\n",
      "Epoch 12, Batch 28, Loss: 0.9653259515762329\n",
      "Epoch 12, Batch 29, Loss: 0.9608057141304016\n",
      "Epoch 12, Batch 30, Loss: 0.9435118436813354\n",
      "Epoch 12, Batch 31, Loss: 0.9364181756973267\n",
      "Epoch 12, Batch 32, Loss: 0.977700412273407\n",
      "Epoch 12, Batch 33, Loss: 0.9724937677383423\n",
      "Epoch 12, Batch 34, Loss: 0.9607399702072144\n",
      "Epoch 12, Batch 35, Loss: 0.9217530488967896\n",
      "Epoch 12, Batch 36, Loss: 0.9362156987190247\n",
      "Epoch 12, Batch 37, Loss: 0.9677373170852661\n",
      "Epoch 12, Batch 38, Loss: 1.0029398202896118\n",
      "Epoch 12, Batch 39, Loss: 0.9780467748641968\n",
      "Epoch 12, Batch 40, Loss: 0.9382113218307495\n",
      "Epoch 12, Batch 41, Loss: 0.9751777648925781\n",
      "Epoch 12, Batch 42, Loss: 0.9699281454086304\n",
      "Epoch 12, Batch 43, Loss: 0.9910313487052917\n",
      "Epoch 12, Batch 44, Loss: 0.9387507438659668\n",
      "Epoch 12, Batch 45, Loss: 0.9683786630630493\n",
      "Epoch 12, Batch 46, Loss: 1.0014259815216064\n",
      "Epoch 12, Batch 47, Loss: 0.975929856300354\n",
      "Epoch 12, Batch 48, Loss: 0.9944584965705872\n",
      "Epoch 12, Batch 49, Loss: 0.9646078944206238\n",
      "Epoch 12, Batch 50, Loss: 0.9203133583068848\n",
      "Epoch 12, Batch 51, Loss: 0.9459131360054016\n",
      "Epoch 12, Batch 52, Loss: 0.9487388730049133\n",
      "Epoch 12, Batch 53, Loss: 0.9137200117111206\n",
      "Epoch 12, Batch 54, Loss: 0.9335938692092896\n",
      "Epoch 12, Batch 55, Loss: 0.9749106168746948\n",
      "Epoch 12, Batch 56, Loss: 0.9349665641784668\n",
      "Epoch 12, Batch 57, Loss: 0.9278662800788879\n",
      "Epoch 12, Batch 58, Loss: 0.9411677122116089\n",
      "Epoch 12, Batch 59, Loss: 0.9606624841690063\n",
      "Epoch 12, Batch 60, Loss: 0.9429255723953247\n",
      "Epoch 12, Batch 61, Loss: 0.9492777585983276\n",
      "Epoch 12, Batch 62, Loss: 0.9494034051895142\n",
      "Epoch 12, Batch 63, Loss: 0.9457142949104309\n",
      "Epoch 12, Batch 64, Loss: 0.9341519474983215\n",
      "Epoch 12, Batch 65, Loss: 0.9252249002456665\n",
      "Epoch 12, Batch 66, Loss: 0.929959774017334\n",
      "Epoch 12, Batch 67, Loss: 0.9608453512191772\n",
      "Epoch 12, Batch 68, Loss: 0.9428035616874695\n",
      "Epoch 12, Batch 69, Loss: 0.9685794711112976\n",
      "Epoch 12, Batch 70, Loss: 0.9523093104362488\n",
      "Epoch 12, Batch 71, Loss: 0.9636598825454712\n",
      "Epoch 12, Batch 72, Loss: 0.9754388928413391\n",
      "Epoch 12, Batch 73, Loss: 0.9548943638801575\n",
      "Epoch 12, Batch 74, Loss: 0.9601410031318665\n",
      "Epoch 12, Batch 75, Loss: 0.9782626032829285\n",
      "Epoch 12, Batch 76, Loss: 0.961783766746521\n",
      "Epoch 12, Batch 77, Loss: 0.9780614972114563\n",
      "Epoch 12, Batch 78, Loss: 0.898841381072998\n",
      "Epoch 12, Batch 79, Loss: 0.9604231715202332\n",
      "Epoch 12, Batch 80, Loss: 0.9346505999565125\n",
      "Epoch 12, Batch 81, Loss: 0.9627124667167664\n",
      "Epoch 12, Batch 82, Loss: 0.9358096122741699\n",
      "Epoch 12, Batch 83, Loss: 0.9239657521247864\n",
      "Epoch 12, Batch 84, Loss: 0.9381667375564575\n",
      "Epoch 12, Batch 85, Loss: 0.8946253061294556\n",
      "Epoch 12, Batch 86, Loss: 0.933696448802948\n",
      "Epoch 12, Batch 87, Loss: 0.9095504879951477\n",
      "Epoch 12, Batch 88, Loss: 0.9305028915405273\n",
      "Epoch 12, Batch 89, Loss: 0.9204848408699036\n",
      "Epoch 12, Batch 90, Loss: 0.9169800877571106\n",
      "Epoch 12, Batch 91, Loss: 0.930284321308136\n",
      "Epoch 12, Batch 92, Loss: 0.9434181451797485\n",
      "Epoch 12, Batch 93, Loss: 0.9860810041427612\n",
      "Epoch 13, Batch 0, Loss: 0.9234142303466797\n",
      "Epoch 13, Batch 1, Loss: 0.9410870671272278\n",
      "Epoch 13, Batch 2, Loss: 0.9231498837471008\n",
      "Epoch 13, Batch 3, Loss: 0.9306852221488953\n",
      "Epoch 13, Batch 4, Loss: 0.9092528223991394\n",
      "Epoch 13, Batch 5, Loss: 0.9149361848831177\n",
      "Epoch 13, Batch 6, Loss: 0.9221893548965454\n",
      "Epoch 13, Batch 7, Loss: 0.9034260511398315\n",
      "Epoch 13, Batch 8, Loss: 0.9474361538887024\n",
      "Epoch 13, Batch 9, Loss: 0.941700279712677\n",
      "Epoch 13, Batch 10, Loss: 0.9553743600845337\n",
      "Epoch 13, Batch 11, Loss: 0.9500259160995483\n",
      "Epoch 13, Batch 12, Loss: 0.9472257494926453\n",
      "Epoch 13, Batch 13, Loss: 0.9096722602844238\n",
      "Epoch 13, Batch 14, Loss: 0.9334558248519897\n",
      "Epoch 13, Batch 15, Loss: 0.9658963084220886\n",
      "Epoch 13, Batch 16, Loss: 0.9407373666763306\n",
      "Epoch 13, Batch 17, Loss: 0.9490228891372681\n",
      "Epoch 13, Batch 18, Loss: 0.915585994720459\n",
      "Epoch 13, Batch 19, Loss: 0.9294942617416382\n",
      "Epoch 13, Batch 20, Loss: 0.8911811113357544\n",
      "Epoch 13, Batch 21, Loss: 0.9208360910415649\n",
      "Epoch 13, Batch 22, Loss: 0.9669303894042969\n",
      "Epoch 13, Batch 23, Loss: 0.9111024737358093\n",
      "Epoch 13, Batch 24, Loss: 0.9428328275680542\n",
      "Epoch 13, Batch 25, Loss: 0.8954349756240845\n",
      "Epoch 13, Batch 26, Loss: 0.905534565448761\n",
      "Epoch 13, Batch 27, Loss: 0.9302495718002319\n",
      "Epoch 13, Batch 28, Loss: 0.9197295904159546\n",
      "Epoch 13, Batch 29, Loss: 0.9521335363388062\n",
      "Epoch 13, Batch 30, Loss: 0.933791995048523\n",
      "Epoch 13, Batch 31, Loss: 0.8887178301811218\n",
      "Epoch 13, Batch 32, Loss: 0.9315388798713684\n",
      "Epoch 13, Batch 33, Loss: 0.9178024530410767\n",
      "Epoch 13, Batch 34, Loss: 0.9512609243392944\n",
      "Epoch 13, Batch 35, Loss: 0.9402579069137573\n",
      "Epoch 13, Batch 36, Loss: 0.9494377970695496\n",
      "Epoch 13, Batch 37, Loss: 0.8998158574104309\n",
      "Epoch 13, Batch 38, Loss: 0.8698484301567078\n",
      "Epoch 13, Batch 39, Loss: 0.9281666874885559\n",
      "Epoch 13, Batch 40, Loss: 0.9025020599365234\n",
      "Epoch 13, Batch 41, Loss: 0.9304901361465454\n",
      "Epoch 13, Batch 42, Loss: 0.8754838109016418\n",
      "Epoch 13, Batch 43, Loss: 0.926145076751709\n",
      "Epoch 13, Batch 44, Loss: 0.914071261882782\n",
      "Epoch 13, Batch 45, Loss: 0.8969250917434692\n",
      "Epoch 13, Batch 46, Loss: 0.924301028251648\n",
      "Epoch 13, Batch 47, Loss: 0.8659208416938782\n",
      "Epoch 13, Batch 48, Loss: 0.9042288661003113\n",
      "Epoch 13, Batch 49, Loss: 0.9228070974349976\n",
      "Epoch 13, Batch 50, Loss: 0.9091532826423645\n",
      "Epoch 13, Batch 51, Loss: 0.9131771922111511\n",
      "Epoch 13, Batch 52, Loss: 0.8853112459182739\n",
      "Epoch 13, Batch 53, Loss: 0.8821872472763062\n",
      "Epoch 13, Batch 54, Loss: 0.8904207348823547\n",
      "Epoch 13, Batch 55, Loss: 0.9002509117126465\n",
      "Epoch 13, Batch 56, Loss: 0.9036208987236023\n",
      "Epoch 13, Batch 57, Loss: 0.9254342317581177\n",
      "Epoch 13, Batch 58, Loss: 0.8997446894645691\n",
      "Epoch 13, Batch 59, Loss: 0.8544699549674988\n",
      "Epoch 13, Batch 60, Loss: 0.8464958071708679\n",
      "Epoch 13, Batch 61, Loss: 0.8816944360733032\n",
      "Epoch 13, Batch 62, Loss: 0.9274176359176636\n",
      "Epoch 13, Batch 63, Loss: 0.9229199290275574\n",
      "Epoch 13, Batch 64, Loss: 0.9168230891227722\n",
      "Epoch 13, Batch 65, Loss: 0.8946240544319153\n",
      "Epoch 13, Batch 66, Loss: 0.8406936526298523\n",
      "Epoch 13, Batch 67, Loss: 0.8608239889144897\n",
      "Epoch 13, Batch 68, Loss: 0.929294764995575\n",
      "Epoch 13, Batch 69, Loss: 0.899093747138977\n",
      "Epoch 13, Batch 70, Loss: 0.881127655506134\n",
      "Epoch 13, Batch 71, Loss: 0.891283392906189\n",
      "Epoch 13, Batch 72, Loss: 0.8747005462646484\n",
      "Epoch 13, Batch 73, Loss: 0.901379406452179\n",
      "Epoch 13, Batch 74, Loss: 0.8513171076774597\n",
      "Epoch 13, Batch 75, Loss: 0.8742974996566772\n",
      "Epoch 13, Batch 76, Loss: 0.8570716977119446\n",
      "Epoch 13, Batch 77, Loss: 0.9095527529716492\n",
      "Epoch 13, Batch 78, Loss: 0.8611238598823547\n",
      "Epoch 13, Batch 79, Loss: 0.8664711117744446\n",
      "Epoch 13, Batch 80, Loss: 0.8740686178207397\n",
      "Epoch 13, Batch 81, Loss: 0.8968185186386108\n",
      "Epoch 13, Batch 82, Loss: 0.9003854990005493\n",
      "Epoch 13, Batch 83, Loss: 0.890089213848114\n",
      "Epoch 13, Batch 84, Loss: 0.8596340417861938\n",
      "Epoch 13, Batch 85, Loss: 0.9048064947128296\n",
      "Epoch 13, Batch 86, Loss: 0.8614073991775513\n",
      "Epoch 13, Batch 87, Loss: 0.8985894918441772\n",
      "Epoch 13, Batch 88, Loss: 0.844424843788147\n",
      "Epoch 13, Batch 89, Loss: 0.8985882997512817\n",
      "Epoch 13, Batch 90, Loss: 0.8894761204719543\n",
      "Epoch 13, Batch 91, Loss: 0.8738028407096863\n",
      "Epoch 13, Batch 92, Loss: 0.8393797874450684\n",
      "Epoch 13, Batch 93, Loss: 0.891424834728241\n",
      "Epoch 14, Batch 0, Loss: 0.9067398905754089\n",
      "Epoch 14, Batch 1, Loss: 0.8360828161239624\n",
      "Epoch 14, Batch 2, Loss: 0.8925240635871887\n",
      "Epoch 14, Batch 3, Loss: 0.8986859321594238\n",
      "Epoch 14, Batch 4, Loss: 0.9322509765625\n",
      "Epoch 14, Batch 5, Loss: 0.8499941825866699\n",
      "Epoch 14, Batch 6, Loss: 0.888982892036438\n",
      "Epoch 14, Batch 7, Loss: 0.8512808680534363\n",
      "Epoch 14, Batch 8, Loss: 0.8699514269828796\n",
      "Epoch 14, Batch 9, Loss: 0.8938530683517456\n",
      "Epoch 14, Batch 10, Loss: 0.8610440492630005\n",
      "Epoch 14, Batch 11, Loss: 0.8441122174263\n",
      "Epoch 14, Batch 12, Loss: 0.877245306968689\n",
      "Epoch 14, Batch 13, Loss: 0.8985525369644165\n",
      "Epoch 14, Batch 14, Loss: 0.8721176385879517\n",
      "Epoch 14, Batch 15, Loss: 0.8982022404670715\n",
      "Epoch 14, Batch 16, Loss: 0.8598877191543579\n",
      "Epoch 14, Batch 17, Loss: 0.8769140243530273\n",
      "Epoch 14, Batch 18, Loss: 0.8590124845504761\n",
      "Epoch 14, Batch 19, Loss: 0.895845890045166\n",
      "Epoch 14, Batch 20, Loss: 0.8722460865974426\n",
      "Epoch 14, Batch 21, Loss: 0.8992866277694702\n",
      "Epoch 14, Batch 22, Loss: 0.8598145246505737\n",
      "Epoch 14, Batch 23, Loss: 0.8672757148742676\n",
      "Epoch 14, Batch 24, Loss: 0.8560734987258911\n",
      "Epoch 14, Batch 25, Loss: 0.8538772463798523\n",
      "Epoch 14, Batch 26, Loss: 0.8643204569816589\n",
      "Epoch 14, Batch 27, Loss: 0.8659831881523132\n",
      "Epoch 14, Batch 28, Loss: 0.8630863428115845\n",
      "Epoch 14, Batch 29, Loss: 0.8388575315475464\n",
      "Epoch 14, Batch 30, Loss: 0.836747944355011\n",
      "Epoch 14, Batch 31, Loss: 0.8589798212051392\n",
      "Epoch 14, Batch 32, Loss: 0.8708959817886353\n",
      "Epoch 14, Batch 33, Loss: 0.862393856048584\n",
      "Epoch 14, Batch 34, Loss: 0.8943661451339722\n",
      "Epoch 14, Batch 35, Loss: 0.8975929021835327\n",
      "Epoch 14, Batch 36, Loss: 0.8244304656982422\n",
      "Epoch 14, Batch 37, Loss: 0.8351150751113892\n",
      "Epoch 14, Batch 38, Loss: 0.8470730781555176\n",
      "Epoch 14, Batch 39, Loss: 0.8796972036361694\n",
      "Epoch 14, Batch 40, Loss: 0.844057559967041\n",
      "Epoch 14, Batch 41, Loss: 0.8786638975143433\n",
      "Epoch 14, Batch 42, Loss: 0.8400275111198425\n",
      "Epoch 14, Batch 43, Loss: 0.8501402139663696\n",
      "Epoch 14, Batch 44, Loss: 0.8615587949752808\n",
      "Epoch 14, Batch 45, Loss: 0.8983651399612427\n",
      "Epoch 14, Batch 46, Loss: 0.8986851572990417\n",
      "Epoch 14, Batch 47, Loss: 0.8575650453567505\n",
      "Epoch 14, Batch 48, Loss: 0.8910934329032898\n",
      "Epoch 14, Batch 49, Loss: 0.8575789332389832\n",
      "Epoch 14, Batch 50, Loss: 0.8686311841011047\n",
      "Epoch 14, Batch 51, Loss: 0.8861414194107056\n",
      "Epoch 14, Batch 52, Loss: 0.8817283511161804\n",
      "Epoch 14, Batch 53, Loss: 0.8444797396659851\n",
      "Epoch 14, Batch 54, Loss: 0.8615519404411316\n",
      "Epoch 14, Batch 55, Loss: 0.8539997339248657\n",
      "Epoch 14, Batch 56, Loss: 0.8577038049697876\n",
      "Epoch 14, Batch 57, Loss: 0.8016977310180664\n",
      "Epoch 14, Batch 58, Loss: 0.8500415086746216\n",
      "Epoch 14, Batch 59, Loss: 0.8423774838447571\n",
      "Epoch 14, Batch 60, Loss: 0.9105297923088074\n",
      "Epoch 14, Batch 61, Loss: 0.8313952684402466\n",
      "Epoch 14, Batch 62, Loss: 0.809852123260498\n",
      "Epoch 14, Batch 63, Loss: 0.851675808429718\n",
      "Epoch 14, Batch 64, Loss: 0.8229818344116211\n",
      "Epoch 14, Batch 65, Loss: 0.8581178784370422\n",
      "Epoch 14, Batch 66, Loss: 0.8314569592475891\n",
      "Epoch 14, Batch 67, Loss: 0.7898848056793213\n",
      "Epoch 14, Batch 68, Loss: 0.8754584193229675\n",
      "Epoch 14, Batch 69, Loss: 0.8830563426017761\n",
      "Epoch 14, Batch 70, Loss: 0.8670111894607544\n",
      "Epoch 14, Batch 71, Loss: 0.8488823771476746\n",
      "Epoch 14, Batch 72, Loss: 0.8377845883369446\n",
      "Epoch 14, Batch 73, Loss: 0.8476799130439758\n",
      "Epoch 14, Batch 74, Loss: 0.8918965458869934\n",
      "Epoch 14, Batch 75, Loss: 0.8569005727767944\n",
      "Epoch 14, Batch 76, Loss: 0.7942456603050232\n",
      "Epoch 14, Batch 77, Loss: 0.8149174451828003\n",
      "Epoch 14, Batch 78, Loss: 0.8535412549972534\n",
      "Epoch 14, Batch 79, Loss: 0.8489586114883423\n",
      "Epoch 14, Batch 80, Loss: 0.8415135145187378\n",
      "Epoch 14, Batch 81, Loss: 0.8603955507278442\n",
      "Epoch 14, Batch 82, Loss: 0.8486353158950806\n",
      "Epoch 14, Batch 83, Loss: 0.8569748997688293\n",
      "Epoch 14, Batch 84, Loss: 0.788074254989624\n",
      "Epoch 14, Batch 85, Loss: 0.8500156402587891\n",
      "Epoch 14, Batch 86, Loss: 0.8657679557800293\n",
      "Epoch 14, Batch 87, Loss: 0.8586182594299316\n",
      "Epoch 14, Batch 88, Loss: 0.8034846186637878\n",
      "Epoch 14, Batch 89, Loss: 0.8418126106262207\n",
      "Epoch 14, Batch 90, Loss: 0.8261387944221497\n",
      "Epoch 14, Batch 91, Loss: 0.8529728651046753\n",
      "Epoch 14, Batch 92, Loss: 0.8424914479255676\n",
      "Epoch 14, Batch 93, Loss: 0.8236905932426453\n",
      "Epoch 15, Batch 0, Loss: 0.8212546110153198\n",
      "Epoch 15, Batch 1, Loss: 0.8312951326370239\n",
      "Epoch 15, Batch 2, Loss: 0.8411911725997925\n",
      "Epoch 15, Batch 3, Loss: 0.8322831988334656\n",
      "Epoch 15, Batch 4, Loss: 0.850445568561554\n",
      "Epoch 15, Batch 5, Loss: 0.822222113609314\n",
      "Epoch 15, Batch 6, Loss: 0.8718968629837036\n",
      "Epoch 15, Batch 7, Loss: 0.8562036752700806\n",
      "Epoch 15, Batch 8, Loss: 0.8328835368156433\n",
      "Epoch 15, Batch 9, Loss: 0.8541658520698547\n",
      "Epoch 15, Batch 10, Loss: 0.8525137901306152\n",
      "Epoch 15, Batch 11, Loss: 0.7844656109809875\n",
      "Epoch 15, Batch 12, Loss: 0.78996741771698\n",
      "Epoch 15, Batch 13, Loss: 0.8299078941345215\n",
      "Epoch 15, Batch 14, Loss: 0.8369669914245605\n",
      "Epoch 15, Batch 15, Loss: 0.8306866884231567\n",
      "Epoch 15, Batch 16, Loss: 0.8166882395744324\n",
      "Epoch 15, Batch 17, Loss: 0.8436883687973022\n",
      "Epoch 15, Batch 18, Loss: 0.8181071281433105\n",
      "Epoch 15, Batch 19, Loss: 0.8245224952697754\n",
      "Epoch 15, Batch 20, Loss: 0.830506443977356\n",
      "Epoch 15, Batch 21, Loss: 0.8245951533317566\n",
      "Epoch 15, Batch 22, Loss: 0.8194605708122253\n",
      "Epoch 15, Batch 23, Loss: 0.8860001564025879\n",
      "Epoch 15, Batch 24, Loss: 0.8510233163833618\n",
      "Epoch 15, Batch 25, Loss: 0.8058726191520691\n",
      "Epoch 15, Batch 26, Loss: 0.8206824064254761\n",
      "Epoch 15, Batch 27, Loss: 0.8412450551986694\n",
      "Epoch 15, Batch 28, Loss: 0.7727949023246765\n",
      "Epoch 15, Batch 29, Loss: 0.8252050280570984\n",
      "Epoch 15, Batch 30, Loss: 0.8141734004020691\n",
      "Epoch 15, Batch 31, Loss: 0.8487640619277954\n",
      "Epoch 15, Batch 32, Loss: 0.8459898829460144\n",
      "Epoch 15, Batch 33, Loss: 0.7980316877365112\n",
      "Epoch 15, Batch 34, Loss: 0.8280247449874878\n",
      "Epoch 15, Batch 35, Loss: 0.8099675178527832\n",
      "Epoch 15, Batch 36, Loss: 0.8139058947563171\n",
      "Epoch 15, Batch 37, Loss: 0.8031826019287109\n",
      "Epoch 15, Batch 38, Loss: 0.8370344042778015\n",
      "Epoch 15, Batch 39, Loss: 0.8523443341255188\n",
      "Epoch 15, Batch 40, Loss: 0.79795902967453\n",
      "Epoch 15, Batch 41, Loss: 0.8389673233032227\n",
      "Epoch 15, Batch 42, Loss: 0.7924172282218933\n",
      "Epoch 15, Batch 43, Loss: 0.8043462634086609\n",
      "Epoch 15, Batch 44, Loss: 0.832770824432373\n",
      "Epoch 15, Batch 45, Loss: 0.8199548721313477\n",
      "Epoch 15, Batch 46, Loss: 0.8273369073867798\n",
      "Epoch 15, Batch 47, Loss: 0.8030717968940735\n",
      "Epoch 15, Batch 48, Loss: 0.7999913096427917\n",
      "Epoch 15, Batch 49, Loss: 0.7851050496101379\n",
      "Epoch 15, Batch 50, Loss: 0.8326925039291382\n",
      "Epoch 15, Batch 51, Loss: 0.8357585072517395\n",
      "Epoch 15, Batch 52, Loss: 0.8156391978263855\n",
      "Epoch 15, Batch 53, Loss: 0.7682958841323853\n",
      "Epoch 15, Batch 54, Loss: 0.7966418862342834\n",
      "Epoch 15, Batch 55, Loss: 0.7974007725715637\n",
      "Epoch 15, Batch 56, Loss: 0.8052215576171875\n",
      "Epoch 15, Batch 57, Loss: 0.8228089213371277\n",
      "Epoch 15, Batch 58, Loss: 0.8089138269424438\n",
      "Epoch 15, Batch 59, Loss: 0.8281779289245605\n",
      "Epoch 15, Batch 60, Loss: 0.8359392285346985\n",
      "Epoch 15, Batch 61, Loss: 0.8349210023880005\n",
      "Epoch 15, Batch 62, Loss: 0.8725727796554565\n",
      "Epoch 15, Batch 63, Loss: 0.7856757044792175\n",
      "Epoch 15, Batch 64, Loss: 0.7928476333618164\n",
      "Epoch 15, Batch 65, Loss: 0.823485255241394\n",
      "Epoch 15, Batch 66, Loss: 0.773107647895813\n",
      "Epoch 15, Batch 67, Loss: 0.7642117738723755\n",
      "Epoch 15, Batch 68, Loss: 0.7792217135429382\n",
      "Epoch 15, Batch 69, Loss: 0.8162020444869995\n",
      "Epoch 15, Batch 70, Loss: 0.840984046459198\n",
      "Epoch 15, Batch 71, Loss: 0.7783517241477966\n",
      "Epoch 15, Batch 72, Loss: 0.7928615808486938\n",
      "Epoch 15, Batch 73, Loss: 0.7971970438957214\n",
      "Epoch 15, Batch 74, Loss: 0.7980281114578247\n",
      "Epoch 15, Batch 75, Loss: 0.7837480902671814\n",
      "Epoch 15, Batch 76, Loss: 0.7855690121650696\n",
      "Epoch 15, Batch 77, Loss: 0.7516023516654968\n",
      "Epoch 15, Batch 78, Loss: 0.7929849028587341\n",
      "Epoch 15, Batch 79, Loss: 0.8423676490783691\n",
      "Epoch 15, Batch 80, Loss: 0.8118036389350891\n",
      "Epoch 15, Batch 81, Loss: 0.8079899549484253\n",
      "Epoch 15, Batch 82, Loss: 0.8228676915168762\n",
      "Epoch 15, Batch 83, Loss: 0.7959712743759155\n",
      "Epoch 15, Batch 84, Loss: 0.852273166179657\n",
      "Epoch 15, Batch 85, Loss: 0.8285086750984192\n",
      "Epoch 15, Batch 86, Loss: 0.8191548585891724\n",
      "Epoch 15, Batch 87, Loss: 0.8290578126907349\n",
      "Epoch 15, Batch 88, Loss: 0.7783493399620056\n",
      "Epoch 15, Batch 89, Loss: 0.7933706045150757\n",
      "Epoch 15, Batch 90, Loss: 0.8454368710517883\n",
      "Epoch 15, Batch 91, Loss: 0.8185497522354126\n",
      "Epoch 15, Batch 92, Loss: 0.8364171981811523\n",
      "Epoch 15, Batch 93, Loss: 0.7965589761734009\n",
      "Epoch 16, Batch 0, Loss: 0.7519257068634033\n",
      "Epoch 16, Batch 1, Loss: 0.7725952863693237\n",
      "Epoch 16, Batch 2, Loss: 0.8155645132064819\n",
      "Epoch 16, Batch 3, Loss: 0.8277069330215454\n",
      "Epoch 16, Batch 4, Loss: 0.8469146490097046\n",
      "Epoch 16, Batch 5, Loss: 0.7666587233543396\n",
      "Epoch 16, Batch 6, Loss: 0.806587815284729\n",
      "Epoch 16, Batch 7, Loss: 0.7899966835975647\n",
      "Epoch 16, Batch 8, Loss: 0.7860721349716187\n",
      "Epoch 16, Batch 9, Loss: 0.7585693001747131\n",
      "Epoch 16, Batch 10, Loss: 0.7660159468650818\n",
      "Epoch 16, Batch 11, Loss: 0.8036288022994995\n",
      "Epoch 16, Batch 12, Loss: 0.7910250425338745\n",
      "Epoch 16, Batch 13, Loss: 0.7880088090896606\n",
      "Epoch 16, Batch 14, Loss: 0.8063901662826538\n",
      "Epoch 16, Batch 15, Loss: 0.8353970646858215\n",
      "Epoch 16, Batch 16, Loss: 0.8065816760063171\n",
      "Epoch 16, Batch 17, Loss: 0.8068878054618835\n",
      "Epoch 16, Batch 18, Loss: 0.7624142169952393\n",
      "Epoch 16, Batch 19, Loss: 0.7961952090263367\n",
      "Epoch 16, Batch 20, Loss: 0.8114169836044312\n",
      "Epoch 16, Batch 21, Loss: 0.7593775391578674\n",
      "Epoch 16, Batch 22, Loss: 0.7844767570495605\n",
      "Epoch 16, Batch 23, Loss: 0.7617312073707581\n",
      "Epoch 16, Batch 24, Loss: 0.808774471282959\n",
      "Epoch 16, Batch 25, Loss: 0.8174616694450378\n",
      "Epoch 16, Batch 26, Loss: 0.7751436233520508\n",
      "Epoch 16, Batch 27, Loss: 0.7306405901908875\n",
      "Epoch 16, Batch 28, Loss: 0.8167851567268372\n",
      "Epoch 16, Batch 29, Loss: 0.8226536512374878\n",
      "Epoch 16, Batch 30, Loss: 0.808874785900116\n",
      "Epoch 16, Batch 31, Loss: 0.7832382321357727\n",
      "Epoch 16, Batch 32, Loss: 0.8196935653686523\n",
      "Epoch 16, Batch 33, Loss: 0.7771939039230347\n",
      "Epoch 16, Batch 34, Loss: 0.7551999092102051\n",
      "Epoch 16, Batch 35, Loss: 0.8492283821105957\n",
      "Epoch 16, Batch 36, Loss: 0.7674500942230225\n",
      "Epoch 16, Batch 37, Loss: 0.7506577968597412\n",
      "Epoch 16, Batch 38, Loss: 0.7685158848762512\n",
      "Epoch 16, Batch 39, Loss: 0.8074904680252075\n",
      "Epoch 16, Batch 40, Loss: 0.7828749418258667\n",
      "Epoch 16, Batch 41, Loss: 0.7638472318649292\n",
      "Epoch 16, Batch 42, Loss: 0.8178218603134155\n",
      "Epoch 16, Batch 43, Loss: 0.7713528275489807\n",
      "Epoch 16, Batch 44, Loss: 0.7585121393203735\n",
      "Epoch 16, Batch 45, Loss: 0.7458177804946899\n",
      "Epoch 16, Batch 46, Loss: 0.8294841051101685\n",
      "Epoch 16, Batch 47, Loss: 0.7893956899642944\n",
      "Epoch 16, Batch 48, Loss: 0.8100441694259644\n",
      "Epoch 16, Batch 49, Loss: 0.7567356824874878\n",
      "Epoch 16, Batch 50, Loss: 0.7913903594017029\n",
      "Epoch 16, Batch 51, Loss: 0.7466378211975098\n",
      "Epoch 16, Batch 52, Loss: 0.7809440493583679\n",
      "Epoch 16, Batch 53, Loss: 0.7872565388679504\n",
      "Epoch 16, Batch 54, Loss: 0.8107243776321411\n",
      "Epoch 16, Batch 55, Loss: 0.79619961977005\n",
      "Epoch 16, Batch 56, Loss: 0.7945979833602905\n",
      "Epoch 16, Batch 57, Loss: 0.7819681167602539\n",
      "Epoch 16, Batch 58, Loss: 0.8053364753723145\n",
      "Epoch 16, Batch 59, Loss: 0.7670347094535828\n",
      "Epoch 16, Batch 60, Loss: 0.7957073450088501\n",
      "Epoch 16, Batch 61, Loss: 0.7914469242095947\n",
      "Epoch 16, Batch 62, Loss: 0.8093487024307251\n",
      "Epoch 16, Batch 63, Loss: 0.7744856476783752\n",
      "Epoch 16, Batch 64, Loss: 0.780117928981781\n",
      "Epoch 16, Batch 65, Loss: 0.7223891019821167\n",
      "Epoch 16, Batch 66, Loss: 0.8203598856925964\n",
      "Epoch 16, Batch 67, Loss: 0.7508260011672974\n",
      "Epoch 16, Batch 68, Loss: 0.8182922601699829\n",
      "Epoch 16, Batch 69, Loss: 0.7752895951271057\n",
      "Epoch 16, Batch 70, Loss: 0.7606916427612305\n",
      "Epoch 16, Batch 71, Loss: 0.7620059251785278\n",
      "Epoch 16, Batch 72, Loss: 0.7490308880805969\n",
      "Epoch 16, Batch 73, Loss: 0.7851628065109253\n",
      "Epoch 16, Batch 74, Loss: 0.7450041174888611\n",
      "Epoch 16, Batch 75, Loss: 0.7438896894454956\n",
      "Epoch 16, Batch 76, Loss: 0.7721998691558838\n",
      "Epoch 16, Batch 77, Loss: 0.7894944548606873\n",
      "Epoch 16, Batch 78, Loss: 0.7637184858322144\n",
      "Epoch 16, Batch 79, Loss: 0.780456006526947\n",
      "Epoch 16, Batch 80, Loss: 0.7127701044082642\n",
      "Epoch 16, Batch 81, Loss: 0.7559658288955688\n",
      "Epoch 16, Batch 82, Loss: 0.7631089687347412\n",
      "Epoch 16, Batch 83, Loss: 0.7600299715995789\n",
      "Epoch 16, Batch 84, Loss: 0.7548010349273682\n",
      "Epoch 16, Batch 85, Loss: 0.7492102980613708\n",
      "Epoch 16, Batch 86, Loss: 0.7759557366371155\n",
      "Epoch 16, Batch 87, Loss: 0.733386218547821\n",
      "Epoch 16, Batch 88, Loss: 0.7700071334838867\n",
      "Epoch 16, Batch 89, Loss: 0.783882737159729\n",
      "Epoch 16, Batch 90, Loss: 0.8005231022834778\n",
      "Epoch 16, Batch 91, Loss: 0.7060078382492065\n",
      "Epoch 16, Batch 92, Loss: 0.7879021167755127\n",
      "Epoch 16, Batch 93, Loss: 0.7644850015640259\n",
      "Epoch 17, Batch 0, Loss: 0.7570391893386841\n",
      "Epoch 17, Batch 1, Loss: 0.7393633127212524\n",
      "Epoch 17, Batch 2, Loss: 0.7687934637069702\n",
      "Epoch 17, Batch 3, Loss: 0.7726251482963562\n",
      "Epoch 17, Batch 4, Loss: 0.8177593946456909\n",
      "Epoch 17, Batch 5, Loss: 0.75163733959198\n",
      "Epoch 17, Batch 6, Loss: 0.749347984790802\n",
      "Epoch 17, Batch 7, Loss: 0.7975353598594666\n",
      "Epoch 17, Batch 8, Loss: 0.7407665252685547\n",
      "Epoch 17, Batch 9, Loss: 0.7399395704269409\n",
      "Epoch 17, Batch 10, Loss: 0.7669071555137634\n",
      "Epoch 17, Batch 11, Loss: 0.7808840274810791\n",
      "Epoch 17, Batch 12, Loss: 0.6926557421684265\n",
      "Epoch 17, Batch 13, Loss: 0.7420638799667358\n",
      "Epoch 17, Batch 14, Loss: 0.8059049844741821\n",
      "Epoch 17, Batch 15, Loss: 0.7502011060714722\n",
      "Epoch 17, Batch 16, Loss: 0.7232891321182251\n",
      "Epoch 17, Batch 17, Loss: 0.773503303527832\n",
      "Epoch 17, Batch 18, Loss: 0.7643158435821533\n",
      "Epoch 17, Batch 19, Loss: 0.7569476366043091\n",
      "Epoch 17, Batch 20, Loss: 0.7829973101615906\n",
      "Epoch 17, Batch 21, Loss: 0.7513120174407959\n",
      "Epoch 17, Batch 22, Loss: 0.8049026727676392\n",
      "Epoch 17, Batch 23, Loss: 0.7432680726051331\n",
      "Epoch 17, Batch 24, Loss: 0.7238564491271973\n",
      "Epoch 17, Batch 25, Loss: 0.7335177659988403\n",
      "Epoch 17, Batch 26, Loss: 0.7704157829284668\n",
      "Epoch 17, Batch 27, Loss: 0.775005042552948\n",
      "Epoch 17, Batch 28, Loss: 0.7794820070266724\n",
      "Epoch 17, Batch 29, Loss: 0.721724271774292\n",
      "Epoch 17, Batch 30, Loss: 0.7533242702484131\n",
      "Epoch 17, Batch 31, Loss: 0.7667233943939209\n",
      "Epoch 17, Batch 32, Loss: 0.7633923292160034\n",
      "Epoch 17, Batch 33, Loss: 0.7317461371421814\n",
      "Epoch 17, Batch 34, Loss: 0.7502756714820862\n",
      "Epoch 17, Batch 35, Loss: 0.7772216796875\n",
      "Epoch 17, Batch 36, Loss: 0.7380806803703308\n",
      "Epoch 17, Batch 37, Loss: 0.7907770872116089\n",
      "Epoch 17, Batch 38, Loss: 0.7088643312454224\n",
      "Epoch 17, Batch 39, Loss: 0.7634369134902954\n",
      "Epoch 17, Batch 40, Loss: 0.7600671052932739\n",
      "Epoch 17, Batch 41, Loss: 0.7650131583213806\n",
      "Epoch 17, Batch 42, Loss: 0.749211311340332\n",
      "Epoch 17, Batch 43, Loss: 0.6935364603996277\n",
      "Epoch 17, Batch 44, Loss: 0.7473307847976685\n",
      "Epoch 17, Batch 45, Loss: 0.7753926515579224\n",
      "Epoch 17, Batch 46, Loss: 0.7664809226989746\n",
      "Epoch 17, Batch 47, Loss: 0.7291742563247681\n",
      "Epoch 17, Batch 48, Loss: 0.7469040155410767\n",
      "Epoch 17, Batch 49, Loss: 0.733640193939209\n",
      "Epoch 17, Batch 50, Loss: 0.7708321213722229\n",
      "Epoch 17, Batch 51, Loss: 0.7756755948066711\n",
      "Epoch 17, Batch 52, Loss: 0.7198828458786011\n",
      "Epoch 17, Batch 53, Loss: 0.7343488335609436\n",
      "Epoch 17, Batch 54, Loss: 0.7373610138893127\n",
      "Epoch 17, Batch 55, Loss: 0.7669473886489868\n",
      "Epoch 17, Batch 56, Loss: 0.7576552033424377\n",
      "Epoch 17, Batch 57, Loss: 0.7494869232177734\n",
      "Epoch 17, Batch 58, Loss: 0.7544808983802795\n",
      "Epoch 17, Batch 59, Loss: 0.7186933755874634\n",
      "Epoch 17, Batch 60, Loss: 0.723692774772644\n",
      "Epoch 17, Batch 61, Loss: 0.7549896836280823\n",
      "Epoch 17, Batch 62, Loss: 0.7557663917541504\n",
      "Epoch 17, Batch 63, Loss: 0.7841497659683228\n",
      "Epoch 17, Batch 64, Loss: 0.7675639390945435\n",
      "Epoch 17, Batch 65, Loss: 0.7339450716972351\n",
      "Epoch 17, Batch 66, Loss: 0.7338478565216064\n",
      "Epoch 17, Batch 67, Loss: 0.7174665331840515\n",
      "Epoch 17, Batch 68, Loss: 0.7363694310188293\n",
      "Epoch 17, Batch 69, Loss: 0.7569623589515686\n",
      "Epoch 17, Batch 70, Loss: 0.7151192426681519\n",
      "Epoch 17, Batch 71, Loss: 0.7393658757209778\n",
      "Epoch 17, Batch 72, Loss: 0.7587419748306274\n",
      "Epoch 17, Batch 73, Loss: 0.728006899356842\n",
      "Epoch 17, Batch 74, Loss: 0.7304700613021851\n",
      "Epoch 17, Batch 75, Loss: 0.7368811368942261\n",
      "Epoch 17, Batch 76, Loss: 0.7037737965583801\n",
      "Epoch 17, Batch 77, Loss: 0.7403059601783752\n",
      "Epoch 17, Batch 78, Loss: 0.748023271560669\n",
      "Epoch 17, Batch 79, Loss: 0.7605939507484436\n",
      "Epoch 17, Batch 80, Loss: 0.7437624931335449\n",
      "Epoch 17, Batch 81, Loss: 0.741040825843811\n",
      "Epoch 17, Batch 82, Loss: 0.7706993818283081\n",
      "Epoch 17, Batch 83, Loss: 0.7042733430862427\n",
      "Epoch 17, Batch 84, Loss: 0.7598479986190796\n",
      "Epoch 17, Batch 85, Loss: 0.7180913686752319\n",
      "Epoch 17, Batch 86, Loss: 0.7583625912666321\n",
      "Epoch 17, Batch 87, Loss: 0.7768957018852234\n",
      "Epoch 17, Batch 88, Loss: 0.6907932162284851\n",
      "Epoch 17, Batch 89, Loss: 0.7501055002212524\n",
      "Epoch 17, Batch 90, Loss: 0.7759237885475159\n",
      "Epoch 17, Batch 91, Loss: 0.6858161091804504\n",
      "Epoch 17, Batch 92, Loss: 0.7418333292007446\n",
      "Epoch 17, Batch 93, Loss: 0.7213058471679688\n",
      "Epoch 18, Batch 0, Loss: 0.7143964767456055\n",
      "Epoch 18, Batch 1, Loss: 0.736465334892273\n",
      "Epoch 18, Batch 2, Loss: 0.7353729009628296\n",
      "Epoch 18, Batch 3, Loss: 0.753839910030365\n",
      "Epoch 18, Batch 4, Loss: 0.7188211679458618\n",
      "Epoch 18, Batch 5, Loss: 0.755820631980896\n",
      "Epoch 18, Batch 6, Loss: 0.7714604139328003\n",
      "Epoch 18, Batch 7, Loss: 0.7019621729850769\n",
      "Epoch 18, Batch 8, Loss: 0.705834686756134\n",
      "Epoch 18, Batch 9, Loss: 0.7104021906852722\n",
      "Epoch 18, Batch 10, Loss: 0.7623427510261536\n",
      "Epoch 18, Batch 11, Loss: 0.7044620513916016\n",
      "Epoch 18, Batch 12, Loss: 0.7415708303451538\n",
      "Epoch 18, Batch 13, Loss: 0.7858333587646484\n",
      "Epoch 18, Batch 14, Loss: 0.7262243628501892\n",
      "Epoch 18, Batch 15, Loss: 0.7405108213424683\n",
      "Epoch 18, Batch 16, Loss: 0.7118364572525024\n",
      "Epoch 18, Batch 17, Loss: 0.7372995018959045\n",
      "Epoch 18, Batch 18, Loss: 0.8029467463493347\n",
      "Epoch 18, Batch 19, Loss: 0.7474893927574158\n",
      "Epoch 18, Batch 20, Loss: 0.7165642976760864\n",
      "Epoch 18, Batch 21, Loss: 0.7172097563743591\n",
      "Epoch 18, Batch 22, Loss: 0.6759452819824219\n",
      "Epoch 18, Batch 23, Loss: 0.7593250870704651\n",
      "Epoch 18, Batch 24, Loss: 0.7012883424758911\n",
      "Epoch 18, Batch 25, Loss: 0.703080415725708\n",
      "Epoch 18, Batch 26, Loss: 0.7345489263534546\n",
      "Epoch 18, Batch 27, Loss: 0.7273736000061035\n",
      "Epoch 18, Batch 28, Loss: 0.7256675958633423\n",
      "Epoch 18, Batch 29, Loss: 0.7222455143928528\n",
      "Epoch 18, Batch 30, Loss: 0.7496770024299622\n",
      "Epoch 18, Batch 31, Loss: 0.7177703380584717\n",
      "Epoch 18, Batch 32, Loss: 0.74843430519104\n",
      "Epoch 18, Batch 33, Loss: 0.6944965124130249\n",
      "Epoch 18, Batch 34, Loss: 0.7321367859840393\n",
      "Epoch 18, Batch 35, Loss: 0.7252848744392395\n",
      "Epoch 18, Batch 36, Loss: 0.6847099661827087\n",
      "Epoch 18, Batch 37, Loss: 0.6883226037025452\n",
      "Epoch 18, Batch 38, Loss: 0.7005256414413452\n",
      "Epoch 18, Batch 39, Loss: 0.7108238339424133\n",
      "Epoch 18, Batch 40, Loss: 0.7274718880653381\n",
      "Epoch 18, Batch 41, Loss: 0.7122880816459656\n",
      "Epoch 18, Batch 42, Loss: 0.6914892792701721\n",
      "Epoch 18, Batch 43, Loss: 0.7414678335189819\n",
      "Epoch 18, Batch 44, Loss: 0.7039116621017456\n",
      "Epoch 18, Batch 45, Loss: 0.6768583059310913\n",
      "Epoch 18, Batch 46, Loss: 0.7164879441261292\n",
      "Epoch 18, Batch 47, Loss: 0.7163317799568176\n",
      "Epoch 18, Batch 48, Loss: 0.7229786515235901\n",
      "Epoch 18, Batch 49, Loss: 0.7089747786521912\n",
      "Epoch 18, Batch 50, Loss: 0.7429934144020081\n",
      "Epoch 18, Batch 51, Loss: 0.7317830324172974\n",
      "Epoch 18, Batch 52, Loss: 0.7033318281173706\n",
      "Epoch 18, Batch 53, Loss: 0.7427025437355042\n",
      "Epoch 18, Batch 54, Loss: 0.7357346415519714\n",
      "Epoch 18, Batch 55, Loss: 0.6722041964530945\n",
      "Epoch 18, Batch 56, Loss: 0.7002178430557251\n",
      "Epoch 18, Batch 57, Loss: 0.7199875116348267\n",
      "Epoch 18, Batch 58, Loss: 0.7125450968742371\n",
      "Epoch 18, Batch 59, Loss: 0.7246072888374329\n",
      "Epoch 18, Batch 60, Loss: 0.7282439470291138\n",
      "Epoch 18, Batch 61, Loss: 0.7266867160797119\n",
      "Epoch 18, Batch 62, Loss: 0.6903589963912964\n",
      "Epoch 18, Batch 63, Loss: 0.7183619737625122\n",
      "Epoch 18, Batch 64, Loss: 0.6751450896263123\n",
      "Epoch 18, Batch 65, Loss: 0.7725597620010376\n",
      "Epoch 18, Batch 66, Loss: 0.7227671146392822\n",
      "Epoch 18, Batch 67, Loss: 0.7141398191452026\n",
      "Epoch 18, Batch 68, Loss: 0.6769980788230896\n",
      "Epoch 18, Batch 69, Loss: 0.7137677073478699\n",
      "Epoch 18, Batch 70, Loss: 0.7384952306747437\n",
      "Epoch 18, Batch 71, Loss: 0.6908828020095825\n",
      "Epoch 18, Batch 72, Loss: 0.7305710911750793\n",
      "Epoch 18, Batch 73, Loss: 0.7503716945648193\n",
      "Epoch 18, Batch 74, Loss: 0.7110642790794373\n",
      "Epoch 18, Batch 75, Loss: 0.6674477458000183\n",
      "Epoch 18, Batch 76, Loss: 0.7269997000694275\n",
      "Epoch 18, Batch 77, Loss: 0.695873498916626\n",
      "Epoch 18, Batch 78, Loss: 0.715135931968689\n",
      "Epoch 18, Batch 79, Loss: 0.6970074772834778\n",
      "Epoch 18, Batch 80, Loss: 0.7451789975166321\n",
      "Epoch 18, Batch 81, Loss: 0.7000441551208496\n",
      "Epoch 18, Batch 82, Loss: 0.7107905745506287\n",
      "Epoch 18, Batch 83, Loss: 0.7071601152420044\n",
      "Epoch 18, Batch 84, Loss: 0.7144360542297363\n",
      "Epoch 18, Batch 85, Loss: 0.7576010227203369\n",
      "Epoch 18, Batch 86, Loss: 0.73310786485672\n",
      "Epoch 18, Batch 87, Loss: 0.6958162784576416\n",
      "Epoch 18, Batch 88, Loss: 0.785935640335083\n",
      "Epoch 18, Batch 89, Loss: 0.6941442489624023\n",
      "Epoch 18, Batch 90, Loss: 0.6909380555152893\n",
      "Epoch 18, Batch 91, Loss: 0.6766009330749512\n",
      "Epoch 18, Batch 92, Loss: 0.7176774144172668\n",
      "Epoch 18, Batch 93, Loss: 0.7807945013046265\n",
      "Epoch 19, Batch 0, Loss: 0.6964893341064453\n",
      "Epoch 19, Batch 1, Loss: 0.6582501530647278\n",
      "Epoch 19, Batch 2, Loss: 0.7145243287086487\n",
      "Epoch 19, Batch 3, Loss: 0.7371619939804077\n",
      "Epoch 19, Batch 4, Loss: 0.6986531019210815\n",
      "Epoch 19, Batch 5, Loss: 0.6703563928604126\n",
      "Epoch 19, Batch 6, Loss: 0.6672893166542053\n",
      "Epoch 19, Batch 7, Loss: 0.7355431318283081\n",
      "Epoch 19, Batch 8, Loss: 0.6928759813308716\n",
      "Epoch 19, Batch 9, Loss: 0.7484962940216064\n",
      "Epoch 19, Batch 10, Loss: 0.7053426504135132\n",
      "Epoch 19, Batch 11, Loss: 0.7163811326026917\n",
      "Epoch 19, Batch 12, Loss: 0.7204069495201111\n",
      "Epoch 19, Batch 13, Loss: 0.7236657738685608\n",
      "Epoch 19, Batch 14, Loss: 0.7017044425010681\n",
      "Epoch 19, Batch 15, Loss: 0.6566475033760071\n",
      "Epoch 19, Batch 16, Loss: 0.7084876298904419\n",
      "Epoch 19, Batch 17, Loss: 0.6889849305152893\n",
      "Epoch 19, Batch 18, Loss: 0.6697253584861755\n",
      "Epoch 19, Batch 19, Loss: 0.6780680418014526\n",
      "Epoch 19, Batch 20, Loss: 0.7254701852798462\n",
      "Epoch 19, Batch 21, Loss: 0.7105684280395508\n",
      "Epoch 19, Batch 22, Loss: 0.7274166941642761\n",
      "Epoch 19, Batch 23, Loss: 0.7090960741043091\n",
      "Epoch 19, Batch 24, Loss: 0.7476624250411987\n",
      "Epoch 19, Batch 25, Loss: 0.6712437272071838\n",
      "Epoch 19, Batch 26, Loss: 0.7412556409835815\n",
      "Epoch 19, Batch 27, Loss: 0.7357634902000427\n",
      "Epoch 19, Batch 28, Loss: 0.6964064240455627\n",
      "Epoch 19, Batch 29, Loss: 0.6788934469223022\n",
      "Epoch 19, Batch 30, Loss: 0.6684359312057495\n",
      "Epoch 19, Batch 31, Loss: 0.7227016687393188\n",
      "Epoch 19, Batch 32, Loss: 0.717964768409729\n",
      "Epoch 19, Batch 33, Loss: 0.7021299004554749\n",
      "Epoch 19, Batch 34, Loss: 0.6756565570831299\n",
      "Epoch 19, Batch 35, Loss: 0.7018774151802063\n",
      "Epoch 19, Batch 36, Loss: 0.6671445369720459\n",
      "Epoch 19, Batch 37, Loss: 0.690843939781189\n",
      "Epoch 19, Batch 38, Loss: 0.6821764707565308\n",
      "Epoch 19, Batch 39, Loss: 0.6908482313156128\n",
      "Epoch 19, Batch 40, Loss: 0.7068172693252563\n",
      "Epoch 19, Batch 41, Loss: 0.6834231019020081\n",
      "Epoch 19, Batch 42, Loss: 0.7200039029121399\n",
      "Epoch 19, Batch 43, Loss: 0.739429235458374\n",
      "Epoch 19, Batch 44, Loss: 0.7030959725379944\n",
      "Epoch 19, Batch 45, Loss: 0.6920109391212463\n",
      "Epoch 19, Batch 46, Loss: 0.6794663667678833\n",
      "Epoch 19, Batch 47, Loss: 0.7237074971199036\n",
      "Epoch 19, Batch 48, Loss: 0.7103061079978943\n",
      "Epoch 19, Batch 49, Loss: 0.6960591077804565\n",
      "Epoch 19, Batch 50, Loss: 0.6744980812072754\n",
      "Epoch 19, Batch 51, Loss: 0.724551796913147\n",
      "Epoch 19, Batch 52, Loss: 0.7328721284866333\n",
      "Epoch 19, Batch 53, Loss: 0.7285943627357483\n",
      "Epoch 19, Batch 54, Loss: 0.7229017019271851\n",
      "Epoch 19, Batch 55, Loss: 0.7188390493392944\n",
      "Epoch 19, Batch 56, Loss: 0.688088059425354\n",
      "Epoch 19, Batch 57, Loss: 0.7138807773590088\n",
      "Epoch 19, Batch 58, Loss: 0.7061339616775513\n",
      "Epoch 19, Batch 59, Loss: 0.7260277271270752\n",
      "Epoch 19, Batch 60, Loss: 0.7259562611579895\n",
      "Epoch 19, Batch 61, Loss: 0.6781824827194214\n",
      "Epoch 19, Batch 62, Loss: 0.6932860612869263\n",
      "Epoch 19, Batch 63, Loss: 0.656213641166687\n",
      "Epoch 19, Batch 64, Loss: 0.7096759676933289\n",
      "Epoch 19, Batch 65, Loss: 0.6911154389381409\n",
      "Epoch 19, Batch 66, Loss: 0.6808560490608215\n",
      "Epoch 19, Batch 67, Loss: 0.671566367149353\n",
      "Epoch 19, Batch 68, Loss: 0.6777365803718567\n",
      "Epoch 19, Batch 69, Loss: 0.7080491781234741\n",
      "Epoch 19, Batch 70, Loss: 0.6695764064788818\n",
      "Epoch 19, Batch 71, Loss: 0.6756910085678101\n",
      "Epoch 19, Batch 72, Loss: 0.6953325271606445\n",
      "Epoch 19, Batch 73, Loss: 0.6697052717208862\n",
      "Epoch 19, Batch 74, Loss: 0.719356119632721\n",
      "Epoch 19, Batch 75, Loss: 0.6556555032730103\n",
      "Epoch 19, Batch 76, Loss: 0.6182737946510315\n",
      "Epoch 19, Batch 77, Loss: 0.6963382959365845\n",
      "Epoch 19, Batch 78, Loss: 0.6879467964172363\n",
      "Epoch 19, Batch 79, Loss: 0.6974145174026489\n",
      "Epoch 19, Batch 80, Loss: 0.6829572319984436\n",
      "Epoch 19, Batch 81, Loss: 0.6920557618141174\n",
      "Epoch 19, Batch 82, Loss: 0.6238253116607666\n",
      "Epoch 19, Batch 83, Loss: 0.733474850654602\n",
      "Epoch 19, Batch 84, Loss: 0.6858078241348267\n",
      "Epoch 19, Batch 85, Loss: 0.6730341911315918\n",
      "Epoch 19, Batch 86, Loss: 0.6642826795578003\n",
      "Epoch 19, Batch 87, Loss: 0.6474149823188782\n",
      "Epoch 19, Batch 88, Loss: 0.6612955331802368\n",
      "Epoch 19, Batch 89, Loss: 0.6874266862869263\n",
      "Epoch 19, Batch 90, Loss: 0.6593925356864929\n",
      "Epoch 19, Batch 91, Loss: 0.6692993640899658\n",
      "Epoch 19, Batch 92, Loss: 0.6720242500305176\n",
      "Epoch 19, Batch 93, Loss: 0.6908074617385864\n",
      "Epoch 20, Batch 0, Loss: 0.7087079286575317\n",
      "Epoch 20, Batch 1, Loss: 0.7128501534461975\n",
      "Epoch 20, Batch 2, Loss: 0.6450857520103455\n",
      "Epoch 20, Batch 3, Loss: 0.6315744519233704\n",
      "Epoch 20, Batch 4, Loss: 0.6590853929519653\n",
      "Epoch 20, Batch 5, Loss: 0.6896054744720459\n",
      "Epoch 20, Batch 6, Loss: 0.6750112771987915\n",
      "Epoch 20, Batch 7, Loss: 0.6759024262428284\n",
      "Epoch 20, Batch 8, Loss: 0.6748450398445129\n",
      "Epoch 20, Batch 9, Loss: 0.6672006249427795\n",
      "Epoch 20, Batch 10, Loss: 0.7066816687583923\n",
      "Epoch 20, Batch 11, Loss: 0.6997568607330322\n",
      "Epoch 20, Batch 12, Loss: 0.7214733362197876\n",
      "Epoch 20, Batch 13, Loss: 0.6974107623100281\n",
      "Epoch 20, Batch 14, Loss: 0.6700353622436523\n",
      "Epoch 20, Batch 15, Loss: 0.6785612106323242\n",
      "Epoch 20, Batch 16, Loss: 0.6520613431930542\n",
      "Epoch 20, Batch 17, Loss: 0.6307679414749146\n",
      "Epoch 20, Batch 18, Loss: 0.6475062370300293\n",
      "Epoch 20, Batch 19, Loss: 0.6661454439163208\n",
      "Epoch 20, Batch 20, Loss: 0.6792749166488647\n",
      "Epoch 20, Batch 21, Loss: 0.6615196466445923\n",
      "Epoch 20, Batch 22, Loss: 0.6834314465522766\n",
      "Epoch 20, Batch 23, Loss: 0.7076431512832642\n",
      "Epoch 20, Batch 24, Loss: 0.7074800729751587\n",
      "Epoch 20, Batch 25, Loss: 0.6741586923599243\n",
      "Epoch 20, Batch 26, Loss: 0.6552421450614929\n",
      "Epoch 20, Batch 27, Loss: 0.681567370891571\n",
      "Epoch 20, Batch 28, Loss: 0.663545548915863\n",
      "Epoch 20, Batch 29, Loss: 0.7115527987480164\n",
      "Epoch 20, Batch 30, Loss: 0.6400454044342041\n",
      "Epoch 20, Batch 31, Loss: 0.6652359962463379\n",
      "Epoch 20, Batch 32, Loss: 0.6753886938095093\n",
      "Epoch 20, Batch 33, Loss: 0.6719321012496948\n",
      "Epoch 20, Batch 34, Loss: 0.6891728639602661\n",
      "Epoch 20, Batch 35, Loss: 0.7165490388870239\n",
      "Epoch 20, Batch 36, Loss: 0.7179437875747681\n",
      "Epoch 20, Batch 37, Loss: 0.6607046127319336\n",
      "Epoch 20, Batch 38, Loss: 0.7095333933830261\n",
      "Epoch 20, Batch 39, Loss: 0.623541533946991\n",
      "Epoch 20, Batch 40, Loss: 0.6394132375717163\n",
      "Epoch 20, Batch 41, Loss: 0.656696081161499\n",
      "Epoch 20, Batch 42, Loss: 0.6871457695960999\n",
      "Epoch 20, Batch 43, Loss: 0.6732433438301086\n",
      "Epoch 20, Batch 44, Loss: 0.6782981753349304\n",
      "Epoch 20, Batch 45, Loss: 0.699285089969635\n",
      "Epoch 20, Batch 46, Loss: 0.6885808110237122\n",
      "Epoch 20, Batch 47, Loss: 0.6694074273109436\n",
      "Epoch 20, Batch 48, Loss: 0.6609570384025574\n",
      "Epoch 20, Batch 49, Loss: 0.7020090818405151\n",
      "Epoch 20, Batch 50, Loss: 0.6869397163391113\n",
      "Epoch 20, Batch 51, Loss: 0.6688073873519897\n",
      "Epoch 20, Batch 52, Loss: 0.6536180377006531\n",
      "Epoch 20, Batch 53, Loss: 0.6270087361335754\n",
      "Epoch 20, Batch 54, Loss: 0.6273776292800903\n",
      "Epoch 20, Batch 55, Loss: 0.6669052839279175\n",
      "Epoch 20, Batch 56, Loss: 0.6781765222549438\n",
      "Epoch 20, Batch 57, Loss: 0.6533256769180298\n",
      "Epoch 20, Batch 58, Loss: 0.6763233542442322\n",
      "Epoch 20, Batch 59, Loss: 0.6695737242698669\n",
      "Epoch 20, Batch 60, Loss: 0.6725279092788696\n",
      "Epoch 20, Batch 61, Loss: 0.6496062278747559\n",
      "Epoch 20, Batch 62, Loss: 0.6853321194648743\n",
      "Epoch 20, Batch 63, Loss: 0.6836630702018738\n",
      "Epoch 20, Batch 64, Loss: 0.7037480473518372\n",
      "Epoch 20, Batch 65, Loss: 0.6725701093673706\n",
      "Epoch 20, Batch 66, Loss: 0.6487282514572144\n",
      "Epoch 20, Batch 67, Loss: 0.6480530500411987\n",
      "Epoch 20, Batch 68, Loss: 0.6630764007568359\n",
      "Epoch 20, Batch 69, Loss: 0.655012309551239\n",
      "Epoch 20, Batch 70, Loss: 0.6682657599449158\n",
      "Epoch 20, Batch 71, Loss: 0.6369309425354004\n",
      "Epoch 20, Batch 72, Loss: 0.6705595850944519\n",
      "Epoch 20, Batch 73, Loss: 0.6248046159744263\n",
      "Epoch 20, Batch 74, Loss: 0.645896852016449\n",
      "Epoch 20, Batch 75, Loss: 0.6974661350250244\n",
      "Epoch 20, Batch 76, Loss: 0.6372577548027039\n",
      "Epoch 20, Batch 77, Loss: 0.6782275438308716\n",
      "Epoch 20, Batch 78, Loss: 0.6843559145927429\n",
      "Epoch 20, Batch 79, Loss: 0.6525759696960449\n",
      "Epoch 20, Batch 80, Loss: 0.6927465200424194\n",
      "Epoch 20, Batch 81, Loss: 0.6646068096160889\n",
      "Epoch 20, Batch 82, Loss: 0.6569473147392273\n",
      "Epoch 20, Batch 83, Loss: 0.6538864374160767\n",
      "Epoch 20, Batch 84, Loss: 0.642329216003418\n",
      "Epoch 20, Batch 85, Loss: 0.6591700315475464\n",
      "Epoch 20, Batch 86, Loss: 0.6792929768562317\n",
      "Epoch 20, Batch 87, Loss: 0.7148765921592712\n",
      "Epoch 20, Batch 88, Loss: 0.6944496035575867\n",
      "Epoch 20, Batch 89, Loss: 0.6689570546150208\n",
      "Epoch 20, Batch 90, Loss: 0.6999309659004211\n",
      "Epoch 20, Batch 91, Loss: 0.6864097714424133\n",
      "Epoch 20, Batch 92, Loss: 0.6641455888748169\n",
      "Epoch 20, Batch 93, Loss: 0.6909475922584534\n",
      "Epoch 21, Batch 0, Loss: 0.621111273765564\n",
      "Epoch 21, Batch 1, Loss: 0.6814941763877869\n",
      "Epoch 21, Batch 2, Loss: 0.6449524164199829\n",
      "Epoch 21, Batch 3, Loss: 0.7173780202865601\n",
      "Epoch 21, Batch 4, Loss: 0.6637070775032043\n",
      "Epoch 21, Batch 5, Loss: 0.7026527523994446\n",
      "Epoch 21, Batch 6, Loss: 0.6822894215583801\n",
      "Epoch 21, Batch 7, Loss: 0.6721891164779663\n",
      "Epoch 21, Batch 8, Loss: 0.6577304601669312\n",
      "Epoch 21, Batch 9, Loss: 0.6637992858886719\n",
      "Epoch 21, Batch 10, Loss: 0.6876503229141235\n",
      "Epoch 21, Batch 11, Loss: 0.6729223132133484\n",
      "Epoch 21, Batch 12, Loss: 0.7298377156257629\n",
      "Epoch 21, Batch 13, Loss: 0.6550382375717163\n",
      "Epoch 21, Batch 14, Loss: 0.6962965130805969\n",
      "Epoch 21, Batch 15, Loss: 0.6258102059364319\n",
      "Epoch 21, Batch 16, Loss: 0.7125400304794312\n",
      "Epoch 21, Batch 17, Loss: 0.6772445440292358\n",
      "Epoch 21, Batch 18, Loss: 0.6185453534126282\n",
      "Epoch 21, Batch 19, Loss: 0.6208020448684692\n",
      "Epoch 21, Batch 20, Loss: 0.653794527053833\n",
      "Epoch 21, Batch 21, Loss: 0.6195142865180969\n",
      "Epoch 21, Batch 22, Loss: 0.6069850325584412\n",
      "Epoch 21, Batch 23, Loss: 0.6551098227500916\n",
      "Epoch 21, Batch 24, Loss: 0.683477520942688\n",
      "Epoch 21, Batch 25, Loss: 0.7252885103225708\n",
      "Epoch 21, Batch 26, Loss: 0.6622615456581116\n",
      "Epoch 21, Batch 27, Loss: 0.6584709882736206\n",
      "Epoch 21, Batch 28, Loss: 0.6252717971801758\n",
      "Epoch 21, Batch 29, Loss: 0.6377071142196655\n",
      "Epoch 21, Batch 30, Loss: 0.7237867116928101\n",
      "Epoch 21, Batch 31, Loss: 0.6395509839057922\n",
      "Epoch 21, Batch 32, Loss: 0.6565216779708862\n",
      "Epoch 21, Batch 33, Loss: 0.6786255836486816\n",
      "Epoch 21, Batch 34, Loss: 0.6598540544509888\n",
      "Epoch 21, Batch 35, Loss: 0.6327958106994629\n",
      "Epoch 21, Batch 36, Loss: 0.6101855039596558\n",
      "Epoch 21, Batch 37, Loss: 0.654579758644104\n",
      "Epoch 21, Batch 38, Loss: 0.6617041826248169\n",
      "Epoch 21, Batch 39, Loss: 0.6467315554618835\n",
      "Epoch 21, Batch 40, Loss: 0.623919665813446\n",
      "Epoch 21, Batch 41, Loss: 0.6449665427207947\n",
      "Epoch 21, Batch 42, Loss: 0.6382018327713013\n",
      "Epoch 21, Batch 43, Loss: 0.619261622428894\n",
      "Epoch 21, Batch 44, Loss: 0.6326747536659241\n",
      "Epoch 21, Batch 45, Loss: 0.6339322328567505\n",
      "Epoch 21, Batch 46, Loss: 0.6527425646781921\n",
      "Epoch 21, Batch 47, Loss: 0.6543365716934204\n",
      "Epoch 21, Batch 48, Loss: 0.6866952180862427\n",
      "Epoch 21, Batch 49, Loss: 0.6722501516342163\n",
      "Epoch 21, Batch 50, Loss: 0.6514991521835327\n",
      "Epoch 21, Batch 51, Loss: 0.6006677746772766\n",
      "Epoch 21, Batch 52, Loss: 0.6280043721199036\n",
      "Epoch 21, Batch 53, Loss: 0.6308463215827942\n",
      "Epoch 21, Batch 54, Loss: 0.6773279905319214\n",
      "Epoch 21, Batch 55, Loss: 0.7048414945602417\n",
      "Epoch 21, Batch 56, Loss: 0.6898189783096313\n",
      "Epoch 21, Batch 57, Loss: 0.6194999814033508\n",
      "Epoch 21, Batch 58, Loss: 0.6022142171859741\n",
      "Epoch 21, Batch 59, Loss: 0.6141146421432495\n",
      "Epoch 21, Batch 60, Loss: 0.6482139825820923\n",
      "Epoch 21, Batch 61, Loss: 0.6332317590713501\n",
      "Epoch 21, Batch 62, Loss: 0.6720790266990662\n",
      "Epoch 21, Batch 63, Loss: 0.6579562425613403\n",
      "Epoch 21, Batch 64, Loss: 0.6508647203445435\n",
      "Epoch 21, Batch 65, Loss: 0.6498716473579407\n",
      "Epoch 21, Batch 66, Loss: 0.6772717833518982\n",
      "Epoch 21, Batch 67, Loss: 0.6942247748374939\n",
      "Epoch 21, Batch 68, Loss: 0.629518449306488\n",
      "Epoch 21, Batch 69, Loss: 0.6087878942489624\n",
      "Epoch 21, Batch 70, Loss: 0.6360219120979309\n",
      "Epoch 21, Batch 71, Loss: 0.641465961933136\n",
      "Epoch 21, Batch 72, Loss: 0.6530928015708923\n",
      "Epoch 21, Batch 73, Loss: 0.6539186835289001\n",
      "Epoch 21, Batch 74, Loss: 0.674788773059845\n",
      "Epoch 21, Batch 75, Loss: 0.6358854174613953\n",
      "Epoch 21, Batch 76, Loss: 0.6583176851272583\n",
      "Epoch 21, Batch 77, Loss: 0.6667901277542114\n",
      "Epoch 21, Batch 78, Loss: 0.6288169622421265\n",
      "Epoch 21, Batch 79, Loss: 0.6554932594299316\n",
      "Epoch 21, Batch 80, Loss: 0.6174523830413818\n",
      "Epoch 21, Batch 81, Loss: 0.6001194715499878\n",
      "Epoch 21, Batch 82, Loss: 0.6377555727958679\n",
      "Epoch 21, Batch 83, Loss: 0.6979891061782837\n",
      "Epoch 21, Batch 84, Loss: 0.6337606310844421\n",
      "Epoch 21, Batch 85, Loss: 0.638043999671936\n",
      "Epoch 21, Batch 86, Loss: 0.639276385307312\n",
      "Epoch 21, Batch 87, Loss: 0.6170384287834167\n",
      "Epoch 21, Batch 88, Loss: 0.6060618758201599\n",
      "Epoch 21, Batch 89, Loss: 0.6368770599365234\n",
      "Epoch 21, Batch 90, Loss: 0.6579769253730774\n",
      "Epoch 21, Batch 91, Loss: 0.594192624092102\n",
      "Epoch 21, Batch 92, Loss: 0.6410161256790161\n",
      "Epoch 21, Batch 93, Loss: 0.6486786603927612\n",
      "Epoch 22, Batch 0, Loss: 0.6498731970787048\n",
      "Epoch 22, Batch 1, Loss: 0.6592499017715454\n",
      "Epoch 22, Batch 2, Loss: 0.6548889875411987\n",
      "Epoch 22, Batch 3, Loss: 0.6139804124832153\n",
      "Epoch 22, Batch 4, Loss: 0.6116207838058472\n",
      "Epoch 22, Batch 5, Loss: 0.6572279334068298\n",
      "Epoch 22, Batch 6, Loss: 0.6295661330223083\n",
      "Epoch 22, Batch 7, Loss: 0.6556673049926758\n",
      "Epoch 22, Batch 8, Loss: 0.651039719581604\n",
      "Epoch 22, Batch 9, Loss: 0.6614733934402466\n",
      "Epoch 22, Batch 10, Loss: 0.6240283250808716\n",
      "Epoch 22, Batch 11, Loss: 0.60361647605896\n",
      "Epoch 22, Batch 12, Loss: 0.6079093217849731\n",
      "Epoch 22, Batch 13, Loss: 0.6406663060188293\n",
      "Epoch 22, Batch 14, Loss: 0.6856352686882019\n",
      "Epoch 22, Batch 15, Loss: 0.612363338470459\n",
      "Epoch 22, Batch 16, Loss: 0.638731837272644\n",
      "Epoch 22, Batch 17, Loss: 0.5955967903137207\n",
      "Epoch 22, Batch 18, Loss: 0.6557061672210693\n",
      "Epoch 22, Batch 19, Loss: 0.6445170044898987\n",
      "Epoch 22, Batch 20, Loss: 0.627874493598938\n",
      "Epoch 22, Batch 21, Loss: 0.6637662053108215\n",
      "Epoch 22, Batch 22, Loss: 0.6391405463218689\n",
      "Epoch 22, Batch 23, Loss: 0.61626136302948\n",
      "Epoch 22, Batch 24, Loss: 0.6497941017150879\n",
      "Epoch 22, Batch 25, Loss: 0.6002800464630127\n",
      "Epoch 22, Batch 26, Loss: 0.6607686877250671\n",
      "Epoch 22, Batch 27, Loss: 0.6161617040634155\n",
      "Epoch 22, Batch 28, Loss: 0.6287273168563843\n",
      "Epoch 22, Batch 29, Loss: 0.6338084936141968\n",
      "Epoch 22, Batch 30, Loss: 0.6471008062362671\n",
      "Epoch 22, Batch 31, Loss: 0.6079670190811157\n",
      "Epoch 22, Batch 32, Loss: 0.6178102493286133\n",
      "Epoch 22, Batch 33, Loss: 0.6137336492538452\n",
      "Epoch 22, Batch 34, Loss: 0.6144829392433167\n",
      "Epoch 22, Batch 35, Loss: 0.6268914341926575\n",
      "Epoch 22, Batch 36, Loss: 0.6610820889472961\n",
      "Epoch 22, Batch 37, Loss: 0.6098262071609497\n",
      "Epoch 22, Batch 38, Loss: 0.6367794275283813\n",
      "Epoch 22, Batch 39, Loss: 0.6367993354797363\n",
      "Epoch 22, Batch 40, Loss: 0.6503338813781738\n",
      "Epoch 22, Batch 41, Loss: 0.6524357199668884\n",
      "Epoch 22, Batch 42, Loss: 0.6261200308799744\n",
      "Epoch 22, Batch 43, Loss: 0.6729764938354492\n",
      "Epoch 22, Batch 44, Loss: 0.6498675346374512\n",
      "Epoch 22, Batch 45, Loss: 0.6452130079269409\n",
      "Epoch 22, Batch 46, Loss: 0.6238029599189758\n",
      "Epoch 22, Batch 47, Loss: 0.5947027802467346\n",
      "Epoch 22, Batch 48, Loss: 0.6760609745979309\n",
      "Epoch 22, Batch 49, Loss: 0.6429131627082825\n",
      "Epoch 22, Batch 50, Loss: 0.6690544486045837\n",
      "Epoch 22, Batch 51, Loss: 0.6393160223960876\n",
      "Epoch 22, Batch 52, Loss: 0.5858765244483948\n",
      "Epoch 22, Batch 53, Loss: 0.6434421539306641\n",
      "Epoch 22, Batch 54, Loss: 0.6248485445976257\n",
      "Epoch 22, Batch 55, Loss: 0.6490140557289124\n",
      "Epoch 22, Batch 56, Loss: 0.6258335709571838\n",
      "Epoch 22, Batch 57, Loss: 0.6008545756340027\n",
      "Epoch 22, Batch 58, Loss: 0.6515322923660278\n",
      "Epoch 22, Batch 59, Loss: 0.6458272337913513\n",
      "Epoch 22, Batch 60, Loss: 0.6328746676445007\n",
      "Epoch 22, Batch 61, Loss: 0.6206423044204712\n",
      "Epoch 22, Batch 62, Loss: 0.6277803182601929\n",
      "Epoch 22, Batch 63, Loss: 0.5883380174636841\n",
      "Epoch 22, Batch 64, Loss: 0.6349956393241882\n",
      "Epoch 22, Batch 65, Loss: 0.6193827390670776\n",
      "Epoch 22, Batch 66, Loss: 0.6172734498977661\n",
      "Epoch 22, Batch 67, Loss: 0.6392675638198853\n",
      "Epoch 22, Batch 68, Loss: 0.6814207434654236\n",
      "Epoch 22, Batch 69, Loss: 0.6582712531089783\n",
      "Epoch 22, Batch 70, Loss: 0.6022523045539856\n",
      "Epoch 22, Batch 71, Loss: 0.6504319906234741\n",
      "Epoch 22, Batch 72, Loss: 0.6168946027755737\n",
      "Epoch 22, Batch 73, Loss: 0.6403339505195618\n",
      "Epoch 22, Batch 74, Loss: 0.5900409817695618\n",
      "Epoch 22, Batch 75, Loss: 0.6030643582344055\n",
      "Epoch 22, Batch 76, Loss: 0.6717206239700317\n",
      "Epoch 22, Batch 77, Loss: 0.678767204284668\n",
      "Epoch 22, Batch 78, Loss: 0.6123021245002747\n",
      "Epoch 22, Batch 79, Loss: 0.6384449601173401\n",
      "Epoch 22, Batch 80, Loss: 0.5932108163833618\n",
      "Epoch 22, Batch 81, Loss: 0.6357508897781372\n",
      "Epoch 22, Batch 82, Loss: 0.6156957745552063\n",
      "Epoch 22, Batch 83, Loss: 0.6502255797386169\n",
      "Epoch 22, Batch 84, Loss: 0.6733078956604004\n",
      "Epoch 22, Batch 85, Loss: 0.6225256323814392\n",
      "Epoch 22, Batch 86, Loss: 0.6111788749694824\n",
      "Epoch 22, Batch 87, Loss: 0.6115795373916626\n",
      "Epoch 22, Batch 88, Loss: 0.645004153251648\n",
      "Epoch 22, Batch 89, Loss: 0.6187800168991089\n",
      "Epoch 22, Batch 90, Loss: 0.5982707738876343\n",
      "Epoch 22, Batch 91, Loss: 0.6315476298332214\n",
      "Epoch 22, Batch 92, Loss: 0.629361093044281\n",
      "Epoch 22, Batch 93, Loss: 0.604096531867981\n",
      "Epoch 23, Batch 0, Loss: 0.5900595784187317\n",
      "Epoch 23, Batch 1, Loss: 0.6109336018562317\n",
      "Epoch 23, Batch 2, Loss: 0.6167026162147522\n",
      "Epoch 23, Batch 3, Loss: 0.5966285467147827\n",
      "Epoch 23, Batch 4, Loss: 0.6395447850227356\n",
      "Epoch 23, Batch 5, Loss: 0.6688662767410278\n",
      "Epoch 23, Batch 6, Loss: 0.6678819060325623\n",
      "Epoch 23, Batch 7, Loss: 0.6111338138580322\n",
      "Epoch 23, Batch 8, Loss: 0.6336846947669983\n",
      "Epoch 23, Batch 9, Loss: 0.6570638418197632\n",
      "Epoch 23, Batch 10, Loss: 0.6521603465080261\n",
      "Epoch 23, Batch 11, Loss: 0.613517165184021\n",
      "Epoch 23, Batch 12, Loss: 0.5948513746261597\n",
      "Epoch 23, Batch 13, Loss: 0.6254062652587891\n",
      "Epoch 23, Batch 14, Loss: 0.625472366809845\n",
      "Epoch 23, Batch 15, Loss: 0.6438062191009521\n",
      "Epoch 23, Batch 16, Loss: 0.5850844383239746\n",
      "Epoch 23, Batch 17, Loss: 0.5956164002418518\n",
      "Epoch 23, Batch 18, Loss: 0.6186486482620239\n",
      "Epoch 23, Batch 19, Loss: 0.594372570514679\n",
      "Epoch 23, Batch 20, Loss: 0.6159375309944153\n",
      "Epoch 23, Batch 21, Loss: 0.6370984315872192\n",
      "Epoch 23, Batch 22, Loss: 0.6427247524261475\n",
      "Epoch 23, Batch 23, Loss: 0.5639211535453796\n",
      "Epoch 23, Batch 24, Loss: 0.6266886591911316\n",
      "Epoch 23, Batch 25, Loss: 0.6465245485305786\n",
      "Epoch 23, Batch 26, Loss: 0.6002056002616882\n",
      "Epoch 23, Batch 27, Loss: 0.6027824878692627\n",
      "Epoch 23, Batch 28, Loss: 0.6005570292472839\n",
      "Epoch 23, Batch 29, Loss: 0.6014459729194641\n",
      "Epoch 23, Batch 30, Loss: 0.6339890956878662\n",
      "Epoch 23, Batch 31, Loss: 0.62690269947052\n",
      "Epoch 23, Batch 32, Loss: 0.6333946585655212\n",
      "Epoch 23, Batch 33, Loss: 0.586885392665863\n",
      "Epoch 23, Batch 34, Loss: 0.6461341977119446\n",
      "Epoch 23, Batch 35, Loss: 0.5971006155014038\n",
      "Epoch 23, Batch 36, Loss: 0.6321577429771423\n",
      "Epoch 23, Batch 37, Loss: 0.6343768835067749\n",
      "Epoch 23, Batch 38, Loss: 0.5685704946517944\n",
      "Epoch 23, Batch 39, Loss: 0.6021137237548828\n",
      "Epoch 23, Batch 40, Loss: 0.6288431882858276\n",
      "Epoch 23, Batch 41, Loss: 0.6663640737533569\n",
      "Epoch 23, Batch 42, Loss: 0.6541754603385925\n",
      "Epoch 23, Batch 43, Loss: 0.611028254032135\n",
      "Epoch 23, Batch 44, Loss: 0.6050949692726135\n",
      "Epoch 23, Batch 45, Loss: 0.5678179264068604\n",
      "Epoch 23, Batch 46, Loss: 0.6179059743881226\n",
      "Epoch 23, Batch 47, Loss: 0.566344678401947\n",
      "Epoch 23, Batch 48, Loss: 0.6145390868186951\n",
      "Epoch 23, Batch 49, Loss: 0.600304126739502\n",
      "Epoch 23, Batch 50, Loss: 0.6014429330825806\n",
      "Epoch 23, Batch 51, Loss: 0.5821722745895386\n",
      "Epoch 23, Batch 52, Loss: 0.6049038171768188\n",
      "Epoch 23, Batch 53, Loss: 0.6723171472549438\n",
      "Epoch 23, Batch 54, Loss: 0.6428500413894653\n",
      "Epoch 23, Batch 55, Loss: 0.6298551559448242\n",
      "Epoch 23, Batch 56, Loss: 0.5824741125106812\n",
      "Epoch 23, Batch 57, Loss: 0.6349008679389954\n",
      "Epoch 23, Batch 58, Loss: 0.586201548576355\n",
      "Epoch 23, Batch 59, Loss: 0.6273490190505981\n",
      "Epoch 23, Batch 60, Loss: 0.6061695218086243\n",
      "Epoch 23, Batch 61, Loss: 0.6023536324501038\n",
      "Epoch 23, Batch 62, Loss: 0.6475964784622192\n",
      "Epoch 23, Batch 63, Loss: 0.6546594500541687\n",
      "Epoch 23, Batch 64, Loss: 0.6441619396209717\n",
      "Epoch 23, Batch 65, Loss: 0.6451095342636108\n",
      "Epoch 23, Batch 66, Loss: 0.6809168457984924\n",
      "Epoch 23, Batch 67, Loss: 0.631888747215271\n",
      "Epoch 23, Batch 68, Loss: 0.5943825840950012\n",
      "Epoch 23, Batch 69, Loss: 0.5987586975097656\n",
      "Epoch 23, Batch 70, Loss: 0.6017088294029236\n",
      "Epoch 23, Batch 71, Loss: 0.618705153465271\n",
      "Epoch 23, Batch 72, Loss: 0.5846493244171143\n",
      "Epoch 23, Batch 73, Loss: 0.6485239267349243\n",
      "Epoch 23, Batch 74, Loss: 0.6381958723068237\n",
      "Epoch 23, Batch 75, Loss: 0.562332272529602\n",
      "Epoch 23, Batch 76, Loss: 0.6269149780273438\n",
      "Epoch 23, Batch 77, Loss: 0.6102715730667114\n",
      "Epoch 23, Batch 78, Loss: 0.5703706741333008\n",
      "Epoch 23, Batch 79, Loss: 0.6740126609802246\n",
      "Epoch 23, Batch 80, Loss: 0.5359574556350708\n",
      "Epoch 23, Batch 81, Loss: 0.6349103450775146\n",
      "Epoch 23, Batch 82, Loss: 0.604202926158905\n",
      "Epoch 23, Batch 83, Loss: 0.6109820604324341\n",
      "Epoch 23, Batch 84, Loss: 0.5838698148727417\n",
      "Epoch 23, Batch 85, Loss: 0.5712601542472839\n",
      "Epoch 23, Batch 86, Loss: 0.5927928686141968\n",
      "Epoch 23, Batch 87, Loss: 0.6391428112983704\n",
      "Epoch 23, Batch 88, Loss: 0.5769078135490417\n",
      "Epoch 23, Batch 89, Loss: 0.6121780276298523\n",
      "Epoch 23, Batch 90, Loss: 0.5828625559806824\n",
      "Epoch 23, Batch 91, Loss: 0.6869306564331055\n",
      "Epoch 23, Batch 92, Loss: 0.6174397468566895\n",
      "Epoch 23, Batch 93, Loss: 0.55623859167099\n",
      "Epoch 24, Batch 0, Loss: 0.6364543437957764\n",
      "Epoch 24, Batch 1, Loss: 0.5658708810806274\n",
      "Epoch 24, Batch 2, Loss: 0.6166602373123169\n",
      "Epoch 24, Batch 3, Loss: 0.6196962594985962\n",
      "Epoch 24, Batch 4, Loss: 0.5984511971473694\n",
      "Epoch 24, Batch 5, Loss: 0.6392122507095337\n",
      "Epoch 24, Batch 6, Loss: 0.6341802477836609\n",
      "Epoch 24, Batch 7, Loss: 0.568649172782898\n",
      "Epoch 24, Batch 8, Loss: 0.6284688115119934\n",
      "Epoch 24, Batch 9, Loss: 0.6677538752555847\n",
      "Epoch 24, Batch 10, Loss: 0.6117063164710999\n",
      "Epoch 24, Batch 11, Loss: 0.5963930487632751\n",
      "Epoch 24, Batch 12, Loss: 0.5794105529785156\n",
      "Epoch 24, Batch 13, Loss: 0.6032620668411255\n",
      "Epoch 24, Batch 14, Loss: 0.5539907217025757\n",
      "Epoch 24, Batch 15, Loss: 0.5899225473403931\n",
      "Epoch 24, Batch 16, Loss: 0.5779792070388794\n",
      "Epoch 24, Batch 17, Loss: 0.5489883422851562\n",
      "Epoch 24, Batch 18, Loss: 0.6486140489578247\n",
      "Epoch 24, Batch 19, Loss: 0.6009572148323059\n",
      "Epoch 24, Batch 20, Loss: 0.5970731973648071\n",
      "Epoch 24, Batch 21, Loss: 0.6482088565826416\n",
      "Epoch 24, Batch 22, Loss: 0.5854082107543945\n",
      "Epoch 24, Batch 23, Loss: 0.6379591226577759\n",
      "Epoch 24, Batch 24, Loss: 0.617106020450592\n",
      "Epoch 24, Batch 25, Loss: 0.52788907289505\n",
      "Epoch 24, Batch 26, Loss: 0.5938080549240112\n",
      "Epoch 24, Batch 27, Loss: 0.6173325777053833\n",
      "Epoch 24, Batch 28, Loss: 0.578438401222229\n",
      "Epoch 24, Batch 29, Loss: 0.6169073581695557\n",
      "Epoch 24, Batch 30, Loss: 0.581638753414154\n",
      "Epoch 24, Batch 31, Loss: 0.5928002595901489\n",
      "Epoch 24, Batch 32, Loss: 0.626778244972229\n",
      "Epoch 24, Batch 33, Loss: 0.5903798341751099\n",
      "Epoch 24, Batch 34, Loss: 0.5915130972862244\n",
      "Epoch 24, Batch 35, Loss: 0.591598391532898\n",
      "Epoch 24, Batch 36, Loss: 0.5800016522407532\n",
      "Epoch 24, Batch 37, Loss: 0.6151903867721558\n",
      "Epoch 24, Batch 38, Loss: 0.6183938384056091\n",
      "Epoch 24, Batch 39, Loss: 0.5659908056259155\n",
      "Epoch 24, Batch 40, Loss: 0.5856319665908813\n",
      "Epoch 24, Batch 41, Loss: 0.5936992168426514\n",
      "Epoch 24, Batch 42, Loss: 0.5538026094436646\n",
      "Epoch 24, Batch 43, Loss: 0.5814119577407837\n",
      "Epoch 24, Batch 44, Loss: 0.5721404552459717\n",
      "Epoch 24, Batch 45, Loss: 0.6135050654411316\n",
      "Epoch 24, Batch 46, Loss: 0.6428314447402954\n",
      "Epoch 24, Batch 47, Loss: 0.6423801183700562\n",
      "Epoch 24, Batch 48, Loss: 0.6071101427078247\n",
      "Epoch 24, Batch 49, Loss: 0.5948048830032349\n",
      "Epoch 24, Batch 50, Loss: 0.6224549412727356\n",
      "Epoch 24, Batch 51, Loss: 0.5799645781517029\n",
      "Epoch 24, Batch 52, Loss: 0.5577961206436157\n",
      "Epoch 24, Batch 53, Loss: 0.6241162419319153\n",
      "Epoch 24, Batch 54, Loss: 0.6120213270187378\n",
      "Epoch 24, Batch 55, Loss: 0.5957527756690979\n",
      "Epoch 24, Batch 56, Loss: 0.5933708548545837\n",
      "Epoch 24, Batch 57, Loss: 0.6280897855758667\n",
      "Epoch 24, Batch 58, Loss: 0.5669629573822021\n",
      "Epoch 24, Batch 59, Loss: 0.5981545448303223\n",
      "Epoch 24, Batch 60, Loss: 0.6045340299606323\n",
      "Epoch 24, Batch 61, Loss: 0.6267621517181396\n",
      "Epoch 24, Batch 62, Loss: 0.5925036668777466\n",
      "Epoch 24, Batch 63, Loss: 0.592706561088562\n",
      "Epoch 24, Batch 64, Loss: 0.5992976427078247\n",
      "Epoch 24, Batch 65, Loss: 0.6397851705551147\n",
      "Epoch 24, Batch 66, Loss: 0.6036723256111145\n",
      "Epoch 24, Batch 67, Loss: 0.630515456199646\n",
      "Epoch 24, Batch 68, Loss: 0.5573644638061523\n",
      "Epoch 24, Batch 69, Loss: 0.6135474443435669\n",
      "Epoch 24, Batch 70, Loss: 0.6079949736595154\n",
      "Epoch 24, Batch 71, Loss: 0.5843502879142761\n",
      "Epoch 24, Batch 72, Loss: 0.6045960187911987\n",
      "Epoch 24, Batch 73, Loss: 0.5957581996917725\n",
      "Epoch 24, Batch 74, Loss: 0.5927239060401917\n",
      "Epoch 24, Batch 75, Loss: 0.5948755145072937\n",
      "Epoch 24, Batch 76, Loss: 0.6383054852485657\n",
      "Epoch 24, Batch 77, Loss: 0.633031964302063\n",
      "Epoch 24, Batch 78, Loss: 0.5899114608764648\n",
      "Epoch 24, Batch 79, Loss: 0.5922330021858215\n",
      "Epoch 24, Batch 80, Loss: 0.5990484952926636\n",
      "Epoch 24, Batch 81, Loss: 0.597632884979248\n",
      "Epoch 24, Batch 82, Loss: 0.6220689415931702\n",
      "Epoch 24, Batch 83, Loss: 0.6292089223861694\n",
      "Epoch 24, Batch 84, Loss: 0.6501293182373047\n",
      "Epoch 24, Batch 85, Loss: 0.5905133485794067\n",
      "Epoch 24, Batch 86, Loss: 0.5832722783088684\n",
      "Epoch 24, Batch 87, Loss: 0.5446949005126953\n",
      "Epoch 24, Batch 88, Loss: 0.5971940159797668\n",
      "Epoch 24, Batch 89, Loss: 0.5228198766708374\n",
      "Epoch 24, Batch 90, Loss: 0.5936437845230103\n",
      "Epoch 24, Batch 91, Loss: 0.5980299115180969\n",
      "Epoch 24, Batch 92, Loss: 0.5926183462142944\n",
      "Epoch 24, Batch 93, Loss: 0.5886704921722412\n",
      "Epoch 25, Batch 0, Loss: 0.58890300989151\n",
      "Epoch 25, Batch 1, Loss: 0.5881448984146118\n",
      "Epoch 25, Batch 2, Loss: 0.6044676303863525\n",
      "Epoch 25, Batch 3, Loss: 0.6076263189315796\n",
      "Epoch 25, Batch 4, Loss: 0.6041744947433472\n",
      "Epoch 25, Batch 5, Loss: 0.6278172731399536\n",
      "Epoch 25, Batch 6, Loss: 0.6324796676635742\n",
      "Epoch 25, Batch 7, Loss: 0.5957621335983276\n",
      "Epoch 25, Batch 8, Loss: 0.5634490847587585\n",
      "Epoch 25, Batch 9, Loss: 0.5415866374969482\n",
      "Epoch 25, Batch 10, Loss: 0.6133081912994385\n",
      "Epoch 25, Batch 11, Loss: 0.6115726232528687\n",
      "Epoch 25, Batch 12, Loss: 0.6276452541351318\n",
      "Epoch 25, Batch 13, Loss: 0.5360538959503174\n",
      "Epoch 25, Batch 14, Loss: 0.617102324962616\n",
      "Epoch 25, Batch 15, Loss: 0.6264907717704773\n",
      "Epoch 25, Batch 16, Loss: 0.5642048716545105\n",
      "Epoch 25, Batch 17, Loss: 0.5708551406860352\n",
      "Epoch 25, Batch 18, Loss: 0.6220265626907349\n",
      "Epoch 25, Batch 19, Loss: 0.5624048709869385\n",
      "Epoch 25, Batch 20, Loss: 0.6393144130706787\n",
      "Epoch 25, Batch 21, Loss: 0.5938435196876526\n",
      "Epoch 25, Batch 22, Loss: 0.5310784578323364\n",
      "Epoch 25, Batch 23, Loss: 0.6190322637557983\n",
      "Epoch 25, Batch 24, Loss: 0.5713768601417542\n",
      "Epoch 25, Batch 25, Loss: 0.6030407547950745\n",
      "Epoch 25, Batch 26, Loss: 0.5665175318717957\n",
      "Epoch 25, Batch 27, Loss: 0.5984295606613159\n",
      "Epoch 25, Batch 28, Loss: 0.5791460275650024\n",
      "Epoch 25, Batch 29, Loss: 0.6158251762390137\n",
      "Epoch 25, Batch 30, Loss: 0.6015716791152954\n",
      "Epoch 25, Batch 31, Loss: 0.5563835501670837\n",
      "Epoch 25, Batch 32, Loss: 0.581313967704773\n",
      "Epoch 25, Batch 33, Loss: 0.5701059103012085\n",
      "Epoch 25, Batch 34, Loss: 0.5592831969261169\n",
      "Epoch 25, Batch 35, Loss: 0.589651882648468\n",
      "Epoch 25, Batch 36, Loss: 0.6114332675933838\n",
      "Epoch 25, Batch 37, Loss: 0.5986881852149963\n",
      "Epoch 25, Batch 38, Loss: 0.5484007596969604\n",
      "Epoch 25, Batch 39, Loss: 0.6266254186630249\n",
      "Epoch 25, Batch 40, Loss: 0.5851572751998901\n",
      "Epoch 25, Batch 41, Loss: 0.5709883570671082\n",
      "Epoch 25, Batch 42, Loss: 0.5773330926895142\n",
      "Epoch 25, Batch 43, Loss: 0.6193760633468628\n",
      "Epoch 25, Batch 44, Loss: 0.6446220874786377\n",
      "Epoch 25, Batch 45, Loss: 0.5992846488952637\n",
      "Epoch 25, Batch 46, Loss: 0.5758412480354309\n",
      "Epoch 25, Batch 47, Loss: 0.6056469082832336\n",
      "Epoch 25, Batch 48, Loss: 0.5501218438148499\n",
      "Epoch 25, Batch 49, Loss: 0.585729718208313\n",
      "Epoch 25, Batch 50, Loss: 0.6043354272842407\n",
      "Epoch 25, Batch 51, Loss: 0.6225747466087341\n",
      "Epoch 25, Batch 52, Loss: 0.5915367603302002\n",
      "Epoch 25, Batch 53, Loss: 0.602990448474884\n",
      "Epoch 25, Batch 54, Loss: 0.6000157594680786\n",
      "Epoch 25, Batch 55, Loss: 0.5311277508735657\n",
      "Epoch 25, Batch 56, Loss: 0.6208735704421997\n",
      "Epoch 25, Batch 57, Loss: 0.6147745251655579\n",
      "Epoch 25, Batch 58, Loss: 0.5732921957969666\n",
      "Epoch 25, Batch 59, Loss: 0.5822218656539917\n",
      "Epoch 25, Batch 60, Loss: 0.6273876428604126\n",
      "Epoch 25, Batch 61, Loss: 0.6221488118171692\n",
      "Epoch 25, Batch 62, Loss: 0.5273243188858032\n",
      "Epoch 25, Batch 63, Loss: 0.5864404439926147\n",
      "Epoch 25, Batch 64, Loss: 0.5644318461418152\n",
      "Epoch 25, Batch 65, Loss: 0.5444270372390747\n",
      "Epoch 25, Batch 66, Loss: 0.5396966338157654\n",
      "Epoch 25, Batch 67, Loss: 0.5675908327102661\n",
      "Epoch 25, Batch 68, Loss: 0.6016244888305664\n",
      "Epoch 25, Batch 69, Loss: 0.5669106245040894\n",
      "Epoch 25, Batch 70, Loss: 0.5496770143508911\n",
      "Epoch 25, Batch 71, Loss: 0.5917931795120239\n",
      "Epoch 25, Batch 72, Loss: 0.5461512804031372\n",
      "Epoch 25, Batch 73, Loss: 0.6351415514945984\n",
      "Epoch 25, Batch 74, Loss: 0.5532669425010681\n",
      "Epoch 25, Batch 75, Loss: 0.5698329210281372\n",
      "Epoch 25, Batch 76, Loss: 0.5524171590805054\n",
      "Epoch 25, Batch 77, Loss: 0.5795055031776428\n",
      "Epoch 25, Batch 78, Loss: 0.6219314336776733\n",
      "Epoch 25, Batch 79, Loss: 0.5775460004806519\n",
      "Epoch 25, Batch 80, Loss: 0.6031789183616638\n",
      "Epoch 25, Batch 81, Loss: 0.551477313041687\n",
      "Epoch 25, Batch 82, Loss: 0.5470553040504456\n",
      "Epoch 25, Batch 83, Loss: 0.5381170511245728\n",
      "Epoch 25, Batch 84, Loss: 0.5968796014785767\n",
      "Epoch 25, Batch 85, Loss: 0.5631077289581299\n",
      "Epoch 25, Batch 86, Loss: 0.6071532964706421\n",
      "Epoch 25, Batch 87, Loss: 0.5696394443511963\n",
      "Epoch 25, Batch 88, Loss: 0.5543681383132935\n",
      "Epoch 25, Batch 89, Loss: 0.5730093121528625\n",
      "Epoch 25, Batch 90, Loss: 0.5717899203300476\n",
      "Epoch 25, Batch 91, Loss: 0.5819920897483826\n",
      "Epoch 25, Batch 92, Loss: 0.5854184031486511\n",
      "Epoch 25, Batch 93, Loss: 0.598922610282898\n",
      "Epoch 26, Batch 0, Loss: 0.6119309067726135\n",
      "Epoch 26, Batch 1, Loss: 0.596301257610321\n",
      "Epoch 26, Batch 2, Loss: 0.5758436322212219\n",
      "Epoch 26, Batch 3, Loss: 0.5775440335273743\n",
      "Epoch 26, Batch 4, Loss: 0.5981723070144653\n",
      "Epoch 26, Batch 5, Loss: 0.5701389312744141\n",
      "Epoch 26, Batch 6, Loss: 0.5539606809616089\n",
      "Epoch 26, Batch 7, Loss: 0.5309644937515259\n",
      "Epoch 26, Batch 8, Loss: 0.6049834489822388\n",
      "Epoch 26, Batch 9, Loss: 0.5731990933418274\n",
      "Epoch 26, Batch 10, Loss: 0.5707589983940125\n",
      "Epoch 26, Batch 11, Loss: 0.5998905897140503\n",
      "Epoch 26, Batch 12, Loss: 0.5376757383346558\n",
      "Epoch 26, Batch 13, Loss: 0.5929123163223267\n",
      "Epoch 26, Batch 14, Loss: 0.5650303363800049\n",
      "Epoch 26, Batch 15, Loss: 0.5666561126708984\n",
      "Epoch 26, Batch 16, Loss: 0.5659248232841492\n",
      "Epoch 26, Batch 17, Loss: 0.5957176089286804\n",
      "Epoch 26, Batch 18, Loss: 0.57337886095047\n",
      "Epoch 26, Batch 19, Loss: 0.5479553937911987\n",
      "Epoch 26, Batch 20, Loss: 0.6368774175643921\n",
      "Epoch 26, Batch 21, Loss: 0.5963958501815796\n",
      "Epoch 26, Batch 22, Loss: 0.6178810000419617\n",
      "Epoch 26, Batch 23, Loss: 0.6074126362800598\n",
      "Epoch 26, Batch 24, Loss: 0.5770753622055054\n",
      "Epoch 26, Batch 25, Loss: 0.5706020593643188\n",
      "Epoch 26, Batch 26, Loss: 0.5961099863052368\n",
      "Epoch 26, Batch 27, Loss: 0.5937440395355225\n",
      "Epoch 26, Batch 28, Loss: 0.6195551753044128\n",
      "Epoch 26, Batch 29, Loss: 0.5666477084159851\n",
      "Epoch 26, Batch 30, Loss: 0.5895531177520752\n",
      "Epoch 26, Batch 31, Loss: 0.5942123532295227\n",
      "Epoch 26, Batch 32, Loss: 0.5761964321136475\n",
      "Epoch 26, Batch 33, Loss: 0.5575700402259827\n",
      "Epoch 26, Batch 34, Loss: 0.5579063296318054\n",
      "Epoch 26, Batch 35, Loss: 0.599872887134552\n",
      "Epoch 26, Batch 36, Loss: 0.547409176826477\n",
      "Epoch 26, Batch 37, Loss: 0.5632632970809937\n",
      "Epoch 26, Batch 38, Loss: 0.5823363661766052\n",
      "Epoch 26, Batch 39, Loss: 0.5352839231491089\n",
      "Epoch 26, Batch 40, Loss: 0.5598260164260864\n",
      "Epoch 26, Batch 41, Loss: 0.6146253943443298\n",
      "Epoch 26, Batch 42, Loss: 0.5769204497337341\n",
      "Epoch 26, Batch 43, Loss: 0.5023472905158997\n",
      "Epoch 26, Batch 44, Loss: 0.557339072227478\n",
      "Epoch 26, Batch 45, Loss: 0.5711411237716675\n",
      "Epoch 26, Batch 46, Loss: 0.5643021464347839\n",
      "Epoch 26, Batch 47, Loss: 0.508357048034668\n",
      "Epoch 26, Batch 48, Loss: 0.5671597123146057\n",
      "Epoch 26, Batch 49, Loss: 0.5724523067474365\n",
      "Epoch 26, Batch 50, Loss: 0.5797317028045654\n",
      "Epoch 26, Batch 51, Loss: 0.5501260757446289\n",
      "Epoch 26, Batch 52, Loss: 0.5508226156234741\n",
      "Epoch 26, Batch 53, Loss: 0.5621787309646606\n",
      "Epoch 26, Batch 54, Loss: 0.6234633326530457\n",
      "Epoch 26, Batch 55, Loss: 0.5327120423316956\n",
      "Epoch 26, Batch 56, Loss: 0.6188223958015442\n",
      "Epoch 26, Batch 57, Loss: 0.5493074655532837\n",
      "Epoch 26, Batch 58, Loss: 0.5719869136810303\n",
      "Epoch 26, Batch 59, Loss: 0.5713557004928589\n",
      "Epoch 26, Batch 60, Loss: 0.5872225165367126\n",
      "Epoch 26, Batch 61, Loss: 0.5484470129013062\n",
      "Epoch 26, Batch 62, Loss: 0.5739486217498779\n",
      "Epoch 26, Batch 63, Loss: 0.542143702507019\n",
      "Epoch 26, Batch 64, Loss: 0.5443282723426819\n",
      "Epoch 26, Batch 65, Loss: 0.5952146649360657\n",
      "Epoch 26, Batch 66, Loss: 0.5201276540756226\n",
      "Epoch 26, Batch 67, Loss: 0.5597272515296936\n",
      "Epoch 26, Batch 68, Loss: 0.626239538192749\n",
      "Epoch 26, Batch 69, Loss: 0.5784491300582886\n",
      "Epoch 26, Batch 70, Loss: 0.5537598729133606\n",
      "Epoch 26, Batch 71, Loss: 0.5821776986122131\n",
      "Epoch 26, Batch 72, Loss: 0.5500718355178833\n",
      "Epoch 26, Batch 73, Loss: 0.5789337754249573\n",
      "Epoch 26, Batch 74, Loss: 0.5322787761688232\n",
      "Epoch 26, Batch 75, Loss: 0.5784436464309692\n",
      "Epoch 26, Batch 76, Loss: 0.5583484172821045\n",
      "Epoch 26, Batch 77, Loss: 0.5482373833656311\n",
      "Epoch 26, Batch 78, Loss: 0.5686817169189453\n",
      "Epoch 26, Batch 79, Loss: 0.5726719498634338\n",
      "Epoch 26, Batch 80, Loss: 0.546115517616272\n",
      "Epoch 26, Batch 81, Loss: 0.5885506868362427\n",
      "Epoch 26, Batch 82, Loss: 0.6191185116767883\n",
      "Epoch 26, Batch 83, Loss: 0.6209371089935303\n",
      "Epoch 26, Batch 84, Loss: 0.561914324760437\n",
      "Epoch 26, Batch 85, Loss: 0.5266329050064087\n",
      "Epoch 26, Batch 86, Loss: 0.5325120687484741\n",
      "Epoch 26, Batch 87, Loss: 0.6279584169387817\n",
      "Epoch 26, Batch 88, Loss: 0.622562825679779\n",
      "Epoch 26, Batch 89, Loss: 0.5870000720024109\n",
      "Epoch 26, Batch 90, Loss: 0.5465958118438721\n",
      "Epoch 26, Batch 91, Loss: 0.5648060441017151\n",
      "Epoch 26, Batch 92, Loss: 0.5236471891403198\n",
      "Epoch 26, Batch 93, Loss: 0.5877889394760132\n",
      "Epoch 27, Batch 0, Loss: 0.5687111616134644\n",
      "Epoch 27, Batch 1, Loss: 0.6210421919822693\n",
      "Epoch 27, Batch 2, Loss: 0.6063984632492065\n",
      "Epoch 27, Batch 3, Loss: 0.5646585822105408\n",
      "Epoch 27, Batch 4, Loss: 0.5540748238563538\n",
      "Epoch 27, Batch 5, Loss: 0.4998103976249695\n",
      "Epoch 27, Batch 6, Loss: 0.5818923115730286\n",
      "Epoch 27, Batch 7, Loss: 0.5780535936355591\n",
      "Epoch 27, Batch 8, Loss: 0.503290057182312\n",
      "Epoch 27, Batch 9, Loss: 0.5537829995155334\n",
      "Epoch 27, Batch 10, Loss: 0.5585992932319641\n",
      "Epoch 27, Batch 11, Loss: 0.556591808795929\n",
      "Epoch 27, Batch 12, Loss: 0.5612878203392029\n",
      "Epoch 27, Batch 13, Loss: 0.5597493648529053\n",
      "Epoch 27, Batch 14, Loss: 0.5522721409797668\n",
      "Epoch 27, Batch 15, Loss: 0.5705474615097046\n",
      "Epoch 27, Batch 16, Loss: 0.5445205569267273\n",
      "Epoch 27, Batch 17, Loss: 0.5758397579193115\n",
      "Epoch 27, Batch 18, Loss: 0.5345653295516968\n",
      "Epoch 27, Batch 19, Loss: 0.5932337641716003\n",
      "Epoch 27, Batch 20, Loss: 0.5879202485084534\n",
      "Epoch 27, Batch 21, Loss: 0.5823098421096802\n",
      "Epoch 27, Batch 22, Loss: 0.5167690515518188\n",
      "Epoch 27, Batch 23, Loss: 0.5791901350021362\n",
      "Epoch 27, Batch 24, Loss: 0.5415846705436707\n",
      "Epoch 27, Batch 25, Loss: 0.5695879459381104\n",
      "Epoch 27, Batch 26, Loss: 0.5687786340713501\n",
      "Epoch 27, Batch 27, Loss: 0.5416491031646729\n",
      "Epoch 27, Batch 28, Loss: 0.5572428107261658\n",
      "Epoch 27, Batch 29, Loss: 0.5591549873352051\n",
      "Epoch 27, Batch 30, Loss: 0.5971245169639587\n",
      "Epoch 27, Batch 31, Loss: 0.5631951689720154\n",
      "Epoch 27, Batch 32, Loss: 0.542488157749176\n",
      "Epoch 27, Batch 33, Loss: 0.5634670853614807\n",
      "Epoch 27, Batch 34, Loss: 0.5316985249519348\n",
      "Epoch 27, Batch 35, Loss: 0.5832554697990417\n",
      "Epoch 27, Batch 36, Loss: 0.5634948015213013\n",
      "Epoch 27, Batch 37, Loss: 0.5609866380691528\n",
      "Epoch 27, Batch 38, Loss: 0.6259177923202515\n",
      "Epoch 27, Batch 39, Loss: 0.5874665379524231\n",
      "Epoch 27, Batch 40, Loss: 0.5691686272621155\n",
      "Epoch 27, Batch 41, Loss: 0.5398268699645996\n",
      "Epoch 27, Batch 42, Loss: 0.544219970703125\n",
      "Epoch 27, Batch 43, Loss: 0.5502197742462158\n",
      "Epoch 27, Batch 44, Loss: 0.5724084973335266\n",
      "Epoch 27, Batch 45, Loss: 0.5929600596427917\n",
      "Epoch 27, Batch 46, Loss: 0.559505820274353\n",
      "Epoch 27, Batch 47, Loss: 0.5774286985397339\n",
      "Epoch 27, Batch 48, Loss: 0.5666610598564148\n",
      "Epoch 27, Batch 49, Loss: 0.5341192483901978\n",
      "Epoch 27, Batch 50, Loss: 0.5700885653495789\n",
      "Epoch 27, Batch 51, Loss: 0.5623243451118469\n",
      "Epoch 27, Batch 52, Loss: 0.5989007949829102\n",
      "Epoch 27, Batch 53, Loss: 0.5955848097801208\n",
      "Epoch 27, Batch 54, Loss: 0.571439266204834\n",
      "Epoch 27, Batch 55, Loss: 0.5522288680076599\n",
      "Epoch 27, Batch 56, Loss: 0.5198915600776672\n",
      "Epoch 27, Batch 57, Loss: 0.5228758454322815\n",
      "Epoch 27, Batch 58, Loss: 0.582619309425354\n",
      "Epoch 27, Batch 59, Loss: 0.5601006746292114\n",
      "Epoch 27, Batch 60, Loss: 0.5783930420875549\n",
      "Epoch 27, Batch 61, Loss: 0.5544626116752625\n",
      "Epoch 27, Batch 62, Loss: 0.5173153281211853\n",
      "Epoch 27, Batch 63, Loss: 0.5962013006210327\n",
      "Epoch 27, Batch 64, Loss: 0.5504449605941772\n",
      "Epoch 27, Batch 65, Loss: 0.5802583694458008\n",
      "Epoch 27, Batch 66, Loss: 0.5299860239028931\n",
      "Epoch 27, Batch 67, Loss: 0.5302671194076538\n",
      "Epoch 27, Batch 68, Loss: 0.584443211555481\n",
      "Epoch 27, Batch 69, Loss: 0.5496503114700317\n",
      "Epoch 27, Batch 70, Loss: 0.5479636192321777\n",
      "Epoch 27, Batch 71, Loss: 0.561936616897583\n",
      "Epoch 27, Batch 72, Loss: 0.5690876245498657\n",
      "Epoch 27, Batch 73, Loss: 0.5661975741386414\n",
      "Epoch 27, Batch 74, Loss: 0.5355827808380127\n",
      "Epoch 27, Batch 75, Loss: 0.537685751914978\n",
      "Epoch 27, Batch 76, Loss: 0.5867365002632141\n",
      "Epoch 27, Batch 77, Loss: 0.4825878143310547\n",
      "Epoch 27, Batch 78, Loss: 0.5324833989143372\n",
      "Epoch 27, Batch 79, Loss: 0.5448521375656128\n",
      "Epoch 27, Batch 80, Loss: 0.571928083896637\n",
      "Epoch 27, Batch 81, Loss: 0.5578789114952087\n",
      "Epoch 27, Batch 82, Loss: 0.5385895371437073\n",
      "Epoch 27, Batch 83, Loss: 0.5250765085220337\n",
      "Epoch 27, Batch 84, Loss: 0.5983460545539856\n",
      "Epoch 27, Batch 85, Loss: 0.5693319439888\n",
      "Epoch 27, Batch 86, Loss: 0.549979031085968\n",
      "Epoch 27, Batch 87, Loss: 0.5899890661239624\n",
      "Epoch 27, Batch 88, Loss: 0.6042633056640625\n",
      "Epoch 27, Batch 89, Loss: 0.55863356590271\n",
      "Epoch 27, Batch 90, Loss: 0.5450739860534668\n",
      "Epoch 27, Batch 91, Loss: 0.566180408000946\n",
      "Epoch 27, Batch 92, Loss: 0.553171694278717\n",
      "Epoch 27, Batch 93, Loss: 0.5319754481315613\n",
      "Epoch 28, Batch 0, Loss: 0.569284200668335\n",
      "Epoch 28, Batch 1, Loss: 0.5047603845596313\n",
      "Epoch 28, Batch 2, Loss: 0.5530291199684143\n",
      "Epoch 28, Batch 3, Loss: 0.5777534246444702\n",
      "Epoch 28, Batch 4, Loss: 0.5709316730499268\n",
      "Epoch 28, Batch 5, Loss: 0.5917760729789734\n",
      "Epoch 28, Batch 6, Loss: 0.5185686349868774\n",
      "Epoch 28, Batch 7, Loss: 0.5554788708686829\n",
      "Epoch 28, Batch 8, Loss: 0.5802817344665527\n",
      "Epoch 28, Batch 9, Loss: 0.524196982383728\n",
      "Epoch 28, Batch 10, Loss: 0.5724977850914001\n",
      "Epoch 28, Batch 11, Loss: 0.5470553636550903\n",
      "Epoch 28, Batch 12, Loss: 0.5980590581893921\n",
      "Epoch 28, Batch 13, Loss: 0.5450800657272339\n",
      "Epoch 28, Batch 14, Loss: 0.5456477403640747\n",
      "Epoch 28, Batch 15, Loss: 0.5740697383880615\n",
      "Epoch 28, Batch 16, Loss: 0.5753160119056702\n",
      "Epoch 28, Batch 17, Loss: 0.5416690111160278\n",
      "Epoch 28, Batch 18, Loss: 0.5464353561401367\n",
      "Epoch 28, Batch 19, Loss: 0.5256918668746948\n",
      "Epoch 28, Batch 20, Loss: 0.5399335026741028\n",
      "Epoch 28, Batch 21, Loss: 0.5926344990730286\n",
      "Epoch 28, Batch 22, Loss: 0.6065567135810852\n",
      "Epoch 28, Batch 23, Loss: 0.6001076102256775\n",
      "Epoch 28, Batch 24, Loss: 0.5376535058021545\n",
      "Epoch 28, Batch 25, Loss: 0.5219861268997192\n",
      "Epoch 28, Batch 26, Loss: 0.5343754291534424\n",
      "Epoch 28, Batch 27, Loss: 0.5465256571769714\n",
      "Epoch 28, Batch 28, Loss: 0.5250358581542969\n",
      "Epoch 28, Batch 29, Loss: 0.5442758798599243\n",
      "Epoch 28, Batch 30, Loss: 0.5860363245010376\n",
      "Epoch 28, Batch 31, Loss: 0.5319643616676331\n",
      "Epoch 28, Batch 32, Loss: 0.5679863691329956\n",
      "Epoch 28, Batch 33, Loss: 0.5608262419700623\n",
      "Epoch 28, Batch 34, Loss: 0.5373715162277222\n",
      "Epoch 28, Batch 35, Loss: 0.5215464234352112\n",
      "Epoch 28, Batch 36, Loss: 0.595270037651062\n",
      "Epoch 28, Batch 37, Loss: 0.5432809591293335\n",
      "Epoch 28, Batch 38, Loss: 0.5323491096496582\n",
      "Epoch 28, Batch 39, Loss: 0.533053994178772\n",
      "Epoch 28, Batch 40, Loss: 0.5664746761322021\n",
      "Epoch 28, Batch 41, Loss: 0.5367640256881714\n",
      "Epoch 28, Batch 42, Loss: 0.5486984848976135\n",
      "Epoch 28, Batch 43, Loss: 0.4942501485347748\n",
      "Epoch 28, Batch 44, Loss: 0.5672817230224609\n",
      "Epoch 28, Batch 45, Loss: 0.5732530355453491\n",
      "Epoch 28, Batch 46, Loss: 0.5209681391716003\n",
      "Epoch 28, Batch 47, Loss: 0.5475295782089233\n",
      "Epoch 28, Batch 48, Loss: 0.5362261533737183\n",
      "Epoch 28, Batch 49, Loss: 0.5321840643882751\n",
      "Epoch 28, Batch 50, Loss: 0.5030003190040588\n",
      "Epoch 28, Batch 51, Loss: 0.5459076762199402\n",
      "Epoch 28, Batch 52, Loss: 0.5554264783859253\n",
      "Epoch 28, Batch 53, Loss: 0.5460125803947449\n",
      "Epoch 28, Batch 54, Loss: 0.5353338718414307\n",
      "Epoch 28, Batch 55, Loss: 0.5889526009559631\n",
      "Epoch 28, Batch 56, Loss: 0.5874983072280884\n",
      "Epoch 28, Batch 57, Loss: 0.5288872122764587\n",
      "Epoch 28, Batch 58, Loss: 0.5596052408218384\n",
      "Epoch 28, Batch 59, Loss: 0.5316463112831116\n",
      "Epoch 28, Batch 60, Loss: 0.5792697072029114\n",
      "Epoch 28, Batch 61, Loss: 0.5585935711860657\n",
      "Epoch 28, Batch 62, Loss: 0.4904728829860687\n",
      "Epoch 28, Batch 63, Loss: 0.4948679804801941\n",
      "Epoch 28, Batch 64, Loss: 0.5695593953132629\n",
      "Epoch 28, Batch 65, Loss: 0.5561269521713257\n",
      "Epoch 28, Batch 66, Loss: 0.5685232877731323\n",
      "Epoch 28, Batch 67, Loss: 0.5967925190925598\n",
      "Epoch 28, Batch 68, Loss: 0.5420971512794495\n",
      "Epoch 28, Batch 69, Loss: 0.5699471235275269\n",
      "Epoch 28, Batch 70, Loss: 0.5271027684211731\n",
      "Epoch 28, Batch 71, Loss: 0.5500897169113159\n",
      "Epoch 28, Batch 72, Loss: 0.5245102047920227\n",
      "Epoch 28, Batch 73, Loss: 0.5261755585670471\n",
      "Epoch 28, Batch 74, Loss: 0.510688841342926\n",
      "Epoch 28, Batch 75, Loss: 0.5500655770301819\n",
      "Epoch 28, Batch 76, Loss: 0.5621576905250549\n",
      "Epoch 28, Batch 77, Loss: 0.5587828159332275\n",
      "Epoch 28, Batch 78, Loss: 0.5257704257965088\n",
      "Epoch 28, Batch 79, Loss: 0.5652472972869873\n",
      "Epoch 28, Batch 80, Loss: 0.5498613715171814\n",
      "Epoch 28, Batch 81, Loss: 0.5663400888442993\n",
      "Epoch 28, Batch 82, Loss: 0.5056923627853394\n",
      "Epoch 28, Batch 83, Loss: 0.5174008011817932\n",
      "Epoch 28, Batch 84, Loss: 0.5715427398681641\n",
      "Epoch 28, Batch 85, Loss: 0.5691100358963013\n",
      "Epoch 28, Batch 86, Loss: 0.5011860132217407\n",
      "Epoch 28, Batch 87, Loss: 0.5287350416183472\n",
      "Epoch 28, Batch 88, Loss: 0.564895749092102\n",
      "Epoch 28, Batch 89, Loss: 0.5638868808746338\n",
      "Epoch 28, Batch 90, Loss: 0.5674096345901489\n",
      "Epoch 28, Batch 91, Loss: 0.5554267168045044\n",
      "Epoch 28, Batch 92, Loss: 0.5519811511039734\n",
      "Epoch 28, Batch 93, Loss: 0.5224518179893494\n",
      "Epoch 29, Batch 0, Loss: 0.5337640643119812\n",
      "Epoch 29, Batch 1, Loss: 0.5460402965545654\n",
      "Epoch 29, Batch 2, Loss: 0.5378990769386292\n",
      "Epoch 29, Batch 3, Loss: 0.5678411722183228\n",
      "Epoch 29, Batch 4, Loss: 0.5877177715301514\n",
      "Epoch 29, Batch 5, Loss: 0.5244547128677368\n",
      "Epoch 29, Batch 6, Loss: 0.5119219422340393\n",
      "Epoch 29, Batch 7, Loss: 0.5712304711341858\n",
      "Epoch 29, Batch 8, Loss: 0.5672921538352966\n",
      "Epoch 29, Batch 9, Loss: 0.5438421964645386\n",
      "Epoch 29, Batch 10, Loss: 0.5357965230941772\n",
      "Epoch 29, Batch 11, Loss: 0.5207310914993286\n",
      "Epoch 29, Batch 12, Loss: 0.5188782215118408\n",
      "Epoch 29, Batch 13, Loss: 0.5098499059677124\n",
      "Epoch 29, Batch 14, Loss: 0.5325260162353516\n",
      "Epoch 29, Batch 15, Loss: 0.5114739537239075\n",
      "Epoch 29, Batch 16, Loss: 0.5370267629623413\n",
      "Epoch 29, Batch 17, Loss: 0.4923594892024994\n",
      "Epoch 29, Batch 18, Loss: 0.5081561207771301\n",
      "Epoch 29, Batch 19, Loss: 0.5083959698677063\n",
      "Epoch 29, Batch 20, Loss: 0.5180820226669312\n",
      "Epoch 29, Batch 21, Loss: 0.5410639047622681\n",
      "Epoch 29, Batch 22, Loss: 0.5695251226425171\n",
      "Epoch 29, Batch 23, Loss: 0.5618801116943359\n",
      "Epoch 29, Batch 24, Loss: 0.5345239639282227\n",
      "Epoch 29, Batch 25, Loss: 0.5409846305847168\n",
      "Epoch 29, Batch 26, Loss: 0.5878821611404419\n",
      "Epoch 29, Batch 27, Loss: 0.5050123929977417\n",
      "Epoch 29, Batch 28, Loss: 0.5537489056587219\n",
      "Epoch 29, Batch 29, Loss: 0.5387482047080994\n",
      "Epoch 29, Batch 30, Loss: 0.5528765916824341\n",
      "Epoch 29, Batch 31, Loss: 0.5355995893478394\n",
      "Epoch 29, Batch 32, Loss: 0.5364753007888794\n",
      "Epoch 29, Batch 33, Loss: 0.5119060277938843\n",
      "Epoch 29, Batch 34, Loss: 0.5250681042671204\n",
      "Epoch 29, Batch 35, Loss: 0.5183789134025574\n",
      "Epoch 29, Batch 36, Loss: 0.5483062267303467\n",
      "Epoch 29, Batch 37, Loss: 0.5408220291137695\n",
      "Epoch 29, Batch 38, Loss: 0.5210349559783936\n",
      "Epoch 29, Batch 39, Loss: 0.5410813093185425\n",
      "Epoch 29, Batch 40, Loss: 0.5602012872695923\n",
      "Epoch 29, Batch 41, Loss: 0.5359102487564087\n",
      "Epoch 29, Batch 42, Loss: 0.5092366933822632\n",
      "Epoch 29, Batch 43, Loss: 0.5897849798202515\n",
      "Epoch 29, Batch 44, Loss: 0.5101920962333679\n",
      "Epoch 29, Batch 45, Loss: 0.5386067628860474\n",
      "Epoch 29, Batch 46, Loss: 0.5666844844818115\n",
      "Epoch 29, Batch 47, Loss: 0.5396416187286377\n",
      "Epoch 29, Batch 48, Loss: 0.538244366645813\n",
      "Epoch 29, Batch 49, Loss: 0.5785856246948242\n",
      "Epoch 29, Batch 50, Loss: 0.5087997317314148\n",
      "Epoch 29, Batch 51, Loss: 0.4849446713924408\n",
      "Epoch 29, Batch 52, Loss: 0.5438054800033569\n",
      "Epoch 29, Batch 53, Loss: 0.49077901244163513\n",
      "Epoch 29, Batch 54, Loss: 0.5160223245620728\n",
      "Epoch 29, Batch 55, Loss: 0.57401043176651\n",
      "Epoch 29, Batch 56, Loss: 0.5381419062614441\n",
      "Epoch 29, Batch 57, Loss: 0.5565630793571472\n",
      "Epoch 29, Batch 58, Loss: 0.5604475736618042\n",
      "Epoch 29, Batch 59, Loss: 0.5188629031181335\n",
      "Epoch 29, Batch 60, Loss: 0.5319428443908691\n",
      "Epoch 29, Batch 61, Loss: 0.5678170919418335\n",
      "Epoch 29, Batch 62, Loss: 0.5331522226333618\n",
      "Epoch 29, Batch 63, Loss: 0.5304334759712219\n",
      "Epoch 29, Batch 64, Loss: 0.5460320711135864\n",
      "Epoch 29, Batch 65, Loss: 0.5437799692153931\n",
      "Epoch 29, Batch 66, Loss: 0.5454751253128052\n",
      "Epoch 29, Batch 67, Loss: 0.5375838279724121\n",
      "Epoch 29, Batch 68, Loss: 0.5186360478401184\n",
      "Epoch 29, Batch 69, Loss: 0.5154293179512024\n",
      "Epoch 29, Batch 70, Loss: 0.5126127004623413\n",
      "Epoch 29, Batch 71, Loss: 0.5562745928764343\n",
      "Epoch 29, Batch 72, Loss: 0.5339996814727783\n",
      "Epoch 29, Batch 73, Loss: 0.5526487231254578\n",
      "Epoch 29, Batch 74, Loss: 0.5292067527770996\n",
      "Epoch 29, Batch 75, Loss: 0.5360845923423767\n",
      "Epoch 29, Batch 76, Loss: 0.5892115235328674\n",
      "Epoch 29, Batch 77, Loss: 0.5656048059463501\n",
      "Epoch 29, Batch 78, Loss: 0.5013498067855835\n",
      "Epoch 29, Batch 79, Loss: 0.530267059803009\n",
      "Epoch 29, Batch 80, Loss: 0.5373278856277466\n",
      "Epoch 29, Batch 81, Loss: 0.5646845102310181\n",
      "Epoch 29, Batch 82, Loss: 0.572263777256012\n",
      "Epoch 29, Batch 83, Loss: 0.5492261648178101\n",
      "Epoch 29, Batch 84, Loss: 0.5762284994125366\n",
      "Epoch 29, Batch 85, Loss: 0.5279433727264404\n",
      "Epoch 29, Batch 86, Loss: 0.5450099110603333\n",
      "Epoch 29, Batch 87, Loss: 0.5896808505058289\n",
      "Epoch 29, Batch 88, Loss: 0.5560287237167358\n",
      "Epoch 29, Batch 89, Loss: 0.5217275619506836\n",
      "Epoch 29, Batch 90, Loss: 0.5625141263008118\n",
      "Epoch 29, Batch 91, Loss: 0.5251913666725159\n",
      "Epoch 29, Batch 92, Loss: 0.5287823677062988\n",
      "Epoch 29, Batch 93, Loss: 0.5205076932907104\n",
      "Epoch 30, Batch 0, Loss: 0.5348834991455078\n",
      "Epoch 30, Batch 1, Loss: 0.5658459067344666\n",
      "Epoch 30, Batch 2, Loss: 0.506312906742096\n",
      "Epoch 30, Batch 3, Loss: 0.4784252643585205\n",
      "Epoch 30, Batch 4, Loss: 0.500962495803833\n",
      "Epoch 30, Batch 5, Loss: 0.5691325068473816\n",
      "Epoch 30, Batch 6, Loss: 0.5436694622039795\n",
      "Epoch 30, Batch 7, Loss: 0.5630491375923157\n",
      "Epoch 30, Batch 8, Loss: 0.5250458121299744\n",
      "Epoch 30, Batch 9, Loss: 0.5493084788322449\n",
      "Epoch 30, Batch 10, Loss: 0.5385969877243042\n",
      "Epoch 30, Batch 11, Loss: 0.516220211982727\n",
      "Epoch 30, Batch 12, Loss: 0.5364035367965698\n",
      "Epoch 30, Batch 13, Loss: 0.4978359341621399\n",
      "Epoch 30, Batch 14, Loss: 0.5341372489929199\n",
      "Epoch 30, Batch 15, Loss: 0.5095614194869995\n",
      "Epoch 30, Batch 16, Loss: 0.5180131196975708\n",
      "Epoch 30, Batch 17, Loss: 0.5433035492897034\n",
      "Epoch 30, Batch 18, Loss: 0.48951148986816406\n",
      "Epoch 30, Batch 19, Loss: 0.5090836882591248\n",
      "Epoch 30, Batch 20, Loss: 0.4969962239265442\n",
      "Epoch 30, Batch 21, Loss: 0.5713232159614563\n",
      "Epoch 30, Batch 22, Loss: 0.5304392576217651\n",
      "Epoch 30, Batch 23, Loss: 0.5684368014335632\n",
      "Epoch 30, Batch 24, Loss: 0.5356397032737732\n",
      "Epoch 30, Batch 25, Loss: 0.5739496946334839\n",
      "Epoch 30, Batch 26, Loss: 0.5260291695594788\n",
      "Epoch 30, Batch 27, Loss: 0.5017755627632141\n",
      "Epoch 30, Batch 28, Loss: 0.4830474853515625\n",
      "Epoch 30, Batch 29, Loss: 0.5541089177131653\n",
      "Epoch 30, Batch 30, Loss: 0.532871425151825\n",
      "Epoch 30, Batch 31, Loss: 0.5284394025802612\n",
      "Epoch 30, Batch 32, Loss: 0.5749348402023315\n",
      "Epoch 30, Batch 33, Loss: 0.569661021232605\n",
      "Epoch 30, Batch 34, Loss: 0.5537222623825073\n",
      "Epoch 30, Batch 35, Loss: 0.5178723931312561\n",
      "Epoch 30, Batch 36, Loss: 0.48432230949401855\n",
      "Epoch 30, Batch 37, Loss: 0.5329664945602417\n",
      "Epoch 30, Batch 38, Loss: 0.5371695160865784\n",
      "Epoch 30, Batch 39, Loss: 0.5537389516830444\n",
      "Epoch 30, Batch 40, Loss: 0.5555792450904846\n",
      "Epoch 30, Batch 41, Loss: 0.5389508008956909\n",
      "Epoch 30, Batch 42, Loss: 0.5938371419906616\n",
      "Epoch 30, Batch 43, Loss: 0.5101013779640198\n",
      "Epoch 30, Batch 44, Loss: 0.4861031174659729\n",
      "Epoch 30, Batch 45, Loss: 0.5238639116287231\n",
      "Epoch 30, Batch 46, Loss: 0.5500954389572144\n",
      "Epoch 30, Batch 47, Loss: 0.5746625065803528\n",
      "Epoch 30, Batch 48, Loss: 0.5531284213066101\n",
      "Epoch 30, Batch 49, Loss: 0.49529990553855896\n",
      "Epoch 30, Batch 50, Loss: 0.5571466684341431\n",
      "Epoch 30, Batch 51, Loss: 0.5122318863868713\n",
      "Epoch 30, Batch 52, Loss: 0.5622090101242065\n",
      "Epoch 30, Batch 53, Loss: 0.5308233499526978\n",
      "Epoch 30, Batch 54, Loss: 0.5005530118942261\n",
      "Epoch 30, Batch 55, Loss: 0.5288000702857971\n",
      "Epoch 30, Batch 56, Loss: 0.5333662033081055\n",
      "Epoch 30, Batch 57, Loss: 0.5384566187858582\n",
      "Epoch 30, Batch 58, Loss: 0.5202512741088867\n",
      "Epoch 30, Batch 59, Loss: 0.5657645463943481\n",
      "Epoch 30, Batch 60, Loss: 0.5472582578659058\n",
      "Epoch 30, Batch 61, Loss: 0.5030035972595215\n",
      "Epoch 30, Batch 62, Loss: 0.5759075284004211\n",
      "Epoch 30, Batch 63, Loss: 0.5642189979553223\n",
      "Epoch 30, Batch 64, Loss: 0.5235167741775513\n",
      "Epoch 30, Batch 65, Loss: 0.5270999670028687\n",
      "Epoch 30, Batch 66, Loss: 0.5034634470939636\n",
      "Epoch 30, Batch 67, Loss: 0.47566109895706177\n",
      "Epoch 30, Batch 68, Loss: 0.5080754160881042\n",
      "Epoch 30, Batch 69, Loss: 0.5080012083053589\n",
      "Epoch 30, Batch 70, Loss: 0.5009463429450989\n",
      "Epoch 30, Batch 71, Loss: 0.5325602293014526\n",
      "Epoch 30, Batch 72, Loss: 0.5288378000259399\n",
      "Epoch 30, Batch 73, Loss: 0.5415440201759338\n",
      "Epoch 30, Batch 74, Loss: 0.5427168607711792\n",
      "Epoch 30, Batch 75, Loss: 0.570138692855835\n",
      "Epoch 30, Batch 76, Loss: 0.5419877767562866\n",
      "Epoch 30, Batch 77, Loss: 0.5700494050979614\n",
      "Epoch 30, Batch 78, Loss: 0.5281514525413513\n",
      "Epoch 30, Batch 79, Loss: 0.521472156047821\n",
      "Epoch 30, Batch 80, Loss: 0.48709630966186523\n",
      "Epoch 30, Batch 81, Loss: 0.5155108571052551\n",
      "Epoch 30, Batch 82, Loss: 0.5285875201225281\n",
      "Epoch 30, Batch 83, Loss: 0.5180413126945496\n",
      "Epoch 30, Batch 84, Loss: 0.5146492719650269\n",
      "Epoch 30, Batch 85, Loss: 0.4821184277534485\n",
      "Epoch 30, Batch 86, Loss: 0.5377780199050903\n",
      "Epoch 30, Batch 87, Loss: 0.49554187059402466\n",
      "Epoch 30, Batch 88, Loss: 0.5142873525619507\n",
      "Epoch 30, Batch 89, Loss: 0.5027319192886353\n",
      "Epoch 30, Batch 90, Loss: 0.48781782388687134\n",
      "Epoch 30, Batch 91, Loss: 0.555192232131958\n",
      "Epoch 30, Batch 92, Loss: 0.4981043338775635\n",
      "Epoch 30, Batch 93, Loss: 0.5102772116661072\n",
      "Epoch 31, Batch 0, Loss: 0.5304567217826843\n",
      "Epoch 31, Batch 1, Loss: 0.5499295592308044\n",
      "Epoch 31, Batch 2, Loss: 0.49246153235435486\n",
      "Epoch 31, Batch 3, Loss: 0.5616881847381592\n",
      "Epoch 31, Batch 4, Loss: 0.5759338140487671\n",
      "Epoch 31, Batch 5, Loss: 0.5488173961639404\n",
      "Epoch 31, Batch 6, Loss: 0.551502525806427\n",
      "Epoch 31, Batch 7, Loss: 0.5209022760391235\n",
      "Epoch 31, Batch 8, Loss: 0.5290185809135437\n",
      "Epoch 31, Batch 9, Loss: 0.5346963405609131\n",
      "Epoch 31, Batch 10, Loss: 0.5087872743606567\n",
      "Epoch 31, Batch 11, Loss: 0.4931143820285797\n",
      "Epoch 31, Batch 12, Loss: 0.5033999085426331\n",
      "Epoch 31, Batch 13, Loss: 0.49334368109703064\n",
      "Epoch 31, Batch 14, Loss: 0.5462122559547424\n",
      "Epoch 31, Batch 15, Loss: 0.5287606716156006\n",
      "Epoch 31, Batch 16, Loss: 0.5121633410453796\n",
      "Epoch 31, Batch 17, Loss: 0.5399900078773499\n",
      "Epoch 31, Batch 18, Loss: 0.5002211332321167\n",
      "Epoch 31, Batch 19, Loss: 0.5224896669387817\n",
      "Epoch 31, Batch 20, Loss: 0.5308760404586792\n",
      "Epoch 31, Batch 21, Loss: 0.565139651298523\n",
      "Epoch 31, Batch 22, Loss: 0.4844820499420166\n",
      "Epoch 31, Batch 23, Loss: 0.5356946587562561\n",
      "Epoch 31, Batch 24, Loss: 0.5099090337753296\n",
      "Epoch 31, Batch 25, Loss: 0.47574910521507263\n",
      "Epoch 31, Batch 26, Loss: 0.5480474829673767\n",
      "Epoch 31, Batch 27, Loss: 0.5079750418663025\n",
      "Epoch 31, Batch 28, Loss: 0.5201278328895569\n",
      "Epoch 31, Batch 29, Loss: 0.5460121035575867\n",
      "Epoch 31, Batch 30, Loss: 0.5553790330886841\n",
      "Epoch 31, Batch 31, Loss: 0.505565345287323\n",
      "Epoch 31, Batch 32, Loss: 0.5068686604499817\n",
      "Epoch 31, Batch 33, Loss: 0.49418848752975464\n",
      "Epoch 31, Batch 34, Loss: 0.4961375296115875\n",
      "Epoch 31, Batch 35, Loss: 0.5031376481056213\n",
      "Epoch 31, Batch 36, Loss: 0.5343166589736938\n",
      "Epoch 31, Batch 37, Loss: 0.4761357307434082\n",
      "Epoch 31, Batch 38, Loss: 0.5199686884880066\n",
      "Epoch 31, Batch 39, Loss: 0.5516633987426758\n",
      "Epoch 31, Batch 40, Loss: 0.5180580615997314\n",
      "Epoch 31, Batch 41, Loss: 0.5266230702400208\n",
      "Epoch 31, Batch 42, Loss: 0.5626453757286072\n",
      "Epoch 31, Batch 43, Loss: 0.5027156472206116\n",
      "Epoch 31, Batch 44, Loss: 0.49334603548049927\n",
      "Epoch 31, Batch 45, Loss: 0.6049114465713501\n",
      "Epoch 31, Batch 46, Loss: 0.4915977418422699\n",
      "Epoch 31, Batch 47, Loss: 0.5519959330558777\n",
      "Epoch 31, Batch 48, Loss: 0.5108062028884888\n",
      "Epoch 31, Batch 49, Loss: 0.5075205564498901\n",
      "Epoch 31, Batch 50, Loss: 0.5473340153694153\n",
      "Epoch 31, Batch 51, Loss: 0.5370427370071411\n",
      "Epoch 31, Batch 52, Loss: 0.4566786289215088\n",
      "Epoch 31, Batch 53, Loss: 0.5480812191963196\n",
      "Epoch 31, Batch 54, Loss: 0.5496158003807068\n",
      "Epoch 31, Batch 55, Loss: 0.49095529317855835\n",
      "Epoch 31, Batch 56, Loss: 0.5518085360527039\n",
      "Epoch 31, Batch 57, Loss: 0.5521961450576782\n",
      "Epoch 31, Batch 58, Loss: 0.5289397835731506\n",
      "Epoch 31, Batch 59, Loss: 0.5053787231445312\n",
      "Epoch 31, Batch 60, Loss: 0.5264817476272583\n",
      "Epoch 31, Batch 61, Loss: 0.5212181806564331\n",
      "Epoch 31, Batch 62, Loss: 0.49638238549232483\n",
      "Epoch 31, Batch 63, Loss: 0.5696563124656677\n",
      "Epoch 31, Batch 64, Loss: 0.5451252460479736\n",
      "Epoch 31, Batch 65, Loss: 0.5107925534248352\n",
      "Epoch 31, Batch 66, Loss: 0.5286239385604858\n",
      "Epoch 31, Batch 67, Loss: 0.5156727433204651\n",
      "Epoch 31, Batch 68, Loss: 0.48612159490585327\n",
      "Epoch 31, Batch 69, Loss: 0.47135692834854126\n",
      "Epoch 31, Batch 70, Loss: 0.5070279836654663\n",
      "Epoch 31, Batch 71, Loss: 0.5029710531234741\n",
      "Epoch 31, Batch 72, Loss: 0.5176253914833069\n",
      "Epoch 31, Batch 73, Loss: 0.5494664311408997\n",
      "Epoch 31, Batch 74, Loss: 0.524152398109436\n",
      "Epoch 31, Batch 75, Loss: 0.5203818678855896\n",
      "Epoch 31, Batch 76, Loss: 0.4856262803077698\n",
      "Epoch 31, Batch 77, Loss: 0.5005882978439331\n",
      "Epoch 31, Batch 78, Loss: 0.505949079990387\n",
      "Epoch 31, Batch 79, Loss: 0.5319620370864868\n",
      "Epoch 31, Batch 80, Loss: 0.4889400005340576\n",
      "Epoch 31, Batch 81, Loss: 0.47468990087509155\n",
      "Epoch 31, Batch 82, Loss: 0.539150595664978\n",
      "Epoch 31, Batch 83, Loss: 0.4995731711387634\n",
      "Epoch 31, Batch 84, Loss: 0.4955032765865326\n",
      "Epoch 31, Batch 85, Loss: 0.599412739276886\n",
      "Epoch 31, Batch 86, Loss: 0.5088099241256714\n",
      "Epoch 31, Batch 87, Loss: 0.5086737871170044\n",
      "Epoch 31, Batch 88, Loss: 0.5052226781845093\n",
      "Epoch 31, Batch 89, Loss: 0.47540122270584106\n",
      "Epoch 31, Batch 90, Loss: 0.5102020502090454\n",
      "Epoch 31, Batch 91, Loss: 0.46874794363975525\n",
      "Epoch 31, Batch 92, Loss: 0.5691547393798828\n",
      "Epoch 31, Batch 93, Loss: 0.4825810492038727\n",
      "Epoch 32, Batch 0, Loss: 0.5319216251373291\n",
      "Epoch 32, Batch 1, Loss: 0.4985678195953369\n",
      "Epoch 32, Batch 2, Loss: 0.48347821831703186\n",
      "Epoch 32, Batch 3, Loss: 0.5552057027816772\n",
      "Epoch 32, Batch 4, Loss: 0.5615546703338623\n",
      "Epoch 32, Batch 5, Loss: 0.4593625068664551\n",
      "Epoch 32, Batch 6, Loss: 0.42016300559043884\n",
      "Epoch 32, Batch 7, Loss: 0.572091281414032\n",
      "Epoch 32, Batch 8, Loss: 0.5179083347320557\n",
      "Epoch 32, Batch 9, Loss: 0.4852559566497803\n",
      "Epoch 32, Batch 10, Loss: 0.45471519231796265\n",
      "Epoch 32, Batch 11, Loss: 0.5102797746658325\n",
      "Epoch 32, Batch 12, Loss: 0.5244547128677368\n",
      "Epoch 32, Batch 13, Loss: 0.5348379611968994\n",
      "Epoch 32, Batch 14, Loss: 0.5030807256698608\n",
      "Epoch 32, Batch 15, Loss: 0.5000237822532654\n",
      "Epoch 32, Batch 16, Loss: 0.5125218033790588\n",
      "Epoch 32, Batch 17, Loss: 0.5419215559959412\n",
      "Epoch 32, Batch 18, Loss: 0.5350393652915955\n",
      "Epoch 32, Batch 19, Loss: 0.4906597137451172\n",
      "Epoch 32, Batch 20, Loss: 0.5376384854316711\n",
      "Epoch 32, Batch 21, Loss: 0.5006029605865479\n",
      "Epoch 32, Batch 22, Loss: 0.4941704273223877\n",
      "Epoch 32, Batch 23, Loss: 0.5212498903274536\n",
      "Epoch 32, Batch 24, Loss: 0.5071028470993042\n",
      "Epoch 32, Batch 25, Loss: 0.5072230100631714\n",
      "Epoch 32, Batch 26, Loss: 0.503340482711792\n",
      "Epoch 32, Batch 27, Loss: 0.505181610584259\n",
      "Epoch 32, Batch 28, Loss: 0.5273323059082031\n",
      "Epoch 32, Batch 29, Loss: 0.4918416142463684\n",
      "Epoch 32, Batch 30, Loss: 0.5078180432319641\n",
      "Epoch 32, Batch 31, Loss: 0.5187204480171204\n",
      "Epoch 32, Batch 32, Loss: 0.5251500010490417\n",
      "Epoch 32, Batch 33, Loss: 0.5164586305618286\n",
      "Epoch 32, Batch 34, Loss: 0.5206791162490845\n",
      "Epoch 32, Batch 35, Loss: 0.5195897221565247\n",
      "Epoch 32, Batch 36, Loss: 0.5522704124450684\n",
      "Epoch 32, Batch 37, Loss: 0.49826303124427795\n",
      "Epoch 32, Batch 38, Loss: 0.47921037673950195\n",
      "Epoch 32, Batch 39, Loss: 0.4636007845401764\n",
      "Epoch 32, Batch 40, Loss: 0.49737629294395447\n",
      "Epoch 32, Batch 41, Loss: 0.4994839131832123\n",
      "Epoch 32, Batch 42, Loss: 0.5165892839431763\n",
      "Epoch 32, Batch 43, Loss: 0.5376248955726624\n",
      "Epoch 32, Batch 44, Loss: 0.5501061677932739\n",
      "Epoch 32, Batch 45, Loss: 0.5314956903457642\n",
      "Epoch 32, Batch 46, Loss: 0.5073050260543823\n",
      "Epoch 32, Batch 47, Loss: 0.47073546051979065\n",
      "Epoch 32, Batch 48, Loss: 0.5352250933647156\n",
      "Epoch 32, Batch 49, Loss: 0.5277720093727112\n",
      "Epoch 32, Batch 50, Loss: 0.45818668603897095\n",
      "Epoch 32, Batch 51, Loss: 0.49792784452438354\n",
      "Epoch 32, Batch 52, Loss: 0.4888405203819275\n",
      "Epoch 32, Batch 53, Loss: 0.5241864919662476\n",
      "Epoch 32, Batch 54, Loss: 0.5161762237548828\n",
      "Epoch 32, Batch 55, Loss: 0.4865582585334778\n",
      "Epoch 32, Batch 56, Loss: 0.4735732972621918\n",
      "Epoch 32, Batch 57, Loss: 0.49978604912757874\n",
      "Epoch 32, Batch 58, Loss: 0.5437345504760742\n",
      "Epoch 32, Batch 59, Loss: 0.48139041662216187\n",
      "Epoch 32, Batch 60, Loss: 0.5061725378036499\n",
      "Epoch 32, Batch 61, Loss: 0.4370355010032654\n",
      "Epoch 32, Batch 62, Loss: 0.4750604033470154\n",
      "Epoch 32, Batch 63, Loss: 0.545362651348114\n",
      "Epoch 32, Batch 64, Loss: 0.5603063702583313\n",
      "Epoch 32, Batch 65, Loss: 0.5072913765907288\n",
      "Epoch 32, Batch 66, Loss: 0.4885326325893402\n",
      "Epoch 32, Batch 67, Loss: 0.5860440135002136\n",
      "Epoch 32, Batch 68, Loss: 0.5151849985122681\n",
      "Epoch 32, Batch 69, Loss: 0.49122509360313416\n",
      "Epoch 32, Batch 70, Loss: 0.4634961187839508\n",
      "Epoch 32, Batch 71, Loss: 0.5044503808021545\n",
      "Epoch 32, Batch 72, Loss: 0.499776691198349\n",
      "Epoch 32, Batch 73, Loss: 0.5117053389549255\n",
      "Epoch 32, Batch 74, Loss: 0.48421111702919006\n",
      "Epoch 32, Batch 75, Loss: 0.5315130352973938\n",
      "Epoch 32, Batch 76, Loss: 0.48530951142311096\n",
      "Epoch 32, Batch 77, Loss: 0.5815072059631348\n",
      "Epoch 32, Batch 78, Loss: 0.47143658995628357\n",
      "Epoch 32, Batch 79, Loss: 0.507992148399353\n",
      "Epoch 32, Batch 80, Loss: 0.5283669233322144\n",
      "Epoch 32, Batch 81, Loss: 0.5303938984870911\n",
      "Epoch 32, Batch 82, Loss: 0.45738181471824646\n",
      "Epoch 32, Batch 83, Loss: 0.572546124458313\n",
      "Epoch 32, Batch 84, Loss: 0.5415847897529602\n",
      "Epoch 32, Batch 85, Loss: 0.558029055595398\n",
      "Epoch 32, Batch 86, Loss: 0.5378541350364685\n",
      "Epoch 32, Batch 87, Loss: 0.5420138239860535\n",
      "Epoch 32, Batch 88, Loss: 0.5147613286972046\n",
      "Epoch 32, Batch 89, Loss: 0.5003446340560913\n",
      "Epoch 32, Batch 90, Loss: 0.5914800763130188\n",
      "Epoch 32, Batch 91, Loss: 0.5286670923233032\n",
      "Epoch 32, Batch 92, Loss: 0.48066797852516174\n",
      "Epoch 32, Batch 93, Loss: 0.5384588837623596\n",
      "Epoch 33, Batch 0, Loss: 0.5300847291946411\n",
      "Epoch 33, Batch 1, Loss: 0.5084511637687683\n",
      "Epoch 33, Batch 2, Loss: 0.5246113538742065\n",
      "Epoch 33, Batch 3, Loss: 0.49072423577308655\n",
      "Epoch 33, Batch 4, Loss: 0.5565543174743652\n",
      "Epoch 33, Batch 5, Loss: 0.4736543297767639\n",
      "Epoch 33, Batch 6, Loss: 0.5230910181999207\n",
      "Epoch 33, Batch 7, Loss: 0.4864741861820221\n",
      "Epoch 33, Batch 8, Loss: 0.4907658100128174\n",
      "Epoch 33, Batch 9, Loss: 0.5576868653297424\n",
      "Epoch 33, Batch 10, Loss: 0.4922386109828949\n",
      "Epoch 33, Batch 11, Loss: 0.49593639373779297\n",
      "Epoch 33, Batch 12, Loss: 0.5255439877510071\n",
      "Epoch 33, Batch 13, Loss: 0.46829551458358765\n",
      "Epoch 33, Batch 14, Loss: 0.5390691757202148\n",
      "Epoch 33, Batch 15, Loss: 0.544660210609436\n",
      "Epoch 33, Batch 16, Loss: 0.47077640891075134\n",
      "Epoch 33, Batch 17, Loss: 0.5388802886009216\n",
      "Epoch 33, Batch 18, Loss: 0.5218561887741089\n",
      "Epoch 33, Batch 19, Loss: 0.5123715400695801\n",
      "Epoch 33, Batch 20, Loss: 0.49952834844589233\n",
      "Epoch 33, Batch 21, Loss: 0.4994772970676422\n",
      "Epoch 33, Batch 22, Loss: 0.5213745832443237\n",
      "Epoch 33, Batch 23, Loss: 0.5021083950996399\n",
      "Epoch 33, Batch 24, Loss: 0.5801788568496704\n",
      "Epoch 33, Batch 25, Loss: 0.5043870210647583\n",
      "Epoch 33, Batch 26, Loss: 0.49491795897483826\n",
      "Epoch 33, Batch 27, Loss: 0.48965659737586975\n",
      "Epoch 33, Batch 28, Loss: 0.5600829720497131\n",
      "Epoch 33, Batch 29, Loss: 0.4730025827884674\n",
      "Epoch 33, Batch 30, Loss: 0.48188215494155884\n",
      "Epoch 33, Batch 31, Loss: 0.492536723613739\n",
      "Epoch 33, Batch 32, Loss: 0.4724288880825043\n",
      "Epoch 33, Batch 33, Loss: 0.49242329597473145\n",
      "Epoch 33, Batch 34, Loss: 0.4810279905796051\n",
      "Epoch 33, Batch 35, Loss: 0.4933270514011383\n",
      "Epoch 33, Batch 36, Loss: 0.5128317475318909\n",
      "Epoch 33, Batch 37, Loss: 0.46611157059669495\n",
      "Epoch 33, Batch 38, Loss: 0.5732423067092896\n",
      "Epoch 33, Batch 39, Loss: 0.5100957155227661\n",
      "Epoch 33, Batch 40, Loss: 0.5262752771377563\n",
      "Epoch 33, Batch 41, Loss: 0.5099508166313171\n",
      "Epoch 33, Batch 42, Loss: 0.4840095639228821\n",
      "Epoch 33, Batch 43, Loss: 0.4741016924381256\n",
      "Epoch 33, Batch 44, Loss: 0.5046358704566956\n",
      "Epoch 33, Batch 45, Loss: 0.5119310021400452\n",
      "Epoch 33, Batch 46, Loss: 0.48521074652671814\n",
      "Epoch 33, Batch 47, Loss: 0.5012861490249634\n",
      "Epoch 33, Batch 48, Loss: 0.48338285088539124\n",
      "Epoch 33, Batch 49, Loss: 0.48319047689437866\n",
      "Epoch 33, Batch 50, Loss: 0.5061115622520447\n",
      "Epoch 33, Batch 51, Loss: 0.5204078555107117\n",
      "Epoch 33, Batch 52, Loss: 0.4917283058166504\n",
      "Epoch 33, Batch 53, Loss: 0.544904351234436\n",
      "Epoch 33, Batch 54, Loss: 0.4732086658477783\n",
      "Epoch 33, Batch 55, Loss: 0.44199424982070923\n",
      "Epoch 33, Batch 56, Loss: 0.4770585894584656\n",
      "Epoch 33, Batch 57, Loss: 0.4483433663845062\n",
      "Epoch 33, Batch 58, Loss: 0.48809975385665894\n",
      "Epoch 33, Batch 59, Loss: 0.4873386323451996\n",
      "Epoch 33, Batch 60, Loss: 0.5375507473945618\n",
      "Epoch 33, Batch 61, Loss: 0.5276681184768677\n",
      "Epoch 33, Batch 62, Loss: 0.5092343091964722\n",
      "Epoch 33, Batch 63, Loss: 0.5274142026901245\n",
      "Epoch 33, Batch 64, Loss: 0.5264900922775269\n",
      "Epoch 33, Batch 65, Loss: 0.5199378728866577\n",
      "Epoch 33, Batch 66, Loss: 0.5009813904762268\n",
      "Epoch 33, Batch 67, Loss: 0.4891412854194641\n",
      "Epoch 33, Batch 68, Loss: 0.5081244707107544\n",
      "Epoch 33, Batch 69, Loss: 0.48644575476646423\n",
      "Epoch 33, Batch 70, Loss: 0.5570356845855713\n",
      "Epoch 33, Batch 71, Loss: 0.47798117995262146\n",
      "Epoch 33, Batch 72, Loss: 0.4980105757713318\n",
      "Epoch 33, Batch 73, Loss: 0.5077881217002869\n",
      "Epoch 33, Batch 74, Loss: 0.48949894309043884\n",
      "Epoch 33, Batch 75, Loss: 0.5057934522628784\n",
      "Epoch 33, Batch 76, Loss: 0.47058773040771484\n",
      "Epoch 33, Batch 77, Loss: 0.5285659432411194\n",
      "Epoch 33, Batch 78, Loss: 0.4788537621498108\n",
      "Epoch 33, Batch 79, Loss: 0.4632303714752197\n",
      "Epoch 33, Batch 80, Loss: 0.513910174369812\n",
      "Epoch 33, Batch 81, Loss: 0.49066272377967834\n",
      "Epoch 33, Batch 82, Loss: 0.49103742837905884\n",
      "Epoch 33, Batch 83, Loss: 0.4935239255428314\n",
      "Epoch 33, Batch 84, Loss: 0.5341627597808838\n",
      "Epoch 33, Batch 85, Loss: 0.4840323030948639\n",
      "Epoch 33, Batch 86, Loss: 0.540243923664093\n",
      "Epoch 33, Batch 87, Loss: 0.5272809863090515\n",
      "Epoch 33, Batch 88, Loss: 0.5126399993896484\n",
      "Epoch 33, Batch 89, Loss: 0.4547968804836273\n",
      "Epoch 33, Batch 90, Loss: 0.5458678007125854\n",
      "Epoch 33, Batch 91, Loss: 0.47382888197898865\n",
      "Epoch 33, Batch 92, Loss: 0.4766194820404053\n",
      "Epoch 33, Batch 93, Loss: 0.5146242380142212\n",
      "Epoch 34, Batch 0, Loss: 0.5385041832923889\n",
      "Epoch 34, Batch 1, Loss: 0.48340898752212524\n",
      "Epoch 34, Batch 2, Loss: 0.49917277693748474\n",
      "Epoch 34, Batch 3, Loss: 0.5110441446304321\n",
      "Epoch 34, Batch 4, Loss: 0.43325501680374146\n",
      "Epoch 34, Batch 5, Loss: 0.5312389135360718\n",
      "Epoch 34, Batch 6, Loss: 0.4958726465702057\n",
      "Epoch 34, Batch 7, Loss: 0.5162450671195984\n",
      "Epoch 34, Batch 8, Loss: 0.492217481136322\n",
      "Epoch 34, Batch 9, Loss: 0.5104008316993713\n",
      "Epoch 34, Batch 10, Loss: 0.4823814332485199\n",
      "Epoch 34, Batch 11, Loss: 0.4911184310913086\n",
      "Epoch 34, Batch 12, Loss: 0.5223531723022461\n",
      "Epoch 34, Batch 13, Loss: 0.5114244222640991\n",
      "Epoch 34, Batch 14, Loss: 0.4880926012992859\n",
      "Epoch 34, Batch 15, Loss: 0.47804588079452515\n",
      "Epoch 34, Batch 16, Loss: 0.49070459604263306\n",
      "Epoch 34, Batch 17, Loss: 0.5222563743591309\n",
      "Epoch 34, Batch 18, Loss: 0.5402114391326904\n",
      "Epoch 34, Batch 19, Loss: 0.45070576667785645\n",
      "Epoch 34, Batch 20, Loss: 0.5280876755714417\n",
      "Epoch 34, Batch 21, Loss: 0.5447787046432495\n",
      "Epoch 34, Batch 22, Loss: 0.5287482142448425\n",
      "Epoch 34, Batch 23, Loss: 0.48124104738235474\n",
      "Epoch 34, Batch 24, Loss: 0.5407089591026306\n",
      "Epoch 34, Batch 25, Loss: 0.47495532035827637\n",
      "Epoch 34, Batch 26, Loss: 0.5174022912979126\n",
      "Epoch 34, Batch 27, Loss: 0.4982376992702484\n",
      "Epoch 34, Batch 28, Loss: 0.47474971413612366\n",
      "Epoch 34, Batch 29, Loss: 0.4982115626335144\n",
      "Epoch 34, Batch 30, Loss: 0.47912344336509705\n",
      "Epoch 34, Batch 31, Loss: 0.5003482103347778\n",
      "Epoch 34, Batch 32, Loss: 0.5441240668296814\n",
      "Epoch 34, Batch 33, Loss: 0.4460710883140564\n",
      "Epoch 34, Batch 34, Loss: 0.5280484557151794\n",
      "Epoch 34, Batch 35, Loss: 0.4605065882205963\n",
      "Epoch 34, Batch 36, Loss: 0.4977603852748871\n",
      "Epoch 34, Batch 37, Loss: 0.5260604023933411\n",
      "Epoch 34, Batch 38, Loss: 0.48812025785446167\n",
      "Epoch 34, Batch 39, Loss: 0.4751648008823395\n",
      "Epoch 34, Batch 40, Loss: 0.5078743100166321\n",
      "Epoch 34, Batch 41, Loss: 0.4746479392051697\n",
      "Epoch 34, Batch 42, Loss: 0.48761415481567383\n",
      "Epoch 34, Batch 43, Loss: 0.4754694402217865\n",
      "Epoch 34, Batch 44, Loss: 0.5038913488388062\n",
      "Epoch 34, Batch 45, Loss: 0.5208731889724731\n",
      "Epoch 34, Batch 46, Loss: 0.4622679352760315\n",
      "Epoch 34, Batch 47, Loss: 0.4827994704246521\n",
      "Epoch 34, Batch 48, Loss: 0.5215142965316772\n",
      "Epoch 34, Batch 49, Loss: 0.4822556972503662\n",
      "Epoch 34, Batch 50, Loss: 0.5042633414268494\n",
      "Epoch 34, Batch 51, Loss: 0.5146628618240356\n",
      "Epoch 34, Batch 52, Loss: 0.5443553924560547\n",
      "Epoch 34, Batch 53, Loss: 0.49437379837036133\n",
      "Epoch 34, Batch 54, Loss: 0.5352669954299927\n",
      "Epoch 34, Batch 55, Loss: 0.5156573057174683\n",
      "Epoch 34, Batch 56, Loss: 0.5046007037162781\n",
      "Epoch 34, Batch 57, Loss: 0.49443769454956055\n",
      "Epoch 34, Batch 58, Loss: 0.49196305871009827\n",
      "Epoch 34, Batch 59, Loss: 0.510478138923645\n",
      "Epoch 34, Batch 60, Loss: 0.49249354004859924\n",
      "Epoch 34, Batch 61, Loss: 0.4990728795528412\n",
      "Epoch 34, Batch 62, Loss: 0.5035544037818909\n",
      "Epoch 34, Batch 63, Loss: 0.48390135169029236\n",
      "Epoch 34, Batch 64, Loss: 0.5229989886283875\n",
      "Epoch 34, Batch 65, Loss: 0.5202242136001587\n",
      "Epoch 34, Batch 66, Loss: 0.46560922265052795\n",
      "Epoch 34, Batch 67, Loss: 0.49795421957969666\n",
      "Epoch 34, Batch 68, Loss: 0.4779213070869446\n",
      "Epoch 34, Batch 69, Loss: 0.5081523656845093\n",
      "Epoch 34, Batch 70, Loss: 0.499586284160614\n",
      "Epoch 34, Batch 71, Loss: 0.5064257383346558\n",
      "Epoch 34, Batch 72, Loss: 0.4797740876674652\n",
      "Epoch 34, Batch 73, Loss: 0.5128777027130127\n",
      "Epoch 34, Batch 74, Loss: 0.5318536758422852\n",
      "Epoch 34, Batch 75, Loss: 0.4819568991661072\n",
      "Epoch 34, Batch 76, Loss: 0.41919296979904175\n",
      "Epoch 34, Batch 77, Loss: 0.45712852478027344\n",
      "Epoch 34, Batch 78, Loss: 0.507546603679657\n",
      "Epoch 34, Batch 79, Loss: 0.4822230339050293\n",
      "Epoch 34, Batch 80, Loss: 0.4666343331336975\n",
      "Epoch 34, Batch 81, Loss: 0.4720749258995056\n",
      "Epoch 34, Batch 82, Loss: 0.5043622255325317\n",
      "Epoch 34, Batch 83, Loss: 0.5311716794967651\n",
      "Epoch 34, Batch 84, Loss: 0.4714021682739258\n",
      "Epoch 34, Batch 85, Loss: 0.5257647037506104\n",
      "Epoch 34, Batch 86, Loss: 0.4233846068382263\n",
      "Epoch 34, Batch 87, Loss: 0.47329893708229065\n",
      "Epoch 34, Batch 88, Loss: 0.46371155977249146\n",
      "Epoch 34, Batch 89, Loss: 0.5067011117935181\n",
      "Epoch 34, Batch 90, Loss: 0.4963221549987793\n",
      "Epoch 34, Batch 91, Loss: 0.4665074348449707\n",
      "Epoch 34, Batch 92, Loss: 0.5228251814842224\n",
      "Epoch 34, Batch 93, Loss: 0.40564796328544617\n",
      "Epoch 35, Batch 0, Loss: 0.45998531579971313\n",
      "Epoch 35, Batch 1, Loss: 0.5060902237892151\n",
      "Epoch 35, Batch 2, Loss: 0.4801938533782959\n",
      "Epoch 35, Batch 3, Loss: 0.5022183060646057\n",
      "Epoch 35, Batch 4, Loss: 0.47327908873558044\n",
      "Epoch 35, Batch 5, Loss: 0.44706717133522034\n",
      "Epoch 35, Batch 6, Loss: 0.47814375162124634\n",
      "Epoch 35, Batch 7, Loss: 0.5260306596755981\n",
      "Epoch 35, Batch 8, Loss: 0.46029776334762573\n",
      "Epoch 35, Batch 9, Loss: 0.4797372817993164\n",
      "Epoch 35, Batch 10, Loss: 0.4923013150691986\n",
      "Epoch 35, Batch 11, Loss: 0.5027996301651001\n",
      "Epoch 35, Batch 12, Loss: 0.4870053827762604\n",
      "Epoch 35, Batch 13, Loss: 0.4583263397216797\n",
      "Epoch 35, Batch 14, Loss: 0.5197353959083557\n",
      "Epoch 35, Batch 15, Loss: 0.4680801033973694\n",
      "Epoch 35, Batch 16, Loss: 0.4847586750984192\n",
      "Epoch 35, Batch 17, Loss: 0.4625655710697174\n",
      "Epoch 35, Batch 18, Loss: 0.4918801784515381\n",
      "Epoch 35, Batch 19, Loss: 0.5221027135848999\n",
      "Epoch 35, Batch 20, Loss: 0.4533368647098541\n",
      "Epoch 35, Batch 21, Loss: 0.48489803075790405\n",
      "Epoch 35, Batch 22, Loss: 0.4740046560764313\n",
      "Epoch 35, Batch 23, Loss: 0.4902748465538025\n",
      "Epoch 35, Batch 24, Loss: 0.45715317130088806\n",
      "Epoch 35, Batch 25, Loss: 0.5539105534553528\n",
      "Epoch 35, Batch 26, Loss: 0.4673226475715637\n",
      "Epoch 35, Batch 27, Loss: 0.48346489667892456\n",
      "Epoch 35, Batch 28, Loss: 0.4952238202095032\n",
      "Epoch 35, Batch 29, Loss: 0.43937864899635315\n",
      "Epoch 35, Batch 30, Loss: 0.47206392884254456\n",
      "Epoch 35, Batch 31, Loss: 0.48531657457351685\n",
      "Epoch 35, Batch 32, Loss: 0.46170201897621155\n",
      "Epoch 35, Batch 33, Loss: 0.49084019660949707\n",
      "Epoch 35, Batch 34, Loss: 0.541335940361023\n",
      "Epoch 35, Batch 35, Loss: 0.4925346374511719\n",
      "Epoch 35, Batch 36, Loss: 0.535830020904541\n",
      "Epoch 35, Batch 37, Loss: 0.47657084465026855\n",
      "Epoch 35, Batch 38, Loss: 0.4781453013420105\n",
      "Epoch 35, Batch 39, Loss: 0.4928249418735504\n",
      "Epoch 35, Batch 40, Loss: 0.4890153408050537\n",
      "Epoch 35, Batch 41, Loss: 0.4892908036708832\n",
      "Epoch 35, Batch 42, Loss: 0.497046560049057\n",
      "Epoch 35, Batch 43, Loss: 0.5060779452323914\n",
      "Epoch 35, Batch 44, Loss: 0.46092352271080017\n",
      "Epoch 35, Batch 45, Loss: 0.5185970067977905\n",
      "Epoch 35, Batch 46, Loss: 0.4757351875305176\n",
      "Epoch 35, Batch 47, Loss: 0.4686703085899353\n",
      "Epoch 35, Batch 48, Loss: 0.46377629041671753\n",
      "Epoch 35, Batch 49, Loss: 0.49634402990341187\n",
      "Epoch 35, Batch 50, Loss: 0.4748775064945221\n",
      "Epoch 35, Batch 51, Loss: 0.49428701400756836\n",
      "Epoch 35, Batch 52, Loss: 0.5156490802764893\n",
      "Epoch 35, Batch 53, Loss: 0.4583822190761566\n",
      "Epoch 35, Batch 54, Loss: 0.4853314757347107\n",
      "Epoch 35, Batch 55, Loss: 0.4858725070953369\n",
      "Epoch 35, Batch 56, Loss: 0.48170527815818787\n",
      "Epoch 35, Batch 57, Loss: 0.5208560228347778\n",
      "Epoch 35, Batch 58, Loss: 0.4601026177406311\n",
      "Epoch 35, Batch 59, Loss: 0.5242031812667847\n",
      "Epoch 35, Batch 60, Loss: 0.45698365569114685\n",
      "Epoch 35, Batch 61, Loss: 0.45390528440475464\n",
      "Epoch 35, Batch 62, Loss: 0.45565733313560486\n",
      "Epoch 35, Batch 63, Loss: 0.5194858908653259\n",
      "Epoch 35, Batch 64, Loss: 0.4689684808254242\n",
      "Epoch 35, Batch 65, Loss: 0.4740750789642334\n",
      "Epoch 35, Batch 66, Loss: 0.44402652978897095\n",
      "Epoch 35, Batch 67, Loss: 0.498023122549057\n",
      "Epoch 35, Batch 68, Loss: 0.48167943954467773\n",
      "Epoch 35, Batch 69, Loss: 0.5187785029411316\n",
      "Epoch 35, Batch 70, Loss: 0.4754929542541504\n",
      "Epoch 35, Batch 71, Loss: 0.5008600354194641\n",
      "Epoch 35, Batch 72, Loss: 0.4899445176124573\n",
      "Epoch 35, Batch 73, Loss: 0.5373520851135254\n",
      "Epoch 35, Batch 74, Loss: 0.5313643217086792\n",
      "Epoch 35, Batch 75, Loss: 0.45498114824295044\n",
      "Epoch 35, Batch 76, Loss: 0.5011056661605835\n",
      "Epoch 35, Batch 77, Loss: 0.517745852470398\n",
      "Epoch 35, Batch 78, Loss: 0.4945225715637207\n",
      "Epoch 35, Batch 79, Loss: 0.5147634148597717\n",
      "Epoch 35, Batch 80, Loss: 0.4627450108528137\n",
      "Epoch 35, Batch 81, Loss: 0.5117763876914978\n",
      "Epoch 35, Batch 82, Loss: 0.48814862966537476\n",
      "Epoch 35, Batch 83, Loss: 0.4838253855705261\n",
      "Epoch 35, Batch 84, Loss: 0.48595696687698364\n",
      "Epoch 35, Batch 85, Loss: 0.47858887910842896\n",
      "Epoch 35, Batch 86, Loss: 0.5442761182785034\n",
      "Epoch 35, Batch 87, Loss: 0.5359602570533752\n",
      "Epoch 35, Batch 88, Loss: 0.5405898094177246\n",
      "Epoch 35, Batch 89, Loss: 0.4980908930301666\n",
      "Epoch 35, Batch 90, Loss: 0.4554542899131775\n",
      "Epoch 35, Batch 91, Loss: 0.501157820224762\n",
      "Epoch 35, Batch 92, Loss: 0.5137063264846802\n",
      "Epoch 35, Batch 93, Loss: 0.5116981267929077\n",
      "Epoch 36, Batch 0, Loss: 0.4800345003604889\n",
      "Epoch 36, Batch 1, Loss: 0.48702526092529297\n",
      "Epoch 36, Batch 2, Loss: 0.5067604780197144\n",
      "Epoch 36, Batch 3, Loss: 0.44148358702659607\n",
      "Epoch 36, Batch 4, Loss: 0.510262668132782\n",
      "Epoch 36, Batch 5, Loss: 0.440451443195343\n",
      "Epoch 36, Batch 6, Loss: 0.49125751852989197\n",
      "Epoch 36, Batch 7, Loss: 0.4668444097042084\n",
      "Epoch 36, Batch 8, Loss: 0.48820215463638306\n",
      "Epoch 36, Batch 9, Loss: 0.47103577852249146\n",
      "Epoch 36, Batch 10, Loss: 0.5325726270675659\n",
      "Epoch 36, Batch 11, Loss: 0.498124361038208\n",
      "Epoch 36, Batch 12, Loss: 0.5028859376907349\n",
      "Epoch 36, Batch 13, Loss: 0.4973667562007904\n",
      "Epoch 36, Batch 14, Loss: 0.4773176312446594\n",
      "Epoch 36, Batch 15, Loss: 0.4689188599586487\n",
      "Epoch 36, Batch 16, Loss: 0.4858272969722748\n",
      "Epoch 36, Batch 17, Loss: 0.5170283913612366\n",
      "Epoch 36, Batch 18, Loss: 0.47619080543518066\n",
      "Epoch 36, Batch 19, Loss: 0.5127736330032349\n",
      "Epoch 36, Batch 20, Loss: 0.4773632884025574\n",
      "Epoch 36, Batch 21, Loss: 0.4768607020378113\n",
      "Epoch 36, Batch 22, Loss: 0.47845274209976196\n",
      "Epoch 36, Batch 23, Loss: 0.4815696179866791\n",
      "Epoch 36, Batch 24, Loss: 0.5033039450645447\n",
      "Epoch 36, Batch 25, Loss: 0.4536259174346924\n",
      "Epoch 36, Batch 26, Loss: 0.46230942010879517\n",
      "Epoch 36, Batch 27, Loss: 0.4785102903842926\n",
      "Epoch 36, Batch 28, Loss: 0.46860256791114807\n",
      "Epoch 36, Batch 29, Loss: 0.5039588212966919\n",
      "Epoch 36, Batch 30, Loss: 0.48379427194595337\n",
      "Epoch 36, Batch 31, Loss: 0.46484655141830444\n",
      "Epoch 36, Batch 32, Loss: 0.5068464875221252\n",
      "Epoch 36, Batch 33, Loss: 0.521713137626648\n",
      "Epoch 36, Batch 34, Loss: 0.5040258765220642\n",
      "Epoch 36, Batch 35, Loss: 0.5174853801727295\n",
      "Epoch 36, Batch 36, Loss: 0.5144310593605042\n",
      "Epoch 36, Batch 37, Loss: 0.4550720751285553\n",
      "Epoch 36, Batch 38, Loss: 0.4716399312019348\n",
      "Epoch 36, Batch 39, Loss: 0.44955262541770935\n",
      "Epoch 36, Batch 40, Loss: 0.47981277108192444\n",
      "Epoch 36, Batch 41, Loss: 0.4588540196418762\n",
      "Epoch 36, Batch 42, Loss: 0.5093345642089844\n",
      "Epoch 36, Batch 43, Loss: 0.4130973815917969\n",
      "Epoch 36, Batch 44, Loss: 0.457909494638443\n",
      "Epoch 36, Batch 45, Loss: 0.4388027787208557\n",
      "Epoch 36, Batch 46, Loss: 0.4966340661048889\n",
      "Epoch 36, Batch 47, Loss: 0.4740421772003174\n",
      "Epoch 36, Batch 48, Loss: 0.4479805827140808\n",
      "Epoch 36, Batch 49, Loss: 0.4467744827270508\n",
      "Epoch 36, Batch 50, Loss: 0.4945886731147766\n",
      "Epoch 36, Batch 51, Loss: 0.5061541199684143\n",
      "Epoch 36, Batch 52, Loss: 0.5210434198379517\n",
      "Epoch 36, Batch 53, Loss: 0.5197483897209167\n",
      "Epoch 36, Batch 54, Loss: 0.42819246649742126\n",
      "Epoch 36, Batch 55, Loss: 0.4729129672050476\n",
      "Epoch 36, Batch 56, Loss: 0.4746457636356354\n",
      "Epoch 36, Batch 57, Loss: 0.4661184251308441\n",
      "Epoch 36, Batch 58, Loss: 0.5059576630592346\n",
      "Epoch 36, Batch 59, Loss: 0.46590656042099\n",
      "Epoch 36, Batch 60, Loss: 0.4631838798522949\n",
      "Epoch 36, Batch 61, Loss: 0.44374632835388184\n",
      "Epoch 36, Batch 62, Loss: 0.49257031083106995\n",
      "Epoch 36, Batch 63, Loss: 0.46654829382896423\n",
      "Epoch 36, Batch 64, Loss: 0.5076338052749634\n",
      "Epoch 36, Batch 65, Loss: 0.4659233093261719\n",
      "Epoch 36, Batch 66, Loss: 0.507412314414978\n",
      "Epoch 36, Batch 67, Loss: 0.5316692590713501\n",
      "Epoch 36, Batch 68, Loss: 0.4847186505794525\n",
      "Epoch 36, Batch 69, Loss: 0.4845847189426422\n",
      "Epoch 36, Batch 70, Loss: 0.5059871673583984\n",
      "Epoch 36, Batch 71, Loss: 0.43521538376808167\n",
      "Epoch 36, Batch 72, Loss: 0.45850467681884766\n",
      "Epoch 36, Batch 73, Loss: 0.496502548456192\n",
      "Epoch 36, Batch 74, Loss: 0.4861605167388916\n",
      "Epoch 36, Batch 75, Loss: 0.5218273401260376\n",
      "Epoch 36, Batch 76, Loss: 0.4845847189426422\n",
      "Epoch 36, Batch 77, Loss: 0.45376548171043396\n",
      "Epoch 36, Batch 78, Loss: 0.5002723932266235\n",
      "Epoch 36, Batch 79, Loss: 0.5103805661201477\n",
      "Epoch 36, Batch 80, Loss: 0.48610562086105347\n",
      "Epoch 36, Batch 81, Loss: 0.5063422918319702\n",
      "Epoch 36, Batch 82, Loss: 0.518733561038971\n",
      "Epoch 36, Batch 83, Loss: 0.46356457471847534\n",
      "Epoch 36, Batch 84, Loss: 0.4631725251674652\n",
      "Epoch 36, Batch 85, Loss: 0.4678848385810852\n",
      "Epoch 36, Batch 86, Loss: 0.48854780197143555\n",
      "Epoch 36, Batch 87, Loss: 0.4963112473487854\n",
      "Epoch 36, Batch 88, Loss: 0.4414834976196289\n",
      "Epoch 36, Batch 89, Loss: 0.4551936686038971\n",
      "Epoch 36, Batch 90, Loss: 0.5154234170913696\n",
      "Epoch 36, Batch 91, Loss: 0.47875919938087463\n",
      "Epoch 36, Batch 92, Loss: 0.48191922903060913\n",
      "Epoch 36, Batch 93, Loss: 0.5038713216781616\n",
      "Epoch 37, Batch 0, Loss: 0.5169756412506104\n",
      "Epoch 37, Batch 1, Loss: 0.4641396999359131\n",
      "Epoch 37, Batch 2, Loss: 0.4493469297885895\n",
      "Epoch 37, Batch 3, Loss: 0.4587382674217224\n",
      "Epoch 37, Batch 4, Loss: 0.4683237075805664\n",
      "Epoch 37, Batch 5, Loss: 0.5239616632461548\n",
      "Epoch 37, Batch 6, Loss: 0.46761536598205566\n",
      "Epoch 37, Batch 7, Loss: 0.4948302209377289\n",
      "Epoch 37, Batch 8, Loss: 0.4906795024871826\n",
      "Epoch 37, Batch 9, Loss: 0.45050400495529175\n",
      "Epoch 37, Batch 10, Loss: 0.514196515083313\n",
      "Epoch 37, Batch 11, Loss: 0.4628726840019226\n",
      "Epoch 37, Batch 12, Loss: 0.4296015202999115\n",
      "Epoch 37, Batch 13, Loss: 0.5050370097160339\n",
      "Epoch 37, Batch 14, Loss: 0.48511767387390137\n",
      "Epoch 37, Batch 15, Loss: 0.4569075107574463\n",
      "Epoch 37, Batch 16, Loss: 0.489738404750824\n",
      "Epoch 37, Batch 17, Loss: 0.5076739192008972\n",
      "Epoch 37, Batch 18, Loss: 0.4450893998146057\n",
      "Epoch 37, Batch 19, Loss: 0.4693000316619873\n",
      "Epoch 37, Batch 20, Loss: 0.4945511817932129\n",
      "Epoch 37, Batch 21, Loss: 0.5006091594696045\n",
      "Epoch 37, Batch 22, Loss: 0.524281919002533\n",
      "Epoch 37, Batch 23, Loss: 0.4846363067626953\n",
      "Epoch 37, Batch 24, Loss: 0.48033231496810913\n",
      "Epoch 37, Batch 25, Loss: 0.49656519293785095\n",
      "Epoch 37, Batch 26, Loss: 0.4649195075035095\n",
      "Epoch 37, Batch 27, Loss: 0.4509059488773346\n",
      "Epoch 37, Batch 28, Loss: 0.5034810900688171\n",
      "Epoch 37, Batch 29, Loss: 0.46605435013771057\n",
      "Epoch 37, Batch 30, Loss: 0.4656763970851898\n",
      "Epoch 37, Batch 31, Loss: 0.5368410348892212\n",
      "Epoch 37, Batch 32, Loss: 0.4363018870353699\n",
      "Epoch 37, Batch 33, Loss: 0.45221033692359924\n",
      "Epoch 37, Batch 34, Loss: 0.5577226877212524\n",
      "Epoch 37, Batch 35, Loss: 0.484292596578598\n",
      "Epoch 37, Batch 36, Loss: 0.45734721422195435\n",
      "Epoch 37, Batch 37, Loss: 0.4429929852485657\n",
      "Epoch 37, Batch 38, Loss: 0.5340626835823059\n",
      "Epoch 37, Batch 39, Loss: 0.43680769205093384\n",
      "Epoch 37, Batch 40, Loss: 0.49492257833480835\n",
      "Epoch 37, Batch 41, Loss: 0.4718766212463379\n",
      "Epoch 37, Batch 42, Loss: 0.49501878023147583\n",
      "Epoch 37, Batch 43, Loss: 0.49669942259788513\n",
      "Epoch 37, Batch 44, Loss: 0.43355029821395874\n",
      "Epoch 37, Batch 45, Loss: 0.4742991328239441\n",
      "Epoch 37, Batch 46, Loss: 0.48810330033302307\n",
      "Epoch 37, Batch 47, Loss: 0.5064476728439331\n",
      "Epoch 37, Batch 48, Loss: 0.4425991177558899\n",
      "Epoch 37, Batch 49, Loss: 0.48377639055252075\n",
      "Epoch 37, Batch 50, Loss: 0.46124282479286194\n",
      "Epoch 37, Batch 51, Loss: 0.4790009558200836\n",
      "Epoch 37, Batch 52, Loss: 0.4562808871269226\n",
      "Epoch 37, Batch 53, Loss: 0.41364091634750366\n",
      "Epoch 37, Batch 54, Loss: 0.45126017928123474\n",
      "Epoch 37, Batch 55, Loss: 0.5054409503936768\n",
      "Epoch 37, Batch 56, Loss: 0.42881616950035095\n",
      "Epoch 37, Batch 57, Loss: 0.4908030927181244\n",
      "Epoch 37, Batch 58, Loss: 0.49165621399879456\n",
      "Epoch 37, Batch 59, Loss: 0.4886655807495117\n",
      "Epoch 37, Batch 60, Loss: 0.4434452950954437\n",
      "Epoch 37, Batch 61, Loss: 0.452360063791275\n",
      "Epoch 37, Batch 62, Loss: 0.4944678246974945\n",
      "Epoch 37, Batch 63, Loss: 0.4868360161781311\n",
      "Epoch 37, Batch 64, Loss: 0.48028168082237244\n",
      "Epoch 37, Batch 65, Loss: 0.4920748770236969\n",
      "Epoch 37, Batch 66, Loss: 0.5015189051628113\n",
      "Epoch 37, Batch 67, Loss: 0.514024019241333\n",
      "Epoch 37, Batch 68, Loss: 0.4626394808292389\n",
      "Epoch 37, Batch 69, Loss: 0.43652456998825073\n",
      "Epoch 37, Batch 70, Loss: 0.5365577936172485\n",
      "Epoch 37, Batch 71, Loss: 0.4756511151790619\n",
      "Epoch 37, Batch 72, Loss: 0.5276127457618713\n",
      "Epoch 37, Batch 73, Loss: 0.48904499411582947\n",
      "Epoch 37, Batch 74, Loss: 0.45328783988952637\n",
      "Epoch 37, Batch 75, Loss: 0.44213008880615234\n",
      "Epoch 37, Batch 76, Loss: 0.4608486592769623\n",
      "Epoch 37, Batch 77, Loss: 0.4728814661502838\n",
      "Epoch 37, Batch 78, Loss: 0.48426491022109985\n",
      "Epoch 37, Batch 79, Loss: 0.4551692605018616\n",
      "Epoch 37, Batch 80, Loss: 0.43233728408813477\n",
      "Epoch 37, Batch 81, Loss: 0.4387226998806\n",
      "Epoch 37, Batch 82, Loss: 0.4117327332496643\n",
      "Epoch 37, Batch 83, Loss: 0.47595128417015076\n",
      "Epoch 37, Batch 84, Loss: 0.5106172561645508\n",
      "Epoch 37, Batch 85, Loss: 0.4797101616859436\n",
      "Epoch 37, Batch 86, Loss: 0.4821758270263672\n",
      "Epoch 37, Batch 87, Loss: 0.45224514603614807\n",
      "Epoch 37, Batch 88, Loss: 0.47774553298950195\n",
      "Epoch 37, Batch 89, Loss: 0.48567622900009155\n",
      "Epoch 37, Batch 90, Loss: 0.4239247739315033\n",
      "Epoch 37, Batch 91, Loss: 0.5171927809715271\n",
      "Epoch 37, Batch 92, Loss: 0.4894392490386963\n",
      "Epoch 37, Batch 93, Loss: 0.45967721939086914\n",
      "Epoch 38, Batch 0, Loss: 0.4830424189567566\n",
      "Epoch 38, Batch 1, Loss: 0.4790037274360657\n",
      "Epoch 38, Batch 2, Loss: 0.43823662400245667\n",
      "Epoch 38, Batch 3, Loss: 0.47069472074508667\n",
      "Epoch 38, Batch 4, Loss: 0.48536473512649536\n",
      "Epoch 38, Batch 5, Loss: 0.5036526918411255\n",
      "Epoch 38, Batch 6, Loss: 0.4931439757347107\n",
      "Epoch 38, Batch 7, Loss: 0.4465942978858948\n",
      "Epoch 38, Batch 8, Loss: 0.49598997831344604\n",
      "Epoch 38, Batch 9, Loss: 0.5113549828529358\n",
      "Epoch 38, Batch 10, Loss: 0.45883339643478394\n",
      "Epoch 38, Batch 11, Loss: 0.5032327771186829\n",
      "Epoch 38, Batch 12, Loss: 0.48561254143714905\n",
      "Epoch 38, Batch 13, Loss: 0.41093283891677856\n",
      "Epoch 38, Batch 14, Loss: 0.4429331421852112\n",
      "Epoch 38, Batch 15, Loss: 0.4754260182380676\n",
      "Epoch 38, Batch 16, Loss: 0.5213558673858643\n",
      "Epoch 38, Batch 17, Loss: 0.4740718901157379\n",
      "Epoch 38, Batch 18, Loss: 0.45115941762924194\n",
      "Epoch 38, Batch 19, Loss: 0.47944408655166626\n",
      "Epoch 38, Batch 20, Loss: 0.48231762647628784\n",
      "Epoch 38, Batch 21, Loss: 0.47326669096946716\n",
      "Epoch 38, Batch 22, Loss: 0.4227958619594574\n",
      "Epoch 38, Batch 23, Loss: 0.4581896662712097\n",
      "Epoch 38, Batch 24, Loss: 0.4990161061286926\n",
      "Epoch 38, Batch 25, Loss: 0.45792609453201294\n",
      "Epoch 38, Batch 26, Loss: 0.471248060464859\n",
      "Epoch 38, Batch 27, Loss: 0.44487643241882324\n",
      "Epoch 38, Batch 28, Loss: 0.47866854071617126\n",
      "Epoch 38, Batch 29, Loss: 0.44219404458999634\n",
      "Epoch 38, Batch 30, Loss: 0.4526542127132416\n",
      "Epoch 38, Batch 31, Loss: 0.503055214881897\n",
      "Epoch 38, Batch 32, Loss: 0.5151206254959106\n",
      "Epoch 38, Batch 33, Loss: 0.4632527828216553\n",
      "Epoch 38, Batch 34, Loss: 0.4465804994106293\n",
      "Epoch 38, Batch 35, Loss: 0.45945873856544495\n",
      "Epoch 38, Batch 36, Loss: 0.4404295086860657\n",
      "Epoch 38, Batch 37, Loss: 0.46705764532089233\n",
      "Epoch 38, Batch 38, Loss: 0.47543302178382874\n",
      "Epoch 38, Batch 39, Loss: 0.45107245445251465\n",
      "Epoch 38, Batch 40, Loss: 0.4694533348083496\n",
      "Epoch 38, Batch 41, Loss: 0.4368911683559418\n",
      "Epoch 38, Batch 42, Loss: 0.46125927567481995\n",
      "Epoch 38, Batch 43, Loss: 0.45208820700645447\n",
      "Epoch 38, Batch 44, Loss: 0.4631071090698242\n",
      "Epoch 38, Batch 45, Loss: 0.4801080822944641\n",
      "Epoch 38, Batch 46, Loss: 0.45953550934791565\n",
      "Epoch 38, Batch 47, Loss: 0.46120142936706543\n",
      "Epoch 38, Batch 48, Loss: 0.4698869585990906\n",
      "Epoch 38, Batch 49, Loss: 0.467583030462265\n",
      "Epoch 38, Batch 50, Loss: 0.4520317614078522\n",
      "Epoch 38, Batch 51, Loss: 0.4524173140525818\n",
      "Epoch 38, Batch 52, Loss: 0.4643844664096832\n",
      "Epoch 38, Batch 53, Loss: 0.4653109014034271\n",
      "Epoch 38, Batch 54, Loss: 0.46976596117019653\n",
      "Epoch 38, Batch 55, Loss: 0.49029722809791565\n",
      "Epoch 38, Batch 56, Loss: 0.4869888424873352\n",
      "Epoch 38, Batch 57, Loss: 0.44864195585250854\n",
      "Epoch 38, Batch 58, Loss: 0.46944117546081543\n",
      "Epoch 38, Batch 59, Loss: 0.45110616087913513\n",
      "Epoch 38, Batch 60, Loss: 0.5136934518814087\n",
      "Epoch 38, Batch 61, Loss: 0.4537893235683441\n",
      "Epoch 38, Batch 62, Loss: 0.49315181374549866\n",
      "Epoch 38, Batch 63, Loss: 0.425767719745636\n",
      "Epoch 38, Batch 64, Loss: 0.4699028432369232\n",
      "Epoch 38, Batch 65, Loss: 0.49894657731056213\n",
      "Epoch 38, Batch 66, Loss: 0.45470014214515686\n",
      "Epoch 38, Batch 67, Loss: 0.44422951340675354\n",
      "Epoch 38, Batch 68, Loss: 0.5201370120048523\n",
      "Epoch 38, Batch 69, Loss: 0.500876247882843\n",
      "Epoch 38, Batch 70, Loss: 0.4350523352622986\n",
      "Epoch 38, Batch 71, Loss: 0.46173539757728577\n",
      "Epoch 38, Batch 72, Loss: 0.48299527168273926\n",
      "Epoch 38, Batch 73, Loss: 0.5265451669692993\n",
      "Epoch 38, Batch 74, Loss: 0.48533669114112854\n",
      "Epoch 38, Batch 75, Loss: 0.49004238843917847\n",
      "Epoch 38, Batch 76, Loss: 0.5200653076171875\n",
      "Epoch 38, Batch 77, Loss: 0.4839552342891693\n",
      "Epoch 38, Batch 78, Loss: 0.49117445945739746\n",
      "Epoch 38, Batch 79, Loss: 0.48207926750183105\n",
      "Epoch 38, Batch 80, Loss: 0.46229425072669983\n",
      "Epoch 38, Batch 81, Loss: 0.4571767747402191\n",
      "Epoch 38, Batch 82, Loss: 0.44405078887939453\n",
      "Epoch 38, Batch 83, Loss: 0.4926975667476654\n",
      "Epoch 38, Batch 84, Loss: 0.44933050870895386\n",
      "Epoch 38, Batch 85, Loss: 0.4502395987510681\n",
      "Epoch 38, Batch 86, Loss: 0.447225421667099\n",
      "Epoch 38, Batch 87, Loss: 0.45582279562950134\n",
      "Epoch 38, Batch 88, Loss: 0.5147072076797485\n",
      "Epoch 38, Batch 89, Loss: 0.46147292852401733\n",
      "Epoch 38, Batch 90, Loss: 0.48280414938926697\n",
      "Epoch 38, Batch 91, Loss: 0.47704488039016724\n",
      "Epoch 38, Batch 92, Loss: 0.4722447395324707\n",
      "Epoch 38, Batch 93, Loss: 0.42044809460639954\n",
      "Epoch 39, Batch 0, Loss: 0.46045026183128357\n",
      "Epoch 39, Batch 1, Loss: 0.4979553818702698\n",
      "Epoch 39, Batch 2, Loss: 0.4498392939567566\n",
      "Epoch 39, Batch 3, Loss: 0.4409615099430084\n",
      "Epoch 39, Batch 4, Loss: 0.4309219717979431\n",
      "Epoch 39, Batch 5, Loss: 0.4931284487247467\n",
      "Epoch 39, Batch 6, Loss: 0.48910385370254517\n",
      "Epoch 39, Batch 7, Loss: 0.4260334074497223\n",
      "Epoch 39, Batch 8, Loss: 0.4521337151527405\n",
      "Epoch 39, Batch 9, Loss: 0.49110764265060425\n",
      "Epoch 39, Batch 10, Loss: 0.4810013771057129\n",
      "Epoch 39, Batch 11, Loss: 0.4645790457725525\n",
      "Epoch 39, Batch 12, Loss: 0.45251578092575073\n",
      "Epoch 39, Batch 13, Loss: 0.4819517731666565\n",
      "Epoch 39, Batch 14, Loss: 0.5173764824867249\n",
      "Epoch 39, Batch 15, Loss: 0.4424439072608948\n",
      "Epoch 39, Batch 16, Loss: 0.45609068870544434\n",
      "Epoch 39, Batch 17, Loss: 0.4522415101528168\n",
      "Epoch 39, Batch 18, Loss: 0.4385048747062683\n",
      "Epoch 39, Batch 19, Loss: 0.45569926500320435\n",
      "Epoch 39, Batch 20, Loss: 0.45753049850463867\n",
      "Epoch 39, Batch 21, Loss: 0.4735952913761139\n",
      "Epoch 39, Batch 22, Loss: 0.4678274989128113\n",
      "Epoch 39, Batch 23, Loss: 0.4604814052581787\n",
      "Epoch 39, Batch 24, Loss: 0.46237245202064514\n",
      "Epoch 39, Batch 25, Loss: 0.4452084004878998\n",
      "Epoch 39, Batch 26, Loss: 0.45630115270614624\n",
      "Epoch 39, Batch 27, Loss: 0.5301949977874756\n",
      "Epoch 39, Batch 28, Loss: 0.43664106726646423\n",
      "Epoch 39, Batch 29, Loss: 0.4741802215576172\n",
      "Epoch 39, Batch 30, Loss: 0.43006935715675354\n",
      "Epoch 39, Batch 31, Loss: 0.4833287298679352\n",
      "Epoch 39, Batch 32, Loss: 0.4489598870277405\n",
      "Epoch 39, Batch 33, Loss: 0.45900458097457886\n",
      "Epoch 39, Batch 34, Loss: 0.4571438729763031\n",
      "Epoch 39, Batch 35, Loss: 0.465636670589447\n",
      "Epoch 39, Batch 36, Loss: 0.44049271941185\n",
      "Epoch 39, Batch 37, Loss: 0.476388156414032\n",
      "Epoch 39, Batch 38, Loss: 0.46132081747055054\n",
      "Epoch 39, Batch 39, Loss: 0.4612520635128021\n",
      "Epoch 39, Batch 40, Loss: 0.40065398812294006\n",
      "Epoch 39, Batch 41, Loss: 0.46131524443626404\n",
      "Epoch 39, Batch 42, Loss: 0.4433841109275818\n",
      "Epoch 39, Batch 43, Loss: 0.47226300835609436\n",
      "Epoch 39, Batch 44, Loss: 0.4688555598258972\n",
      "Epoch 39, Batch 45, Loss: 0.47820815443992615\n",
      "Epoch 39, Batch 46, Loss: 0.4613753855228424\n",
      "Epoch 39, Batch 47, Loss: 0.46259719133377075\n",
      "Epoch 39, Batch 48, Loss: 0.4365195333957672\n",
      "Epoch 39, Batch 49, Loss: 0.43192964792251587\n",
      "Epoch 39, Batch 50, Loss: 0.4379037916660309\n",
      "Epoch 39, Batch 51, Loss: 0.4560457170009613\n",
      "Epoch 39, Batch 52, Loss: 0.46227869391441345\n",
      "Epoch 39, Batch 53, Loss: 0.4116280674934387\n",
      "Epoch 39, Batch 54, Loss: 0.5057879686355591\n",
      "Epoch 39, Batch 55, Loss: 0.48888128995895386\n",
      "Epoch 39, Batch 56, Loss: 0.5100535154342651\n",
      "Epoch 39, Batch 57, Loss: 0.48103269934654236\n",
      "Epoch 39, Batch 58, Loss: 0.5095353722572327\n",
      "Epoch 39, Batch 59, Loss: 0.4563821256160736\n",
      "Epoch 39, Batch 60, Loss: 0.44431740045547485\n",
      "Epoch 39, Batch 61, Loss: 0.4389377534389496\n",
      "Epoch 39, Batch 62, Loss: 0.4907796382904053\n",
      "Epoch 39, Batch 63, Loss: 0.44071993231773376\n",
      "Epoch 39, Batch 64, Loss: 0.47286802530288696\n",
      "Epoch 39, Batch 65, Loss: 0.4428013861179352\n",
      "Epoch 39, Batch 66, Loss: 0.47227969765663147\n",
      "Epoch 39, Batch 67, Loss: 0.46179133653640747\n",
      "Epoch 39, Batch 68, Loss: 0.47010254859924316\n",
      "Epoch 39, Batch 69, Loss: 0.4638788104057312\n",
      "Epoch 39, Batch 70, Loss: 0.45025092363357544\n",
      "Epoch 39, Batch 71, Loss: 0.4813697934150696\n",
      "Epoch 39, Batch 72, Loss: 0.4851924777030945\n",
      "Epoch 39, Batch 73, Loss: 0.4336514472961426\n",
      "Epoch 39, Batch 74, Loss: 0.4537479281425476\n",
      "Epoch 39, Batch 75, Loss: 0.4813215136528015\n",
      "Epoch 39, Batch 76, Loss: 0.4906388223171234\n",
      "Epoch 39, Batch 77, Loss: 0.5251468420028687\n",
      "Epoch 39, Batch 78, Loss: 0.44325393438339233\n",
      "Epoch 39, Batch 79, Loss: 0.4857751429080963\n",
      "Epoch 39, Batch 80, Loss: 0.5040497779846191\n",
      "Epoch 39, Batch 81, Loss: 0.5023926496505737\n",
      "Epoch 39, Batch 82, Loss: 0.480104923248291\n",
      "Epoch 39, Batch 83, Loss: 0.4848197102546692\n",
      "Epoch 39, Batch 84, Loss: 0.4232729971408844\n",
      "Epoch 39, Batch 85, Loss: 0.44202709197998047\n",
      "Epoch 39, Batch 86, Loss: 0.4763234555721283\n",
      "Epoch 39, Batch 87, Loss: 0.4950675964355469\n",
      "Epoch 39, Batch 88, Loss: 0.44932812452316284\n",
      "Epoch 39, Batch 89, Loss: 0.5032076239585876\n",
      "Epoch 39, Batch 90, Loss: 0.48401251435279846\n",
      "Epoch 39, Batch 91, Loss: 0.4576784670352936\n",
      "Epoch 39, Batch 92, Loss: 0.44952335953712463\n",
      "Epoch 39, Batch 93, Loss: 0.44747018814086914\n",
      "Epoch 40, Batch 0, Loss: 0.4537069797515869\n",
      "Epoch 40, Batch 1, Loss: 0.4367408752441406\n",
      "Epoch 40, Batch 2, Loss: 0.4269227087497711\n",
      "Epoch 40, Batch 3, Loss: 0.492393434047699\n",
      "Epoch 40, Batch 4, Loss: 0.4696970582008362\n",
      "Epoch 40, Batch 5, Loss: 0.47399693727493286\n",
      "Epoch 40, Batch 6, Loss: 0.4505593776702881\n",
      "Epoch 40, Batch 7, Loss: 0.4524957239627838\n",
      "Epoch 40, Batch 8, Loss: 0.46271198987960815\n",
      "Epoch 40, Batch 9, Loss: 0.43408089876174927\n",
      "Epoch 40, Batch 10, Loss: 0.4003576338291168\n",
      "Epoch 40, Batch 11, Loss: 0.4569567143917084\n",
      "Epoch 40, Batch 12, Loss: 0.4880383610725403\n",
      "Epoch 40, Batch 13, Loss: 0.49870172142982483\n",
      "Epoch 40, Batch 14, Loss: 0.4413229525089264\n",
      "Epoch 40, Batch 15, Loss: 0.45613735914230347\n",
      "Epoch 40, Batch 16, Loss: 0.41951069235801697\n",
      "Epoch 40, Batch 17, Loss: 0.4634246826171875\n",
      "Epoch 40, Batch 18, Loss: 0.44529929757118225\n",
      "Epoch 40, Batch 19, Loss: 0.452414333820343\n",
      "Epoch 40, Batch 20, Loss: 0.433951199054718\n",
      "Epoch 40, Batch 21, Loss: 0.49627915024757385\n",
      "Epoch 40, Batch 22, Loss: 0.4243299067020416\n",
      "Epoch 40, Batch 23, Loss: 0.4845360219478607\n",
      "Epoch 40, Batch 24, Loss: 0.5055133104324341\n",
      "Epoch 40, Batch 25, Loss: 0.4660267233848572\n",
      "Epoch 40, Batch 26, Loss: 0.425109326839447\n",
      "Epoch 40, Batch 27, Loss: 0.4563640058040619\n",
      "Epoch 40, Batch 28, Loss: 0.44273996353149414\n",
      "Epoch 40, Batch 29, Loss: 0.49454832077026367\n",
      "Epoch 40, Batch 30, Loss: 0.4631192088127136\n",
      "Epoch 40, Batch 31, Loss: 0.4427977204322815\n",
      "Epoch 40, Batch 32, Loss: 0.4674971103668213\n",
      "Epoch 40, Batch 33, Loss: 0.4755919575691223\n",
      "Epoch 40, Batch 34, Loss: 0.447481632232666\n",
      "Epoch 40, Batch 35, Loss: 0.46007251739501953\n",
      "Epoch 40, Batch 36, Loss: 0.4631512761116028\n",
      "Epoch 40, Batch 37, Loss: 0.47257718443870544\n",
      "Epoch 40, Batch 38, Loss: 0.44782447814941406\n",
      "Epoch 40, Batch 39, Loss: 0.46049267053604126\n",
      "Epoch 40, Batch 40, Loss: 0.4551977217197418\n",
      "Epoch 40, Batch 41, Loss: 0.493828684091568\n",
      "Epoch 40, Batch 42, Loss: 0.5017613172531128\n",
      "Epoch 40, Batch 43, Loss: 0.4160744547843933\n",
      "Epoch 40, Batch 44, Loss: 0.46338725090026855\n",
      "Epoch 40, Batch 45, Loss: 0.5279895067214966\n",
      "Epoch 40, Batch 46, Loss: 0.4702203869819641\n",
      "Epoch 40, Batch 47, Loss: 0.4825058877468109\n",
      "Epoch 40, Batch 48, Loss: 0.4470462203025818\n",
      "Epoch 40, Batch 49, Loss: 0.44867077469825745\n",
      "Epoch 40, Batch 50, Loss: 0.4409823417663574\n",
      "Epoch 40, Batch 51, Loss: 0.4930296838283539\n",
      "Epoch 40, Batch 52, Loss: 0.45417317748069763\n",
      "Epoch 40, Batch 53, Loss: 0.4443616271018982\n",
      "Epoch 40, Batch 54, Loss: 0.4584365785121918\n",
      "Epoch 40, Batch 55, Loss: 0.41630497574806213\n",
      "Epoch 40, Batch 56, Loss: 0.44959133863449097\n",
      "Epoch 40, Batch 57, Loss: 0.4238383173942566\n",
      "Epoch 40, Batch 58, Loss: 0.48292165994644165\n",
      "Epoch 40, Batch 59, Loss: 0.4767330288887024\n",
      "Epoch 40, Batch 60, Loss: 0.4348447322845459\n",
      "Epoch 40, Batch 61, Loss: 0.5231587290763855\n",
      "Epoch 40, Batch 62, Loss: 0.47337430715560913\n",
      "Epoch 40, Batch 63, Loss: 0.5052868723869324\n",
      "Epoch 40, Batch 64, Loss: 0.4507482647895813\n",
      "Epoch 40, Batch 65, Loss: 0.4351476728916168\n",
      "Epoch 40, Batch 66, Loss: 0.501541018486023\n",
      "Epoch 40, Batch 67, Loss: 0.49443140625953674\n",
      "Epoch 40, Batch 68, Loss: 0.48031145334243774\n",
      "Epoch 40, Batch 69, Loss: 0.43725553154945374\n",
      "Epoch 40, Batch 70, Loss: 0.46903640031814575\n",
      "Epoch 40, Batch 71, Loss: 0.47230157256126404\n",
      "Epoch 40, Batch 72, Loss: 0.3864001929759979\n",
      "Epoch 40, Batch 73, Loss: 0.4600590765476227\n",
      "Epoch 40, Batch 74, Loss: 0.44418367743492126\n",
      "Epoch 40, Batch 75, Loss: 0.48354029655456543\n",
      "Epoch 40, Batch 76, Loss: 0.46283894777297974\n",
      "Epoch 40, Batch 77, Loss: 0.4341277480125427\n",
      "Epoch 40, Batch 78, Loss: 0.4326786994934082\n",
      "Epoch 40, Batch 79, Loss: 0.4477836489677429\n",
      "Epoch 40, Batch 80, Loss: 0.42939838767051697\n",
      "Epoch 40, Batch 81, Loss: 0.4874989092350006\n",
      "Epoch 40, Batch 82, Loss: 0.4672291874885559\n",
      "Epoch 40, Batch 83, Loss: 0.4999772012233734\n",
      "Epoch 40, Batch 84, Loss: 0.48327717185020447\n",
      "Epoch 40, Batch 85, Loss: 0.46251457929611206\n",
      "Epoch 40, Batch 86, Loss: 0.43720802664756775\n",
      "Epoch 40, Batch 87, Loss: 0.45100268721580505\n",
      "Epoch 40, Batch 88, Loss: 0.48672622442245483\n",
      "Epoch 40, Batch 89, Loss: 0.41947293281555176\n",
      "Epoch 40, Batch 90, Loss: 0.46976083517074585\n",
      "Epoch 40, Batch 91, Loss: 0.4662169814109802\n",
      "Epoch 40, Batch 92, Loss: 0.39177900552749634\n",
      "Epoch 40, Batch 93, Loss: 0.4546615481376648\n",
      "Epoch 41, Batch 0, Loss: 0.452490895986557\n",
      "Epoch 41, Batch 1, Loss: 0.47283244132995605\n",
      "Epoch 41, Batch 2, Loss: 0.41063863039016724\n",
      "Epoch 41, Batch 3, Loss: 0.42330700159072876\n",
      "Epoch 41, Batch 4, Loss: 0.4422532916069031\n",
      "Epoch 41, Batch 5, Loss: 0.46275243163108826\n",
      "Epoch 41, Batch 6, Loss: 0.4395425319671631\n",
      "Epoch 41, Batch 7, Loss: 0.39526915550231934\n",
      "Epoch 41, Batch 8, Loss: 0.43574342131614685\n",
      "Epoch 41, Batch 9, Loss: 0.48089489340782166\n",
      "Epoch 41, Batch 10, Loss: 0.5087294578552246\n",
      "Epoch 41, Batch 11, Loss: 0.46666043996810913\n",
      "Epoch 41, Batch 12, Loss: 0.437114953994751\n",
      "Epoch 41, Batch 13, Loss: 0.47046518325805664\n",
      "Epoch 41, Batch 14, Loss: 0.4252188801765442\n",
      "Epoch 41, Batch 15, Loss: 0.47191429138183594\n",
      "Epoch 41, Batch 16, Loss: 0.4930664598941803\n",
      "Epoch 41, Batch 17, Loss: 0.4602274000644684\n",
      "Epoch 41, Batch 18, Loss: 0.4410763680934906\n",
      "Epoch 41, Batch 19, Loss: 0.4347025752067566\n",
      "Epoch 41, Batch 20, Loss: 0.4763128161430359\n",
      "Epoch 41, Batch 21, Loss: 0.45305967330932617\n",
      "Epoch 41, Batch 22, Loss: 0.45101499557495117\n",
      "Epoch 41, Batch 23, Loss: 0.4793449938297272\n",
      "Epoch 41, Batch 24, Loss: 0.44600170850753784\n",
      "Epoch 41, Batch 25, Loss: 0.44996923208236694\n",
      "Epoch 41, Batch 26, Loss: 0.462157666683197\n",
      "Epoch 41, Batch 27, Loss: 0.4250756800174713\n",
      "Epoch 41, Batch 28, Loss: 0.461104154586792\n",
      "Epoch 41, Batch 29, Loss: 0.5046201944351196\n",
      "Epoch 41, Batch 30, Loss: 0.4308529794216156\n",
      "Epoch 41, Batch 31, Loss: 0.45665115118026733\n",
      "Epoch 41, Batch 32, Loss: 0.45908302068710327\n",
      "Epoch 41, Batch 33, Loss: 0.42041581869125366\n",
      "Epoch 41, Batch 34, Loss: 0.45049530267715454\n",
      "Epoch 41, Batch 35, Loss: 0.42921996116638184\n",
      "Epoch 41, Batch 36, Loss: 0.49742093682289124\n",
      "Epoch 41, Batch 37, Loss: 0.44837895035743713\n",
      "Epoch 41, Batch 38, Loss: 0.4707568287849426\n",
      "Epoch 41, Batch 39, Loss: 0.4561476707458496\n",
      "Epoch 41, Batch 40, Loss: 0.48864203691482544\n",
      "Epoch 41, Batch 41, Loss: 0.45701804757118225\n",
      "Epoch 41, Batch 42, Loss: 0.48118749260902405\n",
      "Epoch 41, Batch 43, Loss: 0.46593132615089417\n",
      "Epoch 41, Batch 44, Loss: 0.4806594252586365\n",
      "Epoch 41, Batch 45, Loss: 0.47447091341018677\n",
      "Epoch 41, Batch 46, Loss: 0.4246571660041809\n",
      "Epoch 41, Batch 47, Loss: 0.4505665898323059\n",
      "Epoch 41, Batch 48, Loss: 0.47601550817489624\n",
      "Epoch 41, Batch 49, Loss: 0.4492810368537903\n",
      "Epoch 41, Batch 50, Loss: 0.5008385181427002\n",
      "Epoch 41, Batch 51, Loss: 0.46634507179260254\n",
      "Epoch 41, Batch 52, Loss: 0.4294157922267914\n",
      "Epoch 41, Batch 53, Loss: 0.4241039752960205\n",
      "Epoch 41, Batch 54, Loss: 0.4581339955329895\n",
      "Epoch 41, Batch 55, Loss: 0.46272069215774536\n",
      "Epoch 41, Batch 56, Loss: 0.4704543650150299\n",
      "Epoch 41, Batch 57, Loss: 0.421062707901001\n",
      "Epoch 41, Batch 58, Loss: 0.43149590492248535\n",
      "Epoch 41, Batch 59, Loss: 0.469625860452652\n",
      "Epoch 41, Batch 60, Loss: 0.4217340350151062\n",
      "Epoch 41, Batch 61, Loss: 0.4337517321109772\n",
      "Epoch 41, Batch 62, Loss: 0.44610148668289185\n",
      "Epoch 41, Batch 63, Loss: 0.5086172819137573\n",
      "Epoch 41, Batch 64, Loss: 0.43839603662490845\n",
      "Epoch 41, Batch 65, Loss: 0.40851688385009766\n",
      "Epoch 41, Batch 66, Loss: 0.4232404828071594\n",
      "Epoch 41, Batch 67, Loss: 0.48955583572387695\n",
      "Epoch 41, Batch 68, Loss: 0.49069923162460327\n",
      "Epoch 41, Batch 69, Loss: 0.5021398663520813\n",
      "Epoch 41, Batch 70, Loss: 0.4560732841491699\n",
      "Epoch 41, Batch 71, Loss: 0.43902474641799927\n",
      "Epoch 41, Batch 72, Loss: 0.47730904817581177\n",
      "Epoch 41, Batch 73, Loss: 0.4682660698890686\n",
      "Epoch 41, Batch 74, Loss: 0.4738747477531433\n",
      "Epoch 41, Batch 75, Loss: 0.48027482628822327\n",
      "Epoch 41, Batch 76, Loss: 0.43694597482681274\n",
      "Epoch 41, Batch 77, Loss: 0.4606800079345703\n",
      "Epoch 41, Batch 78, Loss: 0.42190924286842346\n",
      "Epoch 41, Batch 79, Loss: 0.47760945558547974\n",
      "Epoch 41, Batch 80, Loss: 0.40730491280555725\n",
      "Epoch 41, Batch 81, Loss: 0.43731436133384705\n",
      "Epoch 41, Batch 82, Loss: 0.4531678259372711\n",
      "Epoch 41, Batch 83, Loss: 0.41250595450401306\n",
      "Epoch 41, Batch 84, Loss: 0.46980729699134827\n",
      "Epoch 41, Batch 85, Loss: 0.4552554488182068\n",
      "Epoch 41, Batch 86, Loss: 0.41060981154441833\n",
      "Epoch 41, Batch 87, Loss: 0.46622538566589355\n",
      "Epoch 41, Batch 88, Loss: 0.43529701232910156\n",
      "Epoch 41, Batch 89, Loss: 0.44521546363830566\n",
      "Epoch 41, Batch 90, Loss: 0.42390018701553345\n",
      "Epoch 41, Batch 91, Loss: 0.4525917172431946\n",
      "Epoch 41, Batch 92, Loss: 0.477222204208374\n",
      "Epoch 41, Batch 93, Loss: 0.4821261763572693\n",
      "Epoch 42, Batch 0, Loss: 0.4588138163089752\n",
      "Epoch 42, Batch 1, Loss: 0.4540690779685974\n",
      "Epoch 42, Batch 2, Loss: 0.5024360418319702\n",
      "Epoch 42, Batch 3, Loss: 0.45180851221084595\n",
      "Epoch 42, Batch 4, Loss: 0.44434675574302673\n",
      "Epoch 42, Batch 5, Loss: 0.4214063286781311\n",
      "Epoch 42, Batch 6, Loss: 0.456706702709198\n",
      "Epoch 42, Batch 7, Loss: 0.4672781825065613\n",
      "Epoch 42, Batch 8, Loss: 0.48325642943382263\n",
      "Epoch 42, Batch 9, Loss: 0.4142923951148987\n",
      "Epoch 42, Batch 10, Loss: 0.4497275948524475\n",
      "Epoch 42, Batch 11, Loss: 0.4312005937099457\n",
      "Epoch 42, Batch 12, Loss: 0.4622751772403717\n",
      "Epoch 42, Batch 13, Loss: 0.4133163392543793\n",
      "Epoch 42, Batch 14, Loss: 0.4203970432281494\n",
      "Epoch 42, Batch 15, Loss: 0.42217597365379333\n",
      "Epoch 42, Batch 16, Loss: 0.4516076147556305\n",
      "Epoch 42, Batch 17, Loss: 0.4069139063358307\n",
      "Epoch 42, Batch 18, Loss: 0.4653697609901428\n",
      "Epoch 42, Batch 19, Loss: 0.4310210347175598\n",
      "Epoch 42, Batch 20, Loss: 0.4030166268348694\n",
      "Epoch 42, Batch 21, Loss: 0.4835970401763916\n",
      "Epoch 42, Batch 22, Loss: 0.41236406564712524\n",
      "Epoch 42, Batch 23, Loss: 0.45179492235183716\n",
      "Epoch 42, Batch 24, Loss: 0.5327468514442444\n",
      "Epoch 42, Batch 25, Loss: 0.46602511405944824\n",
      "Epoch 42, Batch 26, Loss: 0.4440697729587555\n",
      "Epoch 42, Batch 27, Loss: 0.47477203607559204\n",
      "Epoch 42, Batch 28, Loss: 0.4615233838558197\n",
      "Epoch 42, Batch 29, Loss: 0.43164706230163574\n",
      "Epoch 42, Batch 30, Loss: 0.5103066563606262\n",
      "Epoch 42, Batch 31, Loss: 0.37968909740448\n",
      "Epoch 42, Batch 32, Loss: 0.4752441942691803\n",
      "Epoch 42, Batch 33, Loss: 0.4364960789680481\n",
      "Epoch 42, Batch 34, Loss: 0.4574405550956726\n",
      "Epoch 42, Batch 35, Loss: 0.3852359652519226\n",
      "Epoch 42, Batch 36, Loss: 0.4882580637931824\n",
      "Epoch 42, Batch 37, Loss: 0.4544883370399475\n",
      "Epoch 42, Batch 38, Loss: 0.47416773438453674\n",
      "Epoch 42, Batch 39, Loss: 0.4324626922607422\n",
      "Epoch 42, Batch 40, Loss: 0.4404679238796234\n",
      "Epoch 42, Batch 41, Loss: 0.45161789655685425\n",
      "Epoch 42, Batch 42, Loss: 0.4965701997280121\n",
      "Epoch 42, Batch 43, Loss: 0.47297030687332153\n",
      "Epoch 42, Batch 44, Loss: 0.4773675501346588\n",
      "Epoch 42, Batch 45, Loss: 0.44426900148391724\n",
      "Epoch 42, Batch 46, Loss: 0.406155526638031\n",
      "Epoch 42, Batch 47, Loss: 0.48150938749313354\n",
      "Epoch 42, Batch 48, Loss: 0.5195996761322021\n",
      "Epoch 42, Batch 49, Loss: 0.43723979592323303\n",
      "Epoch 42, Batch 50, Loss: 0.43176764249801636\n",
      "Epoch 42, Batch 51, Loss: 0.43682584166526794\n",
      "Epoch 42, Batch 52, Loss: 0.46330180764198303\n",
      "Epoch 42, Batch 53, Loss: 0.4238155484199524\n",
      "Epoch 42, Batch 54, Loss: 0.4419659674167633\n",
      "Epoch 42, Batch 55, Loss: 0.4609149098396301\n",
      "Epoch 42, Batch 56, Loss: 0.4200056195259094\n",
      "Epoch 42, Batch 57, Loss: 0.4940554201602936\n",
      "Epoch 42, Batch 58, Loss: 0.4698958396911621\n",
      "Epoch 42, Batch 59, Loss: 0.5005604028701782\n",
      "Epoch 42, Batch 60, Loss: 0.45554661750793457\n",
      "Epoch 42, Batch 61, Loss: 0.43778276443481445\n",
      "Epoch 42, Batch 62, Loss: 0.4440646171569824\n",
      "Epoch 42, Batch 63, Loss: 0.4733467698097229\n",
      "Epoch 42, Batch 64, Loss: 0.4072081446647644\n",
      "Epoch 42, Batch 65, Loss: 0.43048295378685\n",
      "Epoch 42, Batch 66, Loss: 0.4450814127922058\n",
      "Epoch 42, Batch 67, Loss: 0.4292055070400238\n",
      "Epoch 42, Batch 68, Loss: 0.4827443063259125\n",
      "Epoch 42, Batch 69, Loss: 0.43268364667892456\n",
      "Epoch 42, Batch 70, Loss: 0.4649966359138489\n",
      "Epoch 42, Batch 71, Loss: 0.44308796525001526\n",
      "Epoch 42, Batch 72, Loss: 0.4607781767845154\n",
      "Epoch 42, Batch 73, Loss: 0.46240848302841187\n",
      "Epoch 42, Batch 74, Loss: 0.4665466248989105\n",
      "Epoch 42, Batch 75, Loss: 0.45664867758750916\n",
      "Epoch 42, Batch 76, Loss: 0.42533788084983826\n",
      "Epoch 42, Batch 77, Loss: 0.42251142859458923\n",
      "Epoch 42, Batch 78, Loss: 0.43535295128822327\n",
      "Epoch 42, Batch 79, Loss: 0.4815545678138733\n",
      "Epoch 42, Batch 80, Loss: 0.45460042357444763\n",
      "Epoch 42, Batch 81, Loss: 0.47048813104629517\n",
      "Epoch 42, Batch 82, Loss: 0.4382452368736267\n",
      "Epoch 42, Batch 83, Loss: 0.4421399235725403\n",
      "Epoch 42, Batch 84, Loss: 0.4617574214935303\n",
      "Epoch 42, Batch 85, Loss: 0.4101841449737549\n",
      "Epoch 42, Batch 86, Loss: 0.4200003147125244\n",
      "Epoch 42, Batch 87, Loss: 0.4473215937614441\n",
      "Epoch 42, Batch 88, Loss: 0.398682177066803\n",
      "Epoch 42, Batch 89, Loss: 0.4221009612083435\n",
      "Epoch 42, Batch 90, Loss: 0.44136977195739746\n",
      "Epoch 42, Batch 91, Loss: 0.43014636635780334\n",
      "Epoch 42, Batch 92, Loss: 0.45707783102989197\n",
      "Epoch 42, Batch 93, Loss: 0.4797405004501343\n",
      "Epoch 43, Batch 0, Loss: 0.4969739317893982\n",
      "Epoch 43, Batch 1, Loss: 0.413506418466568\n",
      "Epoch 43, Batch 2, Loss: 0.4366653859615326\n",
      "Epoch 43, Batch 3, Loss: 0.4258057475090027\n",
      "Epoch 43, Batch 4, Loss: 0.4174448847770691\n",
      "Epoch 43, Batch 5, Loss: 0.4142381250858307\n",
      "Epoch 43, Batch 6, Loss: 0.3973068296909332\n",
      "Epoch 43, Batch 7, Loss: 0.5018419623374939\n",
      "Epoch 43, Batch 8, Loss: 0.4818291664123535\n",
      "Epoch 43, Batch 9, Loss: 0.4487381875514984\n",
      "Epoch 43, Batch 10, Loss: 0.4563950002193451\n",
      "Epoch 43, Batch 11, Loss: 0.43777981400489807\n",
      "Epoch 43, Batch 12, Loss: 0.41515254974365234\n",
      "Epoch 43, Batch 13, Loss: 0.4660481810569763\n",
      "Epoch 43, Batch 14, Loss: 0.4519038200378418\n",
      "Epoch 43, Batch 15, Loss: 0.4171392023563385\n",
      "Epoch 43, Batch 16, Loss: 0.42245978116989136\n",
      "Epoch 43, Batch 17, Loss: 0.3979267179965973\n",
      "Epoch 43, Batch 18, Loss: 0.4405505061149597\n",
      "Epoch 43, Batch 19, Loss: 0.4695727229118347\n",
      "Epoch 43, Batch 20, Loss: 0.4370875358581543\n",
      "Epoch 43, Batch 21, Loss: 0.4827605187892914\n",
      "Epoch 43, Batch 22, Loss: 0.4885715842247009\n",
      "Epoch 43, Batch 23, Loss: 0.46446436643600464\n",
      "Epoch 43, Batch 24, Loss: 0.41541585326194763\n",
      "Epoch 43, Batch 25, Loss: 0.4265374541282654\n",
      "Epoch 43, Batch 26, Loss: 0.45717230439186096\n",
      "Epoch 43, Batch 27, Loss: 0.41619163751602173\n",
      "Epoch 43, Batch 28, Loss: 0.4611332416534424\n",
      "Epoch 43, Batch 29, Loss: 0.43364816904067993\n",
      "Epoch 43, Batch 30, Loss: 0.448650985956192\n",
      "Epoch 43, Batch 31, Loss: 0.4210428297519684\n",
      "Epoch 43, Batch 32, Loss: 0.3956950306892395\n",
      "Epoch 43, Batch 33, Loss: 0.46675926446914673\n",
      "Epoch 43, Batch 34, Loss: 0.4472687840461731\n",
      "Epoch 43, Batch 35, Loss: 0.4637594223022461\n",
      "Epoch 43, Batch 36, Loss: 0.4488527774810791\n",
      "Epoch 43, Batch 37, Loss: 0.4568305015563965\n",
      "Epoch 43, Batch 38, Loss: 0.4436306059360504\n",
      "Epoch 43, Batch 39, Loss: 0.4452168345451355\n",
      "Epoch 43, Batch 40, Loss: 0.45376062393188477\n",
      "Epoch 43, Batch 41, Loss: 0.49650388956069946\n",
      "Epoch 43, Batch 42, Loss: 0.3950110673904419\n",
      "Epoch 43, Batch 43, Loss: 0.4340287148952484\n",
      "Epoch 43, Batch 44, Loss: 0.4349871277809143\n",
      "Epoch 43, Batch 45, Loss: 0.4918847978115082\n",
      "Epoch 43, Batch 46, Loss: 0.44466257095336914\n",
      "Epoch 43, Batch 47, Loss: 0.4616730213165283\n",
      "Epoch 43, Batch 48, Loss: 0.4680820405483246\n",
      "Epoch 43, Batch 49, Loss: 0.39019569754600525\n",
      "Epoch 43, Batch 50, Loss: 0.42559975385665894\n",
      "Epoch 43, Batch 51, Loss: 0.4698792099952698\n",
      "Epoch 43, Batch 52, Loss: 0.4487472474575043\n",
      "Epoch 43, Batch 53, Loss: 0.44623661041259766\n",
      "Epoch 43, Batch 54, Loss: 0.43708881735801697\n",
      "Epoch 43, Batch 55, Loss: 0.4611143469810486\n",
      "Epoch 43, Batch 56, Loss: 0.4572821259498596\n",
      "Epoch 43, Batch 57, Loss: 0.4608854651451111\n",
      "Epoch 43, Batch 58, Loss: 0.4660522937774658\n",
      "Epoch 43, Batch 59, Loss: 0.44565367698669434\n",
      "Epoch 43, Batch 60, Loss: 0.45437732338905334\n",
      "Epoch 43, Batch 61, Loss: 0.4090103507041931\n",
      "Epoch 43, Batch 62, Loss: 0.4930620789527893\n",
      "Epoch 43, Batch 63, Loss: 0.4865240454673767\n",
      "Epoch 43, Batch 64, Loss: 0.4487919211387634\n",
      "Epoch 43, Batch 65, Loss: 0.4786762297153473\n",
      "Epoch 43, Batch 66, Loss: 0.4700068533420563\n",
      "Epoch 43, Batch 67, Loss: 0.439107745885849\n",
      "Epoch 43, Batch 68, Loss: 0.5057070851325989\n",
      "Epoch 43, Batch 69, Loss: 0.43798327445983887\n",
      "Epoch 43, Batch 70, Loss: 0.36088982224464417\n",
      "Epoch 43, Batch 71, Loss: 0.4025719165802002\n",
      "Epoch 43, Batch 72, Loss: 0.355743944644928\n",
      "Epoch 43, Batch 73, Loss: 0.4546935558319092\n",
      "Epoch 43, Batch 74, Loss: 0.4716256558895111\n",
      "Epoch 43, Batch 75, Loss: 0.4224153161048889\n",
      "Epoch 43, Batch 76, Loss: 0.4372996389865875\n",
      "Epoch 43, Batch 77, Loss: 0.47215455770492554\n",
      "Epoch 43, Batch 78, Loss: 0.48220449686050415\n",
      "Epoch 43, Batch 79, Loss: 0.4274290204048157\n",
      "Epoch 43, Batch 80, Loss: 0.4370107650756836\n",
      "Epoch 43, Batch 81, Loss: 0.43533486127853394\n",
      "Epoch 43, Batch 82, Loss: 0.46623745560646057\n",
      "Epoch 43, Batch 83, Loss: 0.40221303701400757\n",
      "Epoch 43, Batch 84, Loss: 0.4276890754699707\n",
      "Epoch 43, Batch 85, Loss: 0.4557809829711914\n",
      "Epoch 43, Batch 86, Loss: 0.3966818153858185\n",
      "Epoch 43, Batch 87, Loss: 0.42802706360816956\n",
      "Epoch 43, Batch 88, Loss: 0.4985927641391754\n",
      "Epoch 43, Batch 89, Loss: 0.47909802198410034\n",
      "Epoch 43, Batch 90, Loss: 0.4109262526035309\n",
      "Epoch 43, Batch 91, Loss: 0.43987590074539185\n",
      "Epoch 43, Batch 92, Loss: 0.48300227522850037\n",
      "Epoch 43, Batch 93, Loss: 0.4326402246952057\n",
      "Epoch 44, Batch 0, Loss: 0.436985582113266\n",
      "Epoch 44, Batch 1, Loss: 0.45553985238075256\n",
      "Epoch 44, Batch 2, Loss: 0.43925347924232483\n",
      "Epoch 44, Batch 3, Loss: 0.5085218548774719\n",
      "Epoch 44, Batch 4, Loss: 0.46673041582107544\n",
      "Epoch 44, Batch 5, Loss: 0.40344351530075073\n",
      "Epoch 44, Batch 6, Loss: 0.47425732016563416\n",
      "Epoch 44, Batch 7, Loss: 0.4714342951774597\n",
      "Epoch 44, Batch 8, Loss: 0.4353424906730652\n",
      "Epoch 44, Batch 9, Loss: 0.48309651017189026\n",
      "Epoch 44, Batch 10, Loss: 0.41448721289634705\n",
      "Epoch 44, Batch 11, Loss: 0.4039571285247803\n",
      "Epoch 44, Batch 12, Loss: 0.46789684891700745\n",
      "Epoch 44, Batch 13, Loss: 0.4866373538970947\n",
      "Epoch 44, Batch 14, Loss: 0.45085811614990234\n",
      "Epoch 44, Batch 15, Loss: 0.45385661721229553\n",
      "Epoch 44, Batch 16, Loss: 0.3895213007926941\n",
      "Epoch 44, Batch 17, Loss: 0.4262118339538574\n",
      "Epoch 44, Batch 18, Loss: 0.42686471343040466\n",
      "Epoch 44, Batch 19, Loss: 0.41777557134628296\n",
      "Epoch 44, Batch 20, Loss: 0.46137967705726624\n",
      "Epoch 44, Batch 21, Loss: 0.4379652440547943\n",
      "Epoch 44, Batch 22, Loss: 0.48372071981430054\n",
      "Epoch 44, Batch 23, Loss: 0.4741578996181488\n",
      "Epoch 44, Batch 24, Loss: 0.4079190194606781\n",
      "Epoch 44, Batch 25, Loss: 0.4819280207157135\n",
      "Epoch 44, Batch 26, Loss: 0.4177190661430359\n",
      "Epoch 44, Batch 27, Loss: 0.5175859332084656\n",
      "Epoch 44, Batch 28, Loss: 0.40881285071372986\n",
      "Epoch 44, Batch 29, Loss: 0.42245012521743774\n",
      "Epoch 44, Batch 30, Loss: 0.45675143599510193\n",
      "Epoch 44, Batch 31, Loss: 0.4203224182128906\n",
      "Epoch 44, Batch 32, Loss: 0.4421273171901703\n",
      "Epoch 44, Batch 33, Loss: 0.45212477445602417\n",
      "Epoch 44, Batch 34, Loss: 0.44261521100997925\n",
      "Epoch 44, Batch 35, Loss: 0.4407707750797272\n",
      "Epoch 44, Batch 36, Loss: 0.43930214643478394\n",
      "Epoch 44, Batch 37, Loss: 0.433237224817276\n",
      "Epoch 44, Batch 38, Loss: 0.41415125131607056\n",
      "Epoch 44, Batch 39, Loss: 0.4524504244327545\n",
      "Epoch 44, Batch 40, Loss: 0.43003398180007935\n",
      "Epoch 44, Batch 41, Loss: 0.43071475625038147\n",
      "Epoch 44, Batch 42, Loss: 0.4129094183444977\n",
      "Epoch 44, Batch 43, Loss: 0.45949840545654297\n",
      "Epoch 44, Batch 44, Loss: 0.47739377617836\n",
      "Epoch 44, Batch 45, Loss: 0.4549354910850525\n",
      "Epoch 44, Batch 46, Loss: 0.4547275900840759\n",
      "Epoch 44, Batch 47, Loss: 0.4392189085483551\n",
      "Epoch 44, Batch 48, Loss: 0.405974805355072\n",
      "Epoch 44, Batch 49, Loss: 0.4144410490989685\n",
      "Epoch 44, Batch 50, Loss: 0.418830543756485\n",
      "Epoch 44, Batch 51, Loss: 0.47118911147117615\n",
      "Epoch 44, Batch 52, Loss: 0.4352218508720398\n",
      "Epoch 44, Batch 53, Loss: 0.4531368315219879\n",
      "Epoch 44, Batch 54, Loss: 0.4233471751213074\n",
      "Epoch 44, Batch 55, Loss: 0.4485013484954834\n",
      "Epoch 44, Batch 56, Loss: 0.40699559450149536\n",
      "Epoch 44, Batch 57, Loss: 0.49732694029808044\n",
      "Epoch 44, Batch 58, Loss: 0.41690701246261597\n",
      "Epoch 44, Batch 59, Loss: 0.45897746086120605\n",
      "Epoch 44, Batch 60, Loss: 0.4910619258880615\n",
      "Epoch 44, Batch 61, Loss: 0.4606441557407379\n",
      "Epoch 44, Batch 62, Loss: 0.49358615279197693\n",
      "Epoch 44, Batch 63, Loss: 0.4698609411716461\n",
      "Epoch 44, Batch 64, Loss: 0.44075536727905273\n",
      "Epoch 44, Batch 65, Loss: 0.45686039328575134\n",
      "Epoch 44, Batch 66, Loss: 0.4143912196159363\n",
      "Epoch 44, Batch 67, Loss: 0.4116165041923523\n",
      "Epoch 44, Batch 68, Loss: 0.40902629494667053\n",
      "Epoch 44, Batch 69, Loss: 0.41059356927871704\n",
      "Epoch 44, Batch 70, Loss: 0.3967042863368988\n",
      "Epoch 44, Batch 71, Loss: 0.4511800706386566\n",
      "Epoch 44, Batch 72, Loss: 0.44011545181274414\n",
      "Epoch 44, Batch 73, Loss: 0.39729607105255127\n",
      "Epoch 44, Batch 74, Loss: 0.40663114190101624\n",
      "Epoch 44, Batch 75, Loss: 0.4142530858516693\n",
      "Epoch 44, Batch 76, Loss: 0.4221828579902649\n",
      "Epoch 44, Batch 77, Loss: 0.46445518732070923\n",
      "Epoch 44, Batch 78, Loss: 0.4432617723941803\n",
      "Epoch 44, Batch 79, Loss: 0.4099838137626648\n",
      "Epoch 44, Batch 80, Loss: 0.4449900686740875\n",
      "Epoch 44, Batch 81, Loss: 0.3988535702228546\n",
      "Epoch 44, Batch 82, Loss: 0.5030174851417542\n",
      "Epoch 44, Batch 83, Loss: 0.43207088112831116\n",
      "Epoch 44, Batch 84, Loss: 0.3956722021102905\n",
      "Epoch 44, Batch 85, Loss: 0.43501925468444824\n",
      "Epoch 44, Batch 86, Loss: 0.47471827268600464\n",
      "Epoch 44, Batch 87, Loss: 0.44193869829177856\n",
      "Epoch 44, Batch 88, Loss: 0.4525400698184967\n",
      "Epoch 44, Batch 89, Loss: 0.3934612572193146\n",
      "Epoch 44, Batch 90, Loss: 0.4402744770050049\n",
      "Epoch 44, Batch 91, Loss: 0.4221024513244629\n",
      "Epoch 44, Batch 92, Loss: 0.42887935042381287\n",
      "Epoch 44, Batch 93, Loss: 0.4050615131855011\n",
      "Epoch 45, Batch 0, Loss: 0.4181342124938965\n",
      "Epoch 45, Batch 1, Loss: 0.3881840407848358\n",
      "Epoch 45, Batch 2, Loss: 0.40357908606529236\n",
      "Epoch 45, Batch 3, Loss: 0.40952640771865845\n",
      "Epoch 45, Batch 4, Loss: 0.407040536403656\n",
      "Epoch 45, Batch 5, Loss: 0.41348400712013245\n",
      "Epoch 45, Batch 6, Loss: 0.42924538254737854\n",
      "Epoch 45, Batch 7, Loss: 0.4792572557926178\n",
      "Epoch 45, Batch 8, Loss: 0.4384583532810211\n",
      "Epoch 45, Batch 9, Loss: 0.4640227258205414\n",
      "Epoch 45, Batch 10, Loss: 0.4513123035430908\n",
      "Epoch 45, Batch 11, Loss: 0.4419524073600769\n",
      "Epoch 45, Batch 12, Loss: 0.4542176127433777\n",
      "Epoch 45, Batch 13, Loss: 0.48296457529067993\n",
      "Epoch 45, Batch 14, Loss: 0.4324575364589691\n",
      "Epoch 45, Batch 15, Loss: 0.45753613114356995\n",
      "Epoch 45, Batch 16, Loss: 0.4400879442691803\n",
      "Epoch 45, Batch 17, Loss: 0.4514850676059723\n",
      "Epoch 45, Batch 18, Loss: 0.41459065675735474\n",
      "Epoch 45, Batch 19, Loss: 0.45901042222976685\n",
      "Epoch 45, Batch 20, Loss: 0.4308430254459381\n",
      "Epoch 45, Batch 21, Loss: 0.4444647431373596\n",
      "Epoch 45, Batch 22, Loss: 0.47823095321655273\n",
      "Epoch 45, Batch 23, Loss: 0.4174737334251404\n",
      "Epoch 45, Batch 24, Loss: 0.4570826590061188\n",
      "Epoch 45, Batch 25, Loss: 0.44663724303245544\n",
      "Epoch 45, Batch 26, Loss: 0.460315465927124\n",
      "Epoch 45, Batch 27, Loss: 0.4373733401298523\n",
      "Epoch 45, Batch 28, Loss: 0.4330197274684906\n",
      "Epoch 45, Batch 29, Loss: 0.42268866300582886\n",
      "Epoch 45, Batch 30, Loss: 0.48507362604141235\n",
      "Epoch 45, Batch 31, Loss: 0.40898045897483826\n",
      "Epoch 45, Batch 32, Loss: 0.42934808135032654\n",
      "Epoch 45, Batch 33, Loss: 0.41597968339920044\n",
      "Epoch 45, Batch 34, Loss: 0.4299336373806\n",
      "Epoch 45, Batch 35, Loss: 0.4545974135398865\n",
      "Epoch 45, Batch 36, Loss: 0.44898658990859985\n",
      "Epoch 45, Batch 37, Loss: 0.40293750166893005\n",
      "Epoch 45, Batch 38, Loss: 0.43389683961868286\n",
      "Epoch 45, Batch 39, Loss: 0.4368574023246765\n",
      "Epoch 45, Batch 40, Loss: 0.4174882769584656\n",
      "Epoch 45, Batch 41, Loss: 0.4701015055179596\n",
      "Epoch 45, Batch 42, Loss: 0.4451460838317871\n",
      "Epoch 45, Batch 43, Loss: 0.4410841464996338\n",
      "Epoch 45, Batch 44, Loss: 0.4928377568721771\n",
      "Epoch 45, Batch 45, Loss: 0.40450531244277954\n",
      "Epoch 45, Batch 46, Loss: 0.4203627109527588\n",
      "Epoch 45, Batch 47, Loss: 0.4227500557899475\n",
      "Epoch 45, Batch 48, Loss: 0.40579938888549805\n",
      "Epoch 45, Batch 49, Loss: 0.42367005348205566\n",
      "Epoch 45, Batch 50, Loss: 0.41608065366744995\n",
      "Epoch 45, Batch 51, Loss: 0.43690377473831177\n",
      "Epoch 45, Batch 52, Loss: 0.43707075715065\n",
      "Epoch 45, Batch 53, Loss: 0.4499173164367676\n",
      "Epoch 45, Batch 54, Loss: 0.43724972009658813\n",
      "Epoch 45, Batch 55, Loss: 0.44384685158729553\n",
      "Epoch 45, Batch 56, Loss: 0.4452812671661377\n",
      "Epoch 45, Batch 57, Loss: 0.4271864891052246\n",
      "Epoch 45, Batch 58, Loss: 0.451521635055542\n",
      "Epoch 45, Batch 59, Loss: 0.43153420090675354\n",
      "Epoch 45, Batch 60, Loss: 0.40152034163475037\n",
      "Epoch 45, Batch 61, Loss: 0.3893885910511017\n",
      "Epoch 45, Batch 62, Loss: 0.417684406042099\n",
      "Epoch 45, Batch 63, Loss: 0.39605098962783813\n",
      "Epoch 45, Batch 64, Loss: 0.4720830023288727\n",
      "Epoch 45, Batch 65, Loss: 0.41868144273757935\n",
      "Epoch 45, Batch 66, Loss: 0.47328710556030273\n",
      "Epoch 45, Batch 67, Loss: 0.4161798357963562\n",
      "Epoch 45, Batch 68, Loss: 0.46569499373435974\n",
      "Epoch 45, Batch 69, Loss: 0.4299197793006897\n",
      "Epoch 45, Batch 70, Loss: 0.48158925771713257\n",
      "Epoch 45, Batch 71, Loss: 0.4616215229034424\n",
      "Epoch 45, Batch 72, Loss: 0.4281196594238281\n",
      "Epoch 45, Batch 73, Loss: 0.4305478036403656\n",
      "Epoch 45, Batch 74, Loss: 0.4524311125278473\n",
      "Epoch 45, Batch 75, Loss: 0.4000530242919922\n",
      "Epoch 45, Batch 76, Loss: 0.43696603178977966\n",
      "Epoch 45, Batch 77, Loss: 0.4962431788444519\n",
      "Epoch 45, Batch 78, Loss: 0.43465161323547363\n",
      "Epoch 45, Batch 79, Loss: 0.47678035497665405\n",
      "Epoch 45, Batch 80, Loss: 0.4345344007015228\n",
      "Epoch 45, Batch 81, Loss: 0.45823612809181213\n",
      "Epoch 45, Batch 82, Loss: 0.41998299956321716\n",
      "Epoch 45, Batch 83, Loss: 0.49361687898635864\n",
      "Epoch 45, Batch 84, Loss: 0.42736729979515076\n",
      "Epoch 45, Batch 85, Loss: 0.45739221572875977\n",
      "Epoch 45, Batch 86, Loss: 0.39225059747695923\n",
      "Epoch 45, Batch 87, Loss: 0.3903428912162781\n",
      "Epoch 45, Batch 88, Loss: 0.4142859876155853\n",
      "Epoch 45, Batch 89, Loss: 0.44191113114356995\n",
      "Epoch 45, Batch 90, Loss: 0.4305709898471832\n",
      "Epoch 45, Batch 91, Loss: 0.4136337637901306\n",
      "Epoch 45, Batch 92, Loss: 0.4011278748512268\n",
      "Epoch 45, Batch 93, Loss: 0.42935922741889954\n",
      "Epoch 46, Batch 0, Loss: 0.4275003969669342\n",
      "Epoch 46, Batch 1, Loss: 0.37532731890678406\n",
      "Epoch 46, Batch 2, Loss: 0.431196391582489\n",
      "Epoch 46, Batch 3, Loss: 0.4780329167842865\n",
      "Epoch 46, Batch 4, Loss: 0.42764824628829956\n",
      "Epoch 46, Batch 5, Loss: 0.3913305401802063\n",
      "Epoch 46, Batch 6, Loss: 0.46566829085350037\n",
      "Epoch 46, Batch 7, Loss: 0.4104236662387848\n",
      "Epoch 46, Batch 8, Loss: 0.4473128318786621\n",
      "Epoch 46, Batch 9, Loss: 0.4726645052433014\n",
      "Epoch 46, Batch 10, Loss: 0.44109290838241577\n",
      "Epoch 46, Batch 11, Loss: 0.40309256315231323\n",
      "Epoch 46, Batch 12, Loss: 0.42987003922462463\n",
      "Epoch 46, Batch 13, Loss: 0.4183170795440674\n",
      "Epoch 46, Batch 14, Loss: 0.4293363690376282\n",
      "Epoch 46, Batch 15, Loss: 0.47722727060317993\n",
      "Epoch 46, Batch 16, Loss: 0.39089691638946533\n",
      "Epoch 46, Batch 17, Loss: 0.3966175317764282\n",
      "Epoch 46, Batch 18, Loss: 0.44963374733924866\n",
      "Epoch 46, Batch 19, Loss: 0.42566585540771484\n",
      "Epoch 46, Batch 20, Loss: 0.44572895765304565\n",
      "Epoch 46, Batch 21, Loss: 0.37442415952682495\n",
      "Epoch 46, Batch 22, Loss: 0.41713589429855347\n",
      "Epoch 46, Batch 23, Loss: 0.4073592722415924\n",
      "Epoch 46, Batch 24, Loss: 0.4152304232120514\n",
      "Epoch 46, Batch 25, Loss: 0.4602254033088684\n",
      "Epoch 46, Batch 26, Loss: 0.4647725522518158\n",
      "Epoch 46, Batch 27, Loss: 0.42803579568862915\n",
      "Epoch 46, Batch 28, Loss: 0.41396403312683105\n",
      "Epoch 46, Batch 29, Loss: 0.4336550235748291\n",
      "Epoch 46, Batch 30, Loss: 0.4409083425998688\n",
      "Epoch 46, Batch 31, Loss: 0.45585140585899353\n",
      "Epoch 46, Batch 32, Loss: 0.35750317573547363\n",
      "Epoch 46, Batch 33, Loss: 0.46039897203445435\n",
      "Epoch 46, Batch 34, Loss: 0.42132455110549927\n",
      "Epoch 46, Batch 35, Loss: 0.43826109170913696\n",
      "Epoch 46, Batch 36, Loss: 0.38457155227661133\n",
      "Epoch 46, Batch 37, Loss: 0.4510580897331238\n",
      "Epoch 46, Batch 38, Loss: 0.4341631829738617\n",
      "Epoch 46, Batch 39, Loss: 0.4818008542060852\n",
      "Epoch 46, Batch 40, Loss: 0.4788946211338043\n",
      "Epoch 46, Batch 41, Loss: 0.47294315695762634\n",
      "Epoch 46, Batch 42, Loss: 0.3818083107471466\n",
      "Epoch 46, Batch 43, Loss: 0.442033588886261\n",
      "Epoch 46, Batch 44, Loss: 0.4018830358982086\n",
      "Epoch 46, Batch 45, Loss: 0.40672165155410767\n",
      "Epoch 46, Batch 46, Loss: 0.49713534116744995\n",
      "Epoch 46, Batch 47, Loss: 0.409174382686615\n",
      "Epoch 46, Batch 48, Loss: 0.4279458522796631\n",
      "Epoch 46, Batch 49, Loss: 0.43365636467933655\n",
      "Epoch 46, Batch 50, Loss: 0.400662362575531\n",
      "Epoch 46, Batch 51, Loss: 0.4048319458961487\n",
      "Epoch 46, Batch 52, Loss: 0.4362243711948395\n",
      "Epoch 46, Batch 53, Loss: 0.44219523668289185\n",
      "Epoch 46, Batch 54, Loss: 0.47523966431617737\n",
      "Epoch 46, Batch 55, Loss: 0.45955514907836914\n",
      "Epoch 46, Batch 56, Loss: 0.4442400336265564\n",
      "Epoch 46, Batch 57, Loss: 0.4665384292602539\n",
      "Epoch 46, Batch 58, Loss: 0.4239231050014496\n",
      "Epoch 46, Batch 59, Loss: 0.4609459340572357\n",
      "Epoch 46, Batch 60, Loss: 0.4547754228115082\n",
      "Epoch 46, Batch 61, Loss: 0.43275099992752075\n",
      "Epoch 46, Batch 62, Loss: 0.4168725907802582\n",
      "Epoch 46, Batch 63, Loss: 0.4613582193851471\n",
      "Epoch 46, Batch 64, Loss: 0.40044069290161133\n",
      "Epoch 46, Batch 65, Loss: 0.44929051399230957\n",
      "Epoch 46, Batch 66, Loss: 0.478026807308197\n",
      "Epoch 46, Batch 67, Loss: 0.3890255093574524\n",
      "Epoch 46, Batch 68, Loss: 0.43881097435951233\n",
      "Epoch 46, Batch 69, Loss: 0.43320387601852417\n",
      "Epoch 46, Batch 70, Loss: 0.45710474252700806\n",
      "Epoch 46, Batch 71, Loss: 0.41174569725990295\n",
      "Epoch 46, Batch 72, Loss: 0.42671576142311096\n",
      "Epoch 46, Batch 73, Loss: 0.4656531810760498\n",
      "Epoch 46, Batch 74, Loss: 0.4865603446960449\n",
      "Epoch 46, Batch 75, Loss: 0.42171940207481384\n",
      "Epoch 46, Batch 76, Loss: 0.40935689210891724\n",
      "Epoch 46, Batch 77, Loss: 0.4445667266845703\n",
      "Epoch 46, Batch 78, Loss: 0.4307589530944824\n",
      "Epoch 46, Batch 79, Loss: 0.4611307978630066\n",
      "Epoch 46, Batch 80, Loss: 0.4311920702457428\n",
      "Epoch 46, Batch 81, Loss: 0.4473714232444763\n",
      "Epoch 46, Batch 82, Loss: 0.4112587571144104\n",
      "Epoch 46, Batch 83, Loss: 0.4236328601837158\n",
      "Epoch 46, Batch 84, Loss: 0.41598430275917053\n",
      "Epoch 46, Batch 85, Loss: 0.4432357847690582\n",
      "Epoch 46, Batch 86, Loss: 0.44643133878707886\n",
      "Epoch 46, Batch 87, Loss: 0.4047301709651947\n",
      "Epoch 46, Batch 88, Loss: 0.41168251633644104\n",
      "Epoch 46, Batch 89, Loss: 0.39839303493499756\n",
      "Epoch 46, Batch 90, Loss: 0.4536641240119934\n",
      "Epoch 46, Batch 91, Loss: 0.4184480309486389\n",
      "Epoch 46, Batch 92, Loss: 0.38838425278663635\n",
      "Epoch 46, Batch 93, Loss: 0.4346652030944824\n",
      "Epoch 47, Batch 0, Loss: 0.44702988862991333\n",
      "Epoch 47, Batch 1, Loss: 0.4357909560203552\n",
      "Epoch 47, Batch 2, Loss: 0.3975423574447632\n",
      "Epoch 47, Batch 3, Loss: 0.42038044333457947\n",
      "Epoch 47, Batch 4, Loss: 0.42393478751182556\n",
      "Epoch 47, Batch 5, Loss: 0.40499550104141235\n",
      "Epoch 47, Batch 6, Loss: 0.4161930978298187\n",
      "Epoch 47, Batch 7, Loss: 0.3895837068557739\n",
      "Epoch 47, Batch 8, Loss: 0.4822448194026947\n",
      "Epoch 47, Batch 9, Loss: 0.38559338450431824\n",
      "Epoch 47, Batch 10, Loss: 0.4715519845485687\n",
      "Epoch 47, Batch 11, Loss: 0.4357633590698242\n",
      "Epoch 47, Batch 12, Loss: 0.44156283140182495\n",
      "Epoch 47, Batch 13, Loss: 0.42071548104286194\n",
      "Epoch 47, Batch 14, Loss: 0.4689139723777771\n",
      "Epoch 47, Batch 15, Loss: 0.4544884264469147\n",
      "Epoch 47, Batch 16, Loss: 0.3930622637271881\n",
      "Epoch 47, Batch 17, Loss: 0.42198458313941956\n",
      "Epoch 47, Batch 18, Loss: 0.4382832944393158\n",
      "Epoch 47, Batch 19, Loss: 0.4111882746219635\n",
      "Epoch 47, Batch 20, Loss: 0.45111149549484253\n",
      "Epoch 47, Batch 21, Loss: 0.4245379865169525\n",
      "Epoch 47, Batch 22, Loss: 0.43286728858947754\n",
      "Epoch 47, Batch 23, Loss: 0.4459187090396881\n",
      "Epoch 47, Batch 24, Loss: 0.4241737723350525\n",
      "Epoch 47, Batch 25, Loss: 0.4020618796348572\n",
      "Epoch 47, Batch 26, Loss: 0.41907253861427307\n",
      "Epoch 47, Batch 27, Loss: 0.4399498999118805\n",
      "Epoch 47, Batch 28, Loss: 0.4405279755592346\n",
      "Epoch 47, Batch 29, Loss: 0.39435014128685\n",
      "Epoch 47, Batch 30, Loss: 0.4379705786705017\n",
      "Epoch 47, Batch 31, Loss: 0.49187755584716797\n",
      "Epoch 47, Batch 32, Loss: 0.4368465840816498\n",
      "Epoch 47, Batch 33, Loss: 0.44367772340774536\n",
      "Epoch 47, Batch 34, Loss: 0.4099454879760742\n",
      "Epoch 47, Batch 35, Loss: 0.4230925142765045\n",
      "Epoch 47, Batch 36, Loss: 0.5010727643966675\n",
      "Epoch 47, Batch 37, Loss: 0.3939135670661926\n",
      "Epoch 47, Batch 38, Loss: 0.45682844519615173\n",
      "Epoch 47, Batch 39, Loss: 0.43425995111465454\n",
      "Epoch 47, Batch 40, Loss: 0.44943276047706604\n",
      "Epoch 47, Batch 41, Loss: 0.4228372573852539\n",
      "Epoch 47, Batch 42, Loss: 0.4132784307003021\n",
      "Epoch 47, Batch 43, Loss: 0.4722751975059509\n",
      "Epoch 47, Batch 44, Loss: 0.39058712124824524\n",
      "Epoch 47, Batch 45, Loss: 0.42591461539268494\n",
      "Epoch 47, Batch 46, Loss: 0.43084898591041565\n",
      "Epoch 47, Batch 47, Loss: 0.3985190987586975\n",
      "Epoch 47, Batch 48, Loss: 0.40071550011634827\n",
      "Epoch 47, Batch 49, Loss: 0.5226954221725464\n",
      "Epoch 47, Batch 50, Loss: 0.435894638299942\n",
      "Epoch 47, Batch 51, Loss: 0.41320329904556274\n",
      "Epoch 47, Batch 52, Loss: 0.42250996828079224\n",
      "Epoch 47, Batch 53, Loss: 0.4175390303134918\n",
      "Epoch 47, Batch 54, Loss: 0.388721764087677\n",
      "Epoch 47, Batch 55, Loss: 0.44839373230934143\n",
      "Epoch 47, Batch 56, Loss: 0.4096514582633972\n",
      "Epoch 47, Batch 57, Loss: 0.4460389018058777\n",
      "Epoch 47, Batch 58, Loss: 0.36812809109687805\n",
      "Epoch 47, Batch 59, Loss: 0.43740540742874146\n",
      "Epoch 47, Batch 60, Loss: 0.40482544898986816\n",
      "Epoch 47, Batch 61, Loss: 0.4703255295753479\n",
      "Epoch 47, Batch 62, Loss: 0.4658378064632416\n",
      "Epoch 47, Batch 63, Loss: 0.42697829008102417\n",
      "Epoch 47, Batch 64, Loss: 0.41569676995277405\n",
      "Epoch 47, Batch 65, Loss: 0.4419914186000824\n",
      "Epoch 47, Batch 66, Loss: 0.41571110486984253\n",
      "Epoch 47, Batch 67, Loss: 0.46039533615112305\n",
      "Epoch 47, Batch 68, Loss: 0.4049392640590668\n",
      "Epoch 47, Batch 69, Loss: 0.381428062915802\n",
      "Epoch 47, Batch 70, Loss: 0.37896713614463806\n",
      "Epoch 47, Batch 71, Loss: 0.41691261529922485\n",
      "Epoch 47, Batch 72, Loss: 0.4370146691799164\n",
      "Epoch 47, Batch 73, Loss: 0.3701514005661011\n",
      "Epoch 47, Batch 74, Loss: 0.40549954771995544\n",
      "Epoch 47, Batch 75, Loss: 0.4532987177371979\n",
      "Epoch 47, Batch 76, Loss: 0.41748303174972534\n",
      "Epoch 47, Batch 77, Loss: 0.3939775228500366\n",
      "Epoch 47, Batch 78, Loss: 0.4275819659233093\n",
      "Epoch 47, Batch 79, Loss: 0.40275779366493225\n",
      "Epoch 47, Batch 80, Loss: 0.4224236011505127\n",
      "Epoch 47, Batch 81, Loss: 0.4172423779964447\n",
      "Epoch 47, Batch 82, Loss: 0.4364551603794098\n",
      "Epoch 47, Batch 83, Loss: 0.4394199252128601\n",
      "Epoch 47, Batch 84, Loss: 0.42250317335128784\n",
      "Epoch 47, Batch 85, Loss: 0.46387815475463867\n",
      "Epoch 47, Batch 86, Loss: 0.41810283064842224\n",
      "Epoch 47, Batch 87, Loss: 0.4267895817756653\n",
      "Epoch 47, Batch 88, Loss: 0.4657103419303894\n",
      "Epoch 47, Batch 89, Loss: 0.4505673348903656\n",
      "Epoch 47, Batch 90, Loss: 0.4183950424194336\n",
      "Epoch 47, Batch 91, Loss: 0.4437301754951477\n",
      "Epoch 47, Batch 92, Loss: 0.45353612303733826\n",
      "Epoch 47, Batch 93, Loss: 0.42856672406196594\n",
      "Epoch 48, Batch 0, Loss: 0.4347255825996399\n",
      "Epoch 48, Batch 1, Loss: 0.42363518476486206\n",
      "Epoch 48, Batch 2, Loss: 0.45812612771987915\n",
      "Epoch 48, Batch 3, Loss: 0.3634299337863922\n",
      "Epoch 48, Batch 4, Loss: 0.42000851035118103\n",
      "Epoch 48, Batch 5, Loss: 0.43573641777038574\n",
      "Epoch 48, Batch 6, Loss: 0.4787956178188324\n",
      "Epoch 48, Batch 7, Loss: 0.4511394500732422\n",
      "Epoch 48, Batch 8, Loss: 0.4435667395591736\n",
      "Epoch 48, Batch 9, Loss: 0.4145248830318451\n",
      "Epoch 48, Batch 10, Loss: 0.4553554058074951\n",
      "Epoch 48, Batch 11, Loss: 0.4181978702545166\n",
      "Epoch 48, Batch 12, Loss: 0.3967406153678894\n",
      "Epoch 48, Batch 13, Loss: 0.4747560918331146\n",
      "Epoch 48, Batch 14, Loss: 0.45072755217552185\n",
      "Epoch 48, Batch 15, Loss: 0.45695146918296814\n",
      "Epoch 48, Batch 16, Loss: 0.45963388681411743\n",
      "Epoch 48, Batch 17, Loss: 0.4300936758518219\n",
      "Epoch 48, Batch 18, Loss: 0.35000109672546387\n",
      "Epoch 48, Batch 19, Loss: 0.41428083181381226\n",
      "Epoch 48, Batch 20, Loss: 0.4047335088253021\n",
      "Epoch 48, Batch 21, Loss: 0.44367724657058716\n",
      "Epoch 48, Batch 22, Loss: 0.4345096945762634\n",
      "Epoch 48, Batch 23, Loss: 0.38860195875167847\n",
      "Epoch 48, Batch 24, Loss: 0.4359714388847351\n",
      "Epoch 48, Batch 25, Loss: 0.44364434480667114\n",
      "Epoch 48, Batch 26, Loss: 0.4047709107398987\n",
      "Epoch 48, Batch 27, Loss: 0.39080801606178284\n",
      "Epoch 48, Batch 28, Loss: 0.4622623026371002\n",
      "Epoch 48, Batch 29, Loss: 0.42692747712135315\n",
      "Epoch 48, Batch 30, Loss: 0.4039267599582672\n",
      "Epoch 48, Batch 31, Loss: 0.4048386514186859\n",
      "Epoch 48, Batch 32, Loss: 0.3676624894142151\n",
      "Epoch 48, Batch 33, Loss: 0.4355165958404541\n",
      "Epoch 48, Batch 34, Loss: 0.4478215277194977\n",
      "Epoch 48, Batch 35, Loss: 0.4399999678134918\n",
      "Epoch 48, Batch 36, Loss: 0.4083753228187561\n",
      "Epoch 48, Batch 37, Loss: 0.40582650899887085\n",
      "Epoch 48, Batch 38, Loss: 0.412680447101593\n",
      "Epoch 48, Batch 39, Loss: 0.4454960823059082\n",
      "Epoch 48, Batch 40, Loss: 0.4627082943916321\n",
      "Epoch 48, Batch 41, Loss: 0.411830335855484\n",
      "Epoch 48, Batch 42, Loss: 0.426631361246109\n",
      "Epoch 48, Batch 43, Loss: 0.3776351511478424\n",
      "Epoch 48, Batch 44, Loss: 0.4420284330844879\n",
      "Epoch 48, Batch 45, Loss: 0.4730721414089203\n",
      "Epoch 48, Batch 46, Loss: 0.43365222215652466\n",
      "Epoch 48, Batch 47, Loss: 0.43809062242507935\n",
      "Epoch 48, Batch 48, Loss: 0.4261856973171234\n",
      "Epoch 48, Batch 49, Loss: 0.41656142473220825\n",
      "Epoch 48, Batch 50, Loss: 0.39954251050949097\n",
      "Epoch 48, Batch 51, Loss: 0.44386523962020874\n",
      "Epoch 48, Batch 52, Loss: 0.39803940057754517\n",
      "Epoch 48, Batch 53, Loss: 0.36430948972702026\n",
      "Epoch 48, Batch 54, Loss: 0.38609418272972107\n",
      "Epoch 48, Batch 55, Loss: 0.362875759601593\n",
      "Epoch 48, Batch 56, Loss: 0.45799875259399414\n",
      "Epoch 48, Batch 57, Loss: 0.4460141658782959\n",
      "Epoch 48, Batch 58, Loss: 0.4172387719154358\n",
      "Epoch 48, Batch 59, Loss: 0.4426203668117523\n",
      "Epoch 48, Batch 60, Loss: 0.42908430099487305\n",
      "Epoch 48, Batch 61, Loss: 0.44510483741760254\n",
      "Epoch 48, Batch 62, Loss: 0.38489043712615967\n",
      "Epoch 48, Batch 63, Loss: 0.40946611762046814\n",
      "Epoch 48, Batch 64, Loss: 0.4563101828098297\n",
      "Epoch 48, Batch 65, Loss: 0.3812004625797272\n",
      "Epoch 48, Batch 66, Loss: 0.4367281496524811\n",
      "Epoch 48, Batch 67, Loss: 0.4003539979457855\n",
      "Epoch 48, Batch 68, Loss: 0.4525299072265625\n",
      "Epoch 48, Batch 69, Loss: 0.3936944901943207\n",
      "Epoch 48, Batch 70, Loss: 0.4712704122066498\n",
      "Epoch 48, Batch 71, Loss: 0.4539377689361572\n",
      "Epoch 48, Batch 72, Loss: 0.4324820935726166\n",
      "Epoch 48, Batch 73, Loss: 0.4377763867378235\n",
      "Epoch 48, Batch 74, Loss: 0.44759613275527954\n",
      "Epoch 48, Batch 75, Loss: 0.40704745054244995\n",
      "Epoch 48, Batch 76, Loss: 0.46148839592933655\n",
      "Epoch 48, Batch 77, Loss: 0.40239983797073364\n",
      "Epoch 48, Batch 78, Loss: 0.4416281580924988\n",
      "Epoch 48, Batch 79, Loss: 0.44686561822891235\n",
      "Epoch 48, Batch 80, Loss: 0.4376111626625061\n",
      "Epoch 48, Batch 81, Loss: 0.4174324870109558\n",
      "Epoch 48, Batch 82, Loss: 0.3951181173324585\n",
      "Epoch 48, Batch 83, Loss: 0.43810296058654785\n",
      "Epoch 48, Batch 84, Loss: 0.38858145475387573\n",
      "Epoch 48, Batch 85, Loss: 0.4004282057285309\n",
      "Epoch 48, Batch 86, Loss: 0.43337661027908325\n",
      "Epoch 48, Batch 87, Loss: 0.4352967143058777\n",
      "Epoch 48, Batch 88, Loss: 0.4390367567539215\n",
      "Epoch 48, Batch 89, Loss: 0.41050663590431213\n",
      "Epoch 48, Batch 90, Loss: 0.45683231949806213\n",
      "Epoch 48, Batch 91, Loss: 0.413715660572052\n",
      "Epoch 48, Batch 92, Loss: 0.41443437337875366\n",
      "Epoch 48, Batch 93, Loss: 0.38409629464149475\n",
      "Epoch 49, Batch 0, Loss: 0.4118198752403259\n",
      "Epoch 49, Batch 1, Loss: 0.4112606942653656\n",
      "Epoch 49, Batch 2, Loss: 0.3865639567375183\n",
      "Epoch 49, Batch 3, Loss: 0.4372829496860504\n",
      "Epoch 49, Batch 4, Loss: 0.4722833037376404\n",
      "Epoch 49, Batch 5, Loss: 0.4943731427192688\n",
      "Epoch 49, Batch 6, Loss: 0.4077261984348297\n",
      "Epoch 49, Batch 7, Loss: 0.4675227701663971\n",
      "Epoch 49, Batch 8, Loss: 0.4617621898651123\n",
      "Epoch 49, Batch 9, Loss: 0.4307957589626312\n",
      "Epoch 49, Batch 10, Loss: 0.4205949306488037\n",
      "Epoch 49, Batch 11, Loss: 0.44395413994789124\n",
      "Epoch 49, Batch 12, Loss: 0.39206328988075256\n",
      "Epoch 49, Batch 13, Loss: 0.4117278456687927\n",
      "Epoch 49, Batch 14, Loss: 0.41664838790893555\n",
      "Epoch 49, Batch 15, Loss: 0.36637312173843384\n",
      "Epoch 49, Batch 16, Loss: 0.3744807243347168\n",
      "Epoch 49, Batch 17, Loss: 0.40999191999435425\n",
      "Epoch 49, Batch 18, Loss: 0.4390917718410492\n",
      "Epoch 49, Batch 19, Loss: 0.4551719129085541\n",
      "Epoch 49, Batch 20, Loss: 0.4167197644710541\n",
      "Epoch 49, Batch 21, Loss: 0.38481634855270386\n",
      "Epoch 49, Batch 22, Loss: 0.4235113561153412\n",
      "Epoch 49, Batch 23, Loss: 0.4286067485809326\n",
      "Epoch 49, Batch 24, Loss: 0.4580845236778259\n",
      "Epoch 49, Batch 25, Loss: 0.4430978298187256\n",
      "Epoch 49, Batch 26, Loss: 0.4507810175418854\n",
      "Epoch 49, Batch 27, Loss: 0.3806644082069397\n",
      "Epoch 49, Batch 28, Loss: 0.4301297068595886\n",
      "Epoch 49, Batch 29, Loss: 0.4004491865634918\n",
      "Epoch 49, Batch 30, Loss: 0.4331292510032654\n",
      "Epoch 49, Batch 31, Loss: 0.4475483298301697\n",
      "Epoch 49, Batch 32, Loss: 0.3940882682800293\n",
      "Epoch 49, Batch 33, Loss: 0.43034830689430237\n",
      "Epoch 49, Batch 34, Loss: 0.4180634915828705\n",
      "Epoch 49, Batch 35, Loss: 0.3971185088157654\n",
      "Epoch 49, Batch 36, Loss: 0.4777092933654785\n",
      "Epoch 49, Batch 37, Loss: 0.4035138487815857\n",
      "Epoch 49, Batch 38, Loss: 0.4103074073791504\n",
      "Epoch 49, Batch 39, Loss: 0.45510751008987427\n",
      "Epoch 49, Batch 40, Loss: 0.44449248909950256\n",
      "Epoch 49, Batch 41, Loss: 0.43253350257873535\n",
      "Epoch 49, Batch 42, Loss: 0.4755842685699463\n",
      "Epoch 49, Batch 43, Loss: 0.44477683305740356\n",
      "Epoch 49, Batch 44, Loss: 0.4203311800956726\n",
      "Epoch 49, Batch 45, Loss: 0.46309566497802734\n",
      "Epoch 49, Batch 46, Loss: 0.4127412438392639\n",
      "Epoch 49, Batch 47, Loss: 0.42171651124954224\n",
      "Epoch 49, Batch 48, Loss: 0.45188403129577637\n",
      "Epoch 49, Batch 49, Loss: 0.41308659315109253\n",
      "Epoch 49, Batch 50, Loss: 0.4157327115535736\n",
      "Epoch 49, Batch 51, Loss: 0.4101773798465729\n",
      "Epoch 49, Batch 52, Loss: 0.3796423077583313\n",
      "Epoch 49, Batch 53, Loss: 0.36446136236190796\n",
      "Epoch 49, Batch 54, Loss: 0.40570521354675293\n",
      "Epoch 49, Batch 55, Loss: 0.4012306332588196\n",
      "Epoch 49, Batch 56, Loss: 0.41773396730422974\n",
      "Epoch 49, Batch 57, Loss: 0.4017700254917145\n",
      "Epoch 49, Batch 58, Loss: 0.3982035517692566\n",
      "Epoch 49, Batch 59, Loss: 0.46188145875930786\n",
      "Epoch 49, Batch 60, Loss: 0.38042548298835754\n",
      "Epoch 49, Batch 61, Loss: 0.4056801199913025\n",
      "Epoch 49, Batch 62, Loss: 0.3830219507217407\n",
      "Epoch 49, Batch 63, Loss: 0.4005264341831207\n",
      "Epoch 49, Batch 64, Loss: 0.45065879821777344\n",
      "Epoch 49, Batch 65, Loss: 0.42404547333717346\n",
      "Epoch 49, Batch 66, Loss: 0.4305010735988617\n",
      "Epoch 49, Batch 67, Loss: 0.3926592469215393\n",
      "Epoch 49, Batch 68, Loss: 0.38079720735549927\n",
      "Epoch 49, Batch 69, Loss: 0.390280157327652\n",
      "Epoch 49, Batch 70, Loss: 0.4273605942726135\n",
      "Epoch 49, Batch 71, Loss: 0.4160708785057068\n",
      "Epoch 49, Batch 72, Loss: 0.43845924735069275\n",
      "Epoch 49, Batch 73, Loss: 0.4057435989379883\n",
      "Epoch 49, Batch 74, Loss: 0.4429544508457184\n",
      "Epoch 49, Batch 75, Loss: 0.4108639359474182\n",
      "Epoch 49, Batch 76, Loss: 0.46319836378097534\n",
      "Epoch 49, Batch 77, Loss: 0.3830234408378601\n",
      "Epoch 49, Batch 78, Loss: 0.453804075717926\n",
      "Epoch 49, Batch 79, Loss: 0.35237962007522583\n",
      "Epoch 49, Batch 80, Loss: 0.42433133721351624\n",
      "Epoch 49, Batch 81, Loss: 0.4180901050567627\n",
      "Epoch 49, Batch 82, Loss: 0.4324832856655121\n",
      "Epoch 49, Batch 83, Loss: 0.42808499932289124\n",
      "Epoch 49, Batch 84, Loss: 0.4223625659942627\n",
      "Epoch 49, Batch 85, Loss: 0.40603357553482056\n",
      "Epoch 49, Batch 86, Loss: 0.45092257857322693\n",
      "Epoch 49, Batch 87, Loss: 0.42543596029281616\n",
      "Epoch 49, Batch 88, Loss: 0.44312208890914917\n",
      "Epoch 49, Batch 89, Loss: 0.44076746702194214\n",
      "Epoch 49, Batch 90, Loss: 0.4042882025241852\n",
      "Epoch 49, Batch 91, Loss: 0.43940648436546326\n",
      "Epoch 49, Batch 92, Loss: 0.4057183861732483\n",
      "Epoch 49, Batch 93, Loss: 0.38416117429733276\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.functional.nll_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "for current_epoch in range(n_epoch):\n",
    "  for batch_idx, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(batch_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, batch_labels)\n",
    "    train_losses.append(loss.item())\n",
    "    print(f\"Epoch {current_epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4168150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_losses, color='blue')\n",
    "plt.legend(['Train Loss'], loc='upper right')\n",
    "plt.xlabel('batch count')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b7e21bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.4037, Accuracy: 8921/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.cpu()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "  for data, target in test_loader:\n",
    "    output = model(data)\n",
    "    test_loss += torch.nn.functional.nll_loss(output, target, size_average=False).item()\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab-M5MNRa5y-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
